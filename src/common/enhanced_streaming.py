#!/usr/bin/env python3
"""
å¢å¼ºæµå¼å¤„ç†å™¨
æ”¯æŒæ˜¾ç¤ºLangGraphèŠ‚ç‚¹çº§åˆ«çš„æ‰§è¡Œè¿‡ç¨‹
"""

import asyncio
import time
from typing import AsyncGenerator, Dict, Any, Optional
from .streaming_wrapper import StreamingWrapper


class NodeVisualizer:
    """èŠ‚ç‚¹å¯è§†åŒ–å™¨"""
    
    def __init__(self, show_details: bool = True):
        self.show_details = show_details
        self.step_counter = 0
        
    def format_node_info(self, node_name: str, step: int) -> str:
        """æ ¼å¼åŒ–èŠ‚ç‚¹ä¿¡æ¯"""
        node_icons = {
            "call_model": "ğŸ§ ",
            "tools": "ğŸ”§", 
            "__start__": "ğŸš€",
            "__end__": "âœ…"
        }
        
        icon = node_icons.get(node_name, "âš™ï¸")
        return f"{icon} æ­¥éª¤ {step}: {node_name}"
    
    def format_thinking(self, content: str, max_length: int = 100) -> str:
        """æ ¼å¼åŒ–æ€è€ƒå†…å®¹"""
        if len(content) <= max_length:
            return f"ğŸ’­ æ€è€ƒ: {content}"
        else:
            return f"ğŸ’­ æ€è€ƒ: {content[:max_length]}..."
    
    def format_tool_call(self, tool_call: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨"""
        name = tool_call.get('name', 'unknown')
        args = tool_call.get('args', {})
        
        # ç®€åŒ–å‚æ•°æ˜¾ç¤º
        if len(str(args)) > 100:
            args_str = f"{str(args)[:97]}..."
        else:
            args_str = str(args)
            
        return f"ğŸ”§ è°ƒç”¨å·¥å…·: {name}\n     å‚æ•°: {args_str}"
    
    def format_tool_result(self, name: str, content: str, max_length: int = 200) -> str:
        """æ ¼å¼åŒ–å·¥å…·ç»“æœ"""
        if len(content) <= max_length:
            return f"ğŸ“Š å·¥å…· '{name}' ç»“æœ: {content}"
        else:
            return f"ğŸ“Š å·¥å…· '{name}' ç»“æœ: {content[:max_length]}..."


class EnhancedStreaming:
    """å¢å¼ºæµå¼å¤„ç†å™¨"""
    
    def __init__(self, verbose: bool = False, show_timing: bool = False):
        self.verbose = verbose
        self.show_timing = show_timing
        self.visualizer = NodeVisualizer(show_details=verbose)
        self.streaming_wrapper = StreamingWrapper(base_delay=0.02, punct_delay=0.08)
        
    async def stream_with_node_info(
        self,
        graph_stream: AsyncGenerator[Dict[str, Any], None],
        show_intermediate: bool = True
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¢å¼ºçš„æµå¼å¤„ç†ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹ä¿¡æ¯
        
        Args:
            graph_stream: LangGraphçš„astreamè¾“å‡º
            show_intermediate: æ˜¯å¦æ˜¾ç¤ºä¸­é—´æ­¥éª¤
            
        Yields:
            DictåŒ…å«: type, content, node_name, stepç­‰ä¿¡æ¯
        """
        step = 0
        start_time = time.time() if self.show_timing else None
        
        async for chunk in graph_stream:
            step += 1
            
            for node_name, node_output in chunk.items():
                # å‘é€èŠ‚ç‚¹å¼€å§‹ä¿¡æ¯
                if show_intermediate:
                    yield {
                        "type": "node_start",
                        "node_name": node_name,
                        "step": step,
                        "content": self.visualizer.format_node_info(node_name, step)
                    }
                
                # å¤„ç†æ¶ˆæ¯
                if "messages" in node_output:
                    async for message_event in self._process_messages(
                        node_output["messages"], 
                        node_name, 
                        step, 
                        show_intermediate
                    ):
                        yield message_event
                    
                # å¦‚æœæ˜¯æœ€ç»ˆçš„call_modelèŠ‚ç‚¹ï¼Œè¿”å›æµå¼æ–‡æœ¬
                if node_name == "call_model" and "messages" in node_output:
                    final_message = node_output["messages"][-1]
                    if hasattr(final_message, 'content') and final_message.content:
                        # åªæœ‰å½“æ²¡æœ‰å·¥å…·è°ƒç”¨æ—¶æ‰æµå¼è¾“å‡ºæœ€ç»ˆå›ç­”
                        if not (hasattr(final_message, 'tool_calls') and final_message.tool_calls):
                            yield {
                                "type": "final_response_start",
                                "content": ""
                            }
                            
                            # æµå¼è¾“å‡ºæœ€ç»ˆå›ç­”
                            async for text_chunk in self.streaming_wrapper.simulate_streaming(
                                final_message.content, chunk_size=2
                            ):
                                yield {
                                    "type": "final_response_chunk", 
                                    "content": text_chunk
                                }
                            
                            yield {
                                "type": "final_response_end",
                                "content": ""
                            }
        
        # å‘é€å®Œæˆä¿¡æ¯
        if self.show_timing and start_time:
            duration = time.time() - start_time
            yield {
                "type": "completion", 
                "content": f"â±ï¸ æ€»è€—æ—¶: {duration:.2f}ç§’"
            }
    
    async def _process_messages(
        self, 
        messages: list, 
        node_name: str, 
        step: int, 
        show_intermediate: bool
    ):
        """å¤„ç†æ¶ˆæ¯åˆ—è¡¨"""
        for message in messages:
            # AIæ€è€ƒå†…å®¹
            if hasattr(message, 'content') and message.content and show_intermediate:
                # å¯¹äºä¸­é—´æ­¥éª¤çš„æ€è€ƒï¼Œä¸è¿›è¡Œæµå¼æ˜¾ç¤ºï¼Œç›´æ¥æ˜¾ç¤º
                yield {
                    "type": "thinking",
                    "content": self.visualizer.format_thinking(message.content),
                    "node_name": node_name,
                    "step": step
                }
            
            # å·¥å…·è°ƒç”¨
            if hasattr(message, 'tool_calls') and message.tool_calls:
                for tool_call in message.tool_calls:
                    if show_intermediate:
                        yield {
                            "type": "tool_call",
                            "content": self.visualizer.format_tool_call(tool_call),
                            "node_name": node_name,
                            "step": step,
                            "tool_name": tool_call.get('name', 'unknown')
                        }
            
            # å·¥å…·ç»“æœ
            if hasattr(message, 'name') and show_intermediate:  # ToolMessage
                yield {
                    "type": "tool_result",
                    "content": self.visualizer.format_tool_result(
                        message.name, 
                        str(message.content)
                    ),
                    "node_name": node_name,
                    "step": step,
                    "tool_name": message.name
                }


class CliStreamingHandler:
    """CLIæµå¼å¤„ç†å¥æŸ„"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.enhanced_streaming = EnhancedStreaming(verbose=verbose, show_timing=True)
    
    async def handle_streaming_chat(
        self,
        graph_stream: AsyncGenerator[Dict[str, Any], None],
        session_prompt: str = ""
    ):
        """
        å¤„ç†CLIæµå¼èŠå¤©
        
        Args:
            graph_stream: LangGraphçš„streamè¾“å‡º
            session_prompt: ä¼šè¯æç¤ºç¬¦å‰ç¼€
        """
        print("ğŸ¤” AIæ­£åœ¨åˆ†æå’Œå¤„ç†...", end="", flush=True)
        await asyncio.sleep(0.3)
        
        final_response_started = False
        
        async for event in self.enhanced_streaming.stream_with_node_info(
            graph_stream, 
            show_intermediate=self.verbose
        ):
            event_type = event.get("type")
            content = event.get("content", "")
            
            if event_type == "node_start" and self.verbose:
                print(f"\r{' ' * 50}\r", end="")  # æ¸…é™¤ä¹‹å‰çš„å†…å®¹
                print(f"  {content}")
                
            elif event_type == "thinking" and self.verbose:
                print(f"    {content}")
                
            elif event_type == "tool_call" and self.verbose:
                print(f"    {content}")
                
            elif event_type == "tool_result" and self.verbose:
                print(f"    {content}")
                
            elif event_type == "final_response_start":
                print(f"\r{' ' * 50}\r", end="")  # æ¸…é™¤å¤„ç†æç¤º
                print(f"{session_prompt}ğŸ¤– AI: ", end="", flush=True)
                final_response_started = True
                
            elif event_type == "final_response_chunk" and final_response_started:
                print(content, end="", flush=True)
                
            elif event_type == "final_response_end":
                print()  # æ¢è¡Œ
                final_response_started = False
                
            elif event_type == "completion":
                if self.verbose:
                    print(f"\nğŸ’« {content}")


# ä¾¿æ·å‡½æ•°
async def create_enhanced_stream(graph, state, context, verbose: bool = False):
    """åˆ›å»ºå¢å¼ºæµå¼å¤„ç†"""
    handler = CliStreamingHandler(verbose=verbose)
    graph_stream = graph.astream(state, context=context)
    return handler.handle_streaming_chat(graph_stream)