#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGFlow çŸ¥è¯†åº“å¬å›æ¥å£çš„å®Œæ•´Pythonç¤ºä¾‹
åŒ…å«Python SDKå’ŒHTTP APIä¸¤ç§è°ƒç”¨æ–¹å¼
"""

import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional

import requests
from ragflow_sdk import RAGFlow


class RAGFlowRetrieval:
    """RAGFlowå¬å›æ¥å£å°è£…ç±»"""
    
    def __init__(self, api_key: str, base_url: str):
        """
        åˆå§‹åŒ–RAGFlowå¬å›å®¢æˆ·ç«¯
        
        Args:
            api_key: RAGFlow APIå¯†é’¥
            base_url: RAGFlowæœåŠ¡å™¨åœ°å€ï¼Œæ ¼å¼å¦‚ "http://localhost:9380"
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }
        
        # åˆå§‹åŒ–Python SDKå®¢æˆ·ç«¯
        self.rag_client = RAGFlow(api_key=api_key, base_url=base_url)
    
    def retrieve_chunks_http_api(
        self,
        question: str,
        dataset_ids: Optional[List[str]] = None,
        document_ids: Optional[List[str]] = None,
        page: int = 1,
        page_size: int = 30,
        similarity_threshold: float = 0.2,
        vector_similarity_weight: float = 0.3,
        top_k: int = 1024,
        rerank_id: Optional[str] = None,
        keyword: bool = False,
        highlight: bool = False
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨HTTP APIè°ƒç”¨å¬å›æ¥å£ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            dataset_ids: æ•°æ®é›†IDåˆ—è¡¨
            document_ids: æ–‡æ¡£IDåˆ—è¡¨
            page: é¡µç 
            page_size: æ¯é¡µå¤§å°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            vector_similarity_weight: å‘é‡ç›¸ä¼¼åº¦æƒé‡
            top_k: å€™é€‰chunkæ•°é‡
            rerank_id: é‡æ’æ¨¡å‹ID
            keyword: æ˜¯å¦å¯ç”¨å…³é”®è¯åŒ¹é…
            highlight: æ˜¯å¦é«˜äº®åŒ¹é…è¯
            
        Returns:
            å¬å›ç»“æœå­—å…¸
        """
        url = f"{self.base_url}/api/v1/retrieval"
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "question": question,
            "page": page,
            "page_size": page_size,
            "similarity_threshold": similarity_threshold,
            "vector_similarity_weight": vector_similarity_weight,
            "top_k": top_k,
            "keyword": keyword,
            "highlight": highlight
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if dataset_ids:
            data["dataset_ids"] = dataset_ids
        if document_ids:
            data["document_ids"] = document_ids
        if rerank_id:
            data["rerank_id"] = rerank_id
        
        try:
            # ä¼˜åŒ–ï¼šè®¾ç½®é€‚å½“çš„è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶
            response = requests.post(
                url, 
                headers=self.headers, 
                json=data, 
                timeout=15,  # 15ç§’è¶…æ—¶
                stream=False  # ç¦ç”¨æµå¼ä¼ è¾“ä»¥æé«˜æ•ˆç‡
            )
            response.raise_for_status()
            
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„JSONè§£æ
            result = response.json()
            
            # åŸºæœ¬éªŒè¯è¿”å›ç»“æœ
            if not isinstance(result, dict):
                return {"error": "Invalid response format"}
                
            return result
            
        except requests.exceptions.Timeout:
            return {"error": "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"}
        except requests.exceptions.ConnectionError:
            return {"error": "è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"}
        except requests.exceptions.HTTPError as e:
            return {"error": f"HTTPé”™è¯¯: {e.response.status_code}"}
        except requests.exceptions.RequestException as e:
            return {"error": f"è¯·æ±‚é”™è¯¯: {str(e)}"}
        except json.JSONDecodeError as e:
            return {"error": f"JSONè§£æé”™è¯¯: {str(e)}"}
        except Exception as e:
            return {"error": f"æœªçŸ¥é”™è¯¯: {str(e)}"}
    
    def retrieve_chunks_advanced(
        self,
        question: str,
        dataset_ids: List[str],
        similarity_threshold: float = 0.2,
        top_k: int = 10,
        enable_rerank: bool = False,
        enable_keyword: bool = False,
        enable_highlight: bool = False
    ) -> Dict[str, Any]:
        """
        é«˜çº§å¬å›æ¥å£ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            dataset_ids: çŸ¥è¯†åº“IDåˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
            top_k: è¿”å›çš„chunkæ•°é‡
            enable_rerank: æ˜¯å¦å¯ç”¨é‡æ’
            enable_keyword: æ˜¯å¦å¯ç”¨å…³é”®è¯åŒ¹é…
            enable_highlight: æ˜¯å¦å¯ç”¨é«˜äº®æ˜¾ç¤º
            
        Returns:
            æ ¼å¼åŒ–çš„å¬å›ç»“æœ
        """
        # è¾“å…¥éªŒè¯
        if not question or not question.strip():
            return {"error": "æŸ¥è¯¢é—®é¢˜ä¸èƒ½ä¸ºç©º", "chunks": []}
        
        if not dataset_ids:
            return {"error": "æ•°æ®é›†IDä¸èƒ½ä¸ºç©º", "chunks": []}
        
        result = self.retrieve_chunks_http_api(
            question=question.strip(),
            dataset_ids=dataset_ids,
            page_size=min(top_k, 50),  # é™åˆ¶æœ€å¤§è¿”å›æ•°é‡
            similarity_threshold=max(0.0, min(1.0, similarity_threshold)),  # ç¡®ä¿é˜ˆå€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
            keyword=enable_keyword,
            highlight=enable_highlight
        )
        
        if "error" in result:
            return {"error": result["error"], "chunks": []}
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        if result.get("code") == 0 and "data" in result:
            data = result["data"]
            formatted_result = {
                "question": question,
                "total_chunks": data.get("total", 0),
                "chunks": [],
                "document_stats": data.get("doc_aggs", [])
            }
            
            # ä¼˜åŒ–ï¼šé¢„åˆ†é…åˆ—è¡¨å¤§å°
            chunks_data = data.get("chunks", [])
            formatted_result["chunks"] = []
            
            # æ ¼å¼åŒ–chunkä¿¡æ¯
            for chunk in chunks_data:
                # è·³è¿‡æ— å†…å®¹çš„chunk
                content = chunk.get("content")
                if not content or not content.strip():
                    continue
                    
                formatted_chunk = {
                    "id": chunk.get("id"),
                    "content": content.strip(),
                    "document_name": chunk.get("document_keyword", ""),
                    "document_id": chunk.get("document_id", ""),
                    "similarity_score": float(chunk.get("similarity", 0)),
                    "vector_similarity": float(chunk.get("vector_similarity", 0)),
                    "term_similarity": float(chunk.get("term_similarity", 0)),
                    "highlighted_content": chunk.get("highlight", ""),
                    "important_keywords": chunk.get("important_keywords", [])
                }
                formatted_result["chunks"].append(formatted_chunk)
            
            return formatted_result
        else:
            return {"error": result.get("message", "Unknown error"), "chunks": []}
    
    def batch_retrieve(
        self,
        questions: List[str],
        dataset_ids: List[str],
        similarity_threshold: float = 0.2,
        top_k: int = 5,
        max_workers: int = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¬å›æŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            dataset_ids: çŸ¥è¯†åº“IDåˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: æ¯ä¸ªé—®é¢˜è¿”å›çš„chunkæ•°é‡
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼ˆç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©ï¼‰
            
        Returns:
            æ‰¹é‡å¬å›ç»“æœåˆ—è¡¨
        """
        if not questions:
            return []
        
        # å»é‡é—®é¢˜ï¼Œé¿å…é‡å¤æŸ¥è¯¢
        unique_questions = list(dict.fromkeys(questions))  # ä¿æŒé¡ºåºçš„å»é‡
        question_to_result = {}
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå”¯ä¸€é—®é¢˜ï¼Œç›´æ¥è°ƒç”¨å•ä¸ªæŸ¥è¯¢
        if len(unique_questions) == 1:
            result = self.retrieve_chunks_advanced(
                question=unique_questions[0],
                dataset_ids=dataset_ids,
                similarity_threshold=similarity_threshold,
                top_k=top_k
            )
            question_to_result[unique_questions[0]] = result
        else:
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´åˆç†çš„çº¿ç¨‹æ•°
            optimal_workers = min(max_workers or 4, len(unique_questions), 8)
            
            # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¤šçº¿ç¨‹å¤„ç†
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_question = {
                    executor.submit(
                        self.retrieve_chunks_advanced,
                        question=question,
                        dataset_ids=dataset_ids,
                        similarity_threshold=similarity_threshold,
                        top_k=top_k
                    ): question
                    for question in unique_questions
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_question):
                    question = future_to_question[future]
                    try:
                        result = future.result(timeout=30)  # æ·»åŠ è¶…æ—¶æœºåˆ¶
                        question_to_result[question] = result
                    except Exception as exc:
                        print(f'é—®é¢˜ "{question}" å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}')
                        question_to_result[question] = {"error": f"å¤„ç†å¼‚å¸¸: {exc}", "question": question}
        
        # æŒ‰åŸå§‹é¡ºåºè¿”å›ç»“æœï¼ˆå¤„ç†é‡å¤é—®é¢˜ï¼‰
        results = []
        for question in questions:
            results.append(question_to_result.get(question, {"error": "Unknown error", "question": question}))
        
        return results
    
    def search_in_specific_documents(
        self,
        question: str,
        document_ids: List[str],
        similarity_threshold: float = 0.1,
        top_k: int = 10
    ) -> Dict[str, Any]:
        """
        åœ¨æŒ‡å®šæ–‡æ¡£ä¸­æœç´¢
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            document_ids: æ–‡æ¡£IDåˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æœç´¢ç»“æœ
        """
        return self.retrieve_chunks_http_api(
            question=question,
            document_ids=document_ids,
            page_size=top_k,
            similarity_threshold=similarity_threshold,
            keyword=True,
            highlight=True
        )
    
    def print_retrieval_results(self, results: Dict[str, Any]):
        """
        æ‰“å°å¬å›ç»“æœ
        
        Args:
            results: å¬å›ç»“æœå­—å…¸
        """
        if "error" in results:
            print(f"âŒ é”™è¯¯: {results['error']}")
            return
        
        print(f"ğŸ” æŸ¥è¯¢é—®é¢˜: {results.get('question', 'N/A')}")
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {results.get('total_chunks', 0)} ä¸ªç›¸å…³chunk")
        print(f"ğŸ“„ æ¶‰åŠæ–‡æ¡£: {len(results.get('document_stats', []))} ä¸ª")
        
        print("\n" + "="*60)
        print("ğŸ“‹ å¬å›ç»“æœè¯¦æƒ…:")
        print("="*60)
        
        for i, chunk in enumerate(results.get('chunks', []), 1):
            print(f"\nğŸ”¹ Chunk {i}:")
            print(f"   ID: {chunk.get('id', 'N/A')}")
            print(f"   æ–‡æ¡£: {chunk.get('document_name', 'N/A')}")
            print(f"   ç›¸ä¼¼åº¦: {chunk.get('similarity_score', 0):.4f}")
            print(f"   å‘é‡ç›¸ä¼¼åº¦: {chunk.get('vector_similarity', 0):.4f}")
            print(f"   è¯æ±‡ç›¸ä¼¼åº¦: {chunk.get('term_similarity', 0):.4f}")
            
            content = chunk.get('content', '')
            if len(content) > 200:
                content = content[:200] + "..."
            print(f"   å†…å®¹: {content}")
            
            if chunk.get('highlighted_content'):
                highlighted = chunk.get('highlighted_content', '')
                if len(highlighted) > 200:
                    highlighted = highlighted[:200] + "..."
                print(f"   é«˜äº®: {highlighted}")
            
            if chunk.get('important_keywords'):
                print(f"   å…³é”®è¯: {', '.join(chunk.get('important_keywords', []))}")
        
        print("\n" + "="*60)
        print("ğŸ“ˆ æ–‡æ¡£ç»Ÿè®¡:")
        print("="*60)
        for doc_stat in results.get('document_stats', []):
            print(f"   ğŸ“„ {doc_stat.get('doc_name', 'N/A')}: {doc_stat.get('count', 0)} ä¸ªchunk")

    def find_dataset_id(self) -> str:
        # è·å–çŸ¥è¯†åº“åˆ—è¡¨
        try:
            datasets = self.rag_client.list_datasets()
            if not datasets:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•çŸ¥è¯†åº“ï¼Œè¯·å…ˆåˆ›å»ºçŸ¥è¯†åº“å¹¶ä¸Šä¼ æ–‡æ¡£")
                return None
            
            print(f"ğŸ“š æ‰¾åˆ° {len(datasets)} ä¸ªçŸ¥è¯†åº“:")
            for i, dataset in enumerate(datasets, 1):
                print(f"   {i}. {getattr(dataset, 'name', 'N/A')} (ID: {getattr(dataset, 'id', 'N/A')})")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªçŸ¥è¯†åº“è¿›è¡Œæµ‹è¯•
            dataset_id = getattr(datasets[0], 'id', '')
            dataset_name = getattr(datasets[0], 'name', 'N/A')
            
            if not dataset_id:
                print("âŒ æ— æ³•è·å–çŸ¥è¯†åº“ID")
                return None
            
            print(f"\nğŸ¯ ä½¿ç”¨çŸ¥è¯†åº“: {dataset_name} (ID: {dataset_id})")
            
        except Exception as e:
            print(f"âŒ è·å–çŸ¥è¯†åº“å¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    
    # é…ç½®RAGFlowä¿¡æ¯
    API_KEY = "ragflow-FiOTQ1MGI0MWMyNDExZjA4N2VlMDI0Mm"  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    BASE_URL = "http://172.18.81.4:8080"  # æ›¿æ¢ä¸ºä½ çš„RAGFlowæœåŠ¡å™¨åœ°å€
    DATASET_ID = "01509e3e62cd11f0b66d0242ac140006"
    
    # åˆ›å»ºå¬å›å®¢æˆ·ç«¯
    retrieval = RAGFlowRetrieval(api_key=API_KEY, base_url=BASE_URL)
    
    # ç¤ºä¾‹1: åŸºç¡€å¬å›æŸ¥è¯¢
    print("ğŸ“ ç¤ºä¾‹: åŸºç¡€å¬å›æŸ¥è¯¢")
    print("="*60)
    
    question1 = "å¼€å­¦æ—¶é—´"  # æ›¿æ¢ä¸ºä½ æƒ³æŸ¥è¯¢çš„é—®é¢˜
    results1 = retrieval.retrieve_chunks_advanced(
        question=question1,
        dataset_ids=[DATASET_ID],
        similarity_threshold=0.5,
        top_k=6,
        enable_keyword=False,
        enable_highlight=False
    )
    
    retrieval.print_retrieval_results(results1)
    
    # ç¤ºä¾‹2: æ‰¹é‡å¬å›æŸ¥è¯¢ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
    print("\n\nğŸ“ ç¤ºä¾‹: æ‰¹é‡å¬å›æŸ¥è¯¢ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰")
    print("="*60)
    
    questions = [
        "æ ¡å†…vpn",
        "å¦‚ä½•è¿æ¥æ ¡å›­ç½‘",
        "å›¾ä¹¦é¦†å¼€æ”¾æ—¶é—´",
        "å­¦ç”Ÿå®¿èˆç®¡ç†è§„å®š",
        "è¯¾ç¨‹é€‰è¯¾ç³»ç»Ÿ"
    ]
    
    print(f"ğŸš€ æ­£åœ¨å¹¶å‘å¤„ç† {len(questions)} ä¸ªé—®é¢˜...")
    start_time = time.time()
    
    batch_results = retrieval.batch_retrieve(
        questions=questions,
        dataset_ids=[DATASET_ID],
        similarity_threshold=0.1,
        top_k=3,
        max_workers=3  # è®¾ç½®æœ€å¤§3ä¸ªçº¿ç¨‹
    )
    
    end_time = time.time()
    print(f"â±ï¸ æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"ğŸ“Š å¤„ç†äº† {len(batch_results)} ä¸ªæŸ¥è¯¢ç»“æœ")
    
    # æ‰“å°æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœæ‘˜è¦
    for i, result in enumerate(batch_results, 1):
        if "error" in result:
            print(f"âŒ æŸ¥è¯¢ {i}: {result['error']}")
        else:
            chunks_count = len(result.get('chunks', []))
            question = result.get('question', questions[i-1])
            print(f"âœ… æŸ¥è¯¢ {i} ('{question}'): æ‰¾åˆ° {chunks_count} ä¸ªç›¸å…³chunk")


if __name__ == "__main__":
    main()