#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•ä¼˜åŒ–åçš„RAGæ£€ç´¢æ€§èƒ½
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any
from rag import KB_Retrieval
from llm_server import LLM


class RAGBenchmark:
    """RAGæ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.rag_with_cache = KB_Retrieval(enable_cache=True)
        self.rag_without_cache = KB_Retrieval(enable_cache=False)
        self.llm = LLM(enable_cache=True)
        
        # æµ‹è¯•æŸ¥è¯¢é›†
        self.test_queries = [
            ["å­¦è´¹ç¼´çº³", "å¦‚ä½•ç¼´è´¹", "ç¼´è´¹æ”¿ç­–"],
            ["å›¾ä¹¦é¦†å¼€æ”¾æ—¶é—´", "å€Ÿä¹¦è§„å®š"],
            ["é€‰è¯¾ç³»ç»Ÿ", "è¯¾ç¨‹å®‰æ’", "å­¦åˆ†è¦æ±‚"],
            ["å®¿èˆç®¡ç†", "ä½å®¿ç”³è¯·", "å®¿èˆè§„å®š"],
            ["æ ¡å›­ç½‘", "ç½‘ç»œè¿æ¥", "VPNä½¿ç”¨"],
            ["å­¦ç”Ÿè¯åŠç†", "è¯ä»¶ç”³è¯·"],
            ["é£Ÿå ‚å°±é¤", "é¤å…ä½ç½®", "ç”¨é¤æ—¶é—´"],
            ["è€ƒè¯•å®‰æ’", "æˆç»©æŸ¥è¯¢", "è¡¥è€ƒæ”¿ç­–"],
            ["å¥–å­¦é‡‘ç”³è¯·", "åŠ©å­¦é‡‘", "å‹¤å·¥åŠ©å­¦"],
            ["æ¯•ä¸šè¦æ±‚", "å­¦ä½ç”³è¯·", "æ¯•ä¸šæµç¨‹"]
        ]
    
    def benchmark_retrieval_speed(self, use_cache: bool = True, runs: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•æ£€ç´¢é€Ÿåº¦"""
        rag_client = self.rag_with_cache if use_cache else self.rag_without_cache
        
        times = []
        cache_hits = 0
        
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•æ£€ç´¢æ€§èƒ½ ({'å¯ç”¨ç¼“å­˜' if use_cache else 'ç¦ç”¨ç¼“å­˜'})")
        print(f"{'='*50}")
        
        for run in range(runs):
            print(f"\nè¿è¡Œ {run + 1}/{runs}")
            run_times = []
            
            for i, query in enumerate(self.test_queries):
                start_time = time.time()
                
                try:
                    result = rag_client.retrieve(query)
                    end_time = time.time()
                    
                    duration = end_time - start_time
                    run_times.append(duration)
                    
                    # æ£€æµ‹æ˜¯å¦å‘½ä¸­ç¼“å­˜ï¼ˆç¬¬äºŒæ¬¡æŸ¥è¯¢ç›¸åŒé—®é¢˜ï¼‰
                    if run > 0 and use_cache:
                        cache_start = time.time()
                        rag_client.retrieve(query)
                        cache_duration = time.time() - cache_start
                        if cache_duration < duration * 0.1:  # å¦‚æœæ—¶é—´å‡å°‘90%ä»¥ä¸Šï¼Œè®¤ä¸ºå‘½ä¸­ç¼“å­˜
                            cache_hits += 1
                    
                    print(f"  æŸ¥è¯¢ {i+1}: {duration:.3f}s - {len(result)} chars")
                    
                except Exception as e:
                    print(f"  æŸ¥è¯¢ {i+1} å¤±è´¥: {e}")
                    run_times.append(float('inf'))
            
            times.extend(run_times)
            print(f"  æœ¬è½®å¹³å‡æ—¶é—´: {statistics.mean(run_times):.3f}s")
        
        # è¿‡æ»¤æ— æ•ˆç»“æœ
        valid_times = [t for t in times if t != float('inf')]
        
        if not valid_times:
            return {"error": "æ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥äº†"}
        
        return {
            "cache_enabled": use_cache,
            "total_queries": len(valid_times),
            "cache_hits": cache_hits,
            "avg_time": statistics.mean(valid_times),
            "median_time": statistics.median(valid_times),
            "min_time": min(valid_times),
            "max_time": max(valid_times),
            "std_dev": statistics.stdev(valid_times) if len(valid_times) > 1 else 0,
            "success_rate": len(valid_times) / len(times) * 100
        }
    
    async def benchmark_async_retrieval(self, runs: int = 2) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚æ­¥æ£€ç´¢æ€§èƒ½"""
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•å¼‚æ­¥æ£€ç´¢æ€§èƒ½")
        print(f"{'='*50}")
        
        times = []
        
        for run in range(runs):
            print(f"\nå¼‚æ­¥è¿è¡Œ {run + 1}/{runs}")
            start_time = time.time()
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
            tasks = [
                self.rag_with_cache.async_retrieve(query) 
                for query in self.test_queries
            ]
            
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                end_time = time.time()
                
                duration = end_time - start_time
                times.append(duration)
                
                success_count = sum(1 for r in results if not isinstance(r, Exception))
                print(f"  å¹¶å‘æ‰§è¡Œ {len(tasks)} ä¸ªæŸ¥è¯¢: {duration:.3f}s")
                print(f"  æˆåŠŸ: {success_count}/{len(tasks)}")
                
            except Exception as e:
                print(f"  å¼‚æ­¥æ‰§è¡Œå¤±è´¥: {e}")
        
        if not times:
            return {"error": "å¼‚æ­¥æµ‹è¯•å¤±è´¥"}
        
        return {
            "concurrent_queries": len(self.test_queries),
            "avg_total_time": statistics.mean(times),
            "avg_time_per_query": statistics.mean(times) / len(self.test_queries),
            "runs": runs
        }
    
    def benchmark_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤§é‡æŸ¥è¯¢
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ")
        print(f"{'='*50}")
        
        print(f"åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        for i in range(5):
            for query in self.test_queries:
                self.rag_with_cache.retrieve(query)
        
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"å³°å€¼å†…å­˜: {peak_memory:.2f} MB")
        
        # æ¸…ç©ºç¼“å­˜
        self.rag_with_cache.clear_cache()
        
        after_clear_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"æ¸…é™¤ç¼“å­˜å: {after_clear_memory:.2f} MB")
        
        return {
            "initial_memory_mb": initial_memory,
            "peak_memory_mb": peak_memory,
            "after_clear_memory_mb": after_clear_memory,
            "memory_increase_mb": peak_memory - initial_memory,
            "memory_freed_mb": peak_memory - after_clear_memory
        }
    
    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        results = {}
        
        # æµ‹è¯•æœ‰ç¼“å­˜çš„æ€§èƒ½
        results["with_cache"] = self.benchmark_retrieval_speed(use_cache=True, runs=3)
        
        # æµ‹è¯•æ— ç¼“å­˜çš„æ€§èƒ½
        results["without_cache"] = self.benchmark_retrieval_speed(use_cache=False, runs=2)
        
        # æµ‹è¯•å¼‚æ­¥æ€§èƒ½
        results["async_performance"] = asyncio.run(self.benchmark_async_retrieval(runs=2))
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        results["memory_usage"] = self.benchmark_memory_usage()
        
        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        self.print_summary_report(results)
        
        return results
    
    def print_summary_report(self, results: Dict[str, Any]):
        """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print(f"RAGæ€§èƒ½æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
        print(f"{'='*60}")
        
        if "with_cache" in results and "without_cache" in results:
            with_cache = results["with_cache"]
            without_cache = results["without_cache"]
            
            if "avg_time" in with_cache and "avg_time" in without_cache:
                speedup = without_cache["avg_time"] / with_cache["avg_time"]
                print(f"\nğŸ“Š æ£€ç´¢æ€§èƒ½å¯¹æ¯”:")
                print(f"  ç¼“å­˜å¯ç”¨æ—¶å¹³å‡å“åº”æ—¶é—´: {with_cache['avg_time']:.3f}s")
                print(f"  ç¼“å­˜ç¦ç”¨æ—¶å¹³å‡å“åº”æ—¶é—´: {without_cache['avg_time']:.3f}s")
                print(f"  æ€§èƒ½æå‡å€æ•°: {speedup:.2f}x")
                print(f"  ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {with_cache.get('cache_hits', 0)}")
        
        if "async_performance" in results:
            async_perf = results["async_performance"]
            if "avg_time_per_query" in async_perf:
                print(f"\nğŸš€ å¼‚æ­¥æ€§èƒ½:")
                print(f"  å¹¶å‘æŸ¥è¯¢å¹³å‡å•æ¬¡æ—¶é—´: {async_perf['avg_time_per_query']:.3f}s")
                print(f"  å¹¶å‘æ‰§è¡Œ {async_perf['concurrent_queries']} ä¸ªæŸ¥è¯¢æ€»æ—¶é—´: {async_perf['avg_total_time']:.3f}s")
        
        if "memory_usage" in results:
            memory = results["memory_usage"]
            print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨:")
            print(f"  å†…å­˜å¢é•¿: {memory['memory_increase_mb']:.2f} MB")
            print(f"  ç¼“å­˜é‡Šæ”¾: {memory['memory_freed_mb']:.2f} MB")
            print(f"  å³°å€¼å†…å­˜: {memory['peak_memory_mb']:.2f} MB")
        
        print(f"\nâœ… ä¼˜åŒ–å»ºè®®:")
        
        if "with_cache" in results and results["with_cache"].get("cache_hits", 0) > 0:
            print("  âœ“ ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼Œæ˜¾è‘—æå‡äº†é‡å¤æŸ¥è¯¢çš„æ€§èƒ½")
        
        if "async_performance" in results:
            print("  âœ“ å¼‚æ­¥æŸ¥è¯¢æ”¯æŒå¹¶å‘å¤„ç†ï¼Œé€‚åˆæ‰¹é‡æŸ¥è¯¢åœºæ™¯")
        
        print("  âœ“ å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨ç¼“å­˜ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        print("  âœ“ å¯¹äºé«˜å¹¶å‘åœºæ™¯ï¼Œä½¿ç”¨å¼‚æ­¥æ¥å£å¯ä»¥æå‡ååé‡")


def main():
    """ä¸»å‡½æ•°"""
    benchmark = RAGBenchmark()
    results = benchmark.run_full_benchmark()
    
    # å¯é€‰ï¼šå°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    import json
    with open("rag_benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° rag_benchmark_results.json")


if __name__ == "__main__":
    main()