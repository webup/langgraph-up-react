{
  "id": "f799772a-2faf-4f79-bfb3-35caa2ecda29",
  "messages": [
    {
      "content": "我的绩点",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "human",
      "name": null,
      "id": "ac7e022e-8102-46cf-9fa7-0cc1829d4644",
      "example": false
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_4bdbb166d3ca48c0841170",
            "function": {
              "arguments": "{}",
              "name": "grade_query"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 15,
          "prompt_tokens": 2992,
          "total_tokens": 3007,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-58c21c44-6ccb-962d-86b3-1bea4ffd004e",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--fb8e13e8-1f0d-470e-b95b-80a201e8f340-0",
      "example": false,
      "tool_calls": [
        {
          "name": "grade_query",
          "args": {},
          "id": "call_4bdbb166d3ca48c0841170",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 2992,
        "output_tokens": 15,
        "total_tokens": 3007,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "线性代数：90\n    高等数学：85\n    大学英语：88\n    体育：92\n    思想政治理论：89\n    军事训练：91\n    军事理论：88",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "grade_query",
      "id": "97e138db-1500-4055-9c41-059265705f07",
      "tool_call_id": "call_4bdbb166d3ca48c0841170",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "根据您提供的课程成绩，我将为您计算平均绩点。\n\n按照重庆大学常见的绩点换算标准（百分制成绩对应绩点）：\n- 90分及以上：4.0绩点\n- 85-89分：3.7绩点\n- 80-84分：3.3绩点\n- 75-79分：3.0绩点\n- 70-74分：2.7绩点\n- 65-69分：2.3绩点\n- 60-64分：2.0绩点\n- 60分以下：0绩点\n\n您的各科绩点换算如下：\n- 线性代数：90分 → 4.0\n- 高等数学：85分 → 3.7\n- 大学英语：88分 → 3.7\n- 体育：92分 → 4.0\n- 思想政治理论：89分 → 3.7\n- 军事训练：91分 → 4.0\n- 军事理论：88分 → 3.7\n\n总绩点 = (4.0 + 3.7 + 3.7 + 4.0 + 3.7 + 4.0 + 3.7) / 7 = 26.8 / 7 ≈ 3.83\n\n您的平均绩点约为 **3.83**。\n\n请注意：此计算为简单平均绩点，未考虑课程学分权重。如需精确的加权平均绩点，请提供各课程的学分信息。重庆大学官方的绩点计算方法可能会有具体规定，建议通过教务系统查询官方认定的绩点。",
      "additional_kwargs": {
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 393,
          "prompt_tokens": 3082,
          "total_tokens": 3475,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-56bbc5b7-6a6b-9ff4-8f6b-4a570a60ad52",
        "service_tier": null,
        "finish_reason": "stop",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--8f3d392f-ca4c-4a81-9ac0-793ea2474115-0",
      "example": false,
      "tool_calls": [],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 3082,
        "output_tokens": 393,
        "total_tokens": 3475,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "我的高数成绩",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "human",
      "name": null,
      "id": "601cfe03-590d-4172-a415-d6ba64b1a15c",
      "example": false
    },
    {
      "content": "您的高等数学成绩是 **85分**。",
      "additional_kwargs": {
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 11,
          "prompt_tokens": 3489,
          "total_tokens": 3500,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-377ed06f-043f-9eb4-b015-963122d2aeff",
        "service_tier": null,
        "finish_reason": "stop",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--36c6ba2c-92a5-4c4f-a76f-cde0a5cec999-0",
      "example": false,
      "tool_calls": [],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 3489,
        "output_tokens": 11,
        "total_tokens": 3500,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "分析一下我的高数成绩",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "human",
      "name": null,
      "id": "d1ac5c2c-cc6f-4287-9c18-9941f7e75180",
      "example": false
    },
    {
      "content": "您的高等数学成绩为 **85分**，属于良好水平。根据重庆大学常见的成绩等级划分：\n\n- **成绩定位**：85分处于“良好”区间（通常80-89分为良好），接近“优秀”等级（90分及以上），说明您对高等数学课程的内容掌握较为扎实，具备较强的逻辑思维和数学运算能力。\n\n- **提升空间**：距离90分的优秀线还有5分差距，可能在某些难点章节（如多元函数微分学、重积分、曲线曲面积分或级数等）存在理解不够深入或解题技巧有待提高的地方。\n\n- **建议**：\n  1. 回顾试卷，分析失分点，重点关注错题类型。\n  2. 针对薄弱环节进行专项练习，可参考教材习题、往年真题或参加数学辅导班。\n  3. 若后续有数学相关课程（如线性代数、概率统计、数学物理方程等），85分的基础将为您提供有力支持。\n\n如需更具体的分析，建议咨询任课老师或助教，获取详细的试卷讲评。继续保持努力，争取在后续课程中取得更优异的成绩！",
      "additional_kwargs": {
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 255,
          "prompt_tokens": 3516,
          "total_tokens": 3771,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-7c8760f0-14dd-9c50-9a34-5ff46ffd6259",
        "service_tier": null,
        "finish_reason": "stop",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--c71fdfc5-9311-47ef-839c-4748472269e7-0",
      "example": false,
      "tool_calls": [],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 3516,
        "output_tokens": 255,
        "total_tokens": 3771,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "北京到重庆的直线距离",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "human",
      "name": null,
      "id": "8db74f93-98ed-4acf-a765-a6fcde714bf8",
      "example": false
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_d4d32858c4bb4d4ea2b145",
            "function": {
              "arguments": "{\"address\": \"北京市\"}",
              "name": "maps_geo"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 19,
          "prompt_tokens": 3787,
          "total_tokens": 3806,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-cb9b15bb-5451-91bb-9835-1b8eea6082c8",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--02a330f9-3e75-4dd5-b9f0-a32a3d7be91d-0",
      "example": false,
      "tool_calls": [
        {
          "name": "maps_geo",
          "args": {
            "address": "北京市"
          },
          "id": "call_d4d32858c4bb4d4ea2b145",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 3787,
        "output_tokens": 19,
        "total_tokens": 3806,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "{\"results\":[{\"country\":\"中国\",\"province\":\"北京市\",\"city\":\"北京市\",\"citycode\":\"010\",\"district\":[],\"street\":[],\"number\":[],\"adcode\":\"110000\",\"location\":\"116.407387,39.904179\",\"level\":\"省\"}]}",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "maps_geo",
      "id": "2410d628-5d1a-48ba-83a6-f9e3d00d0aae",
      "tool_call_id": "call_d4d32858c4bb4d4ea2b145",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_d5ec0c23323b4bb994f79f",
            "function": {
              "arguments": "{\"address\": \"重庆市\"}",
              "name": "maps_geo"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 19,
          "prompt_tokens": 3888,
          "total_tokens": 3907,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-993949ef-0268-9c11-886f-80ca157b46be",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--d992a6aa-5c2e-4787-990f-549d0ca6f405-0",
      "example": false,
      "tool_calls": [
        {
          "name": "maps_geo",
          "args": {
            "address": "重庆市"
          },
          "id": "call_d5ec0c23323b4bb994f79f",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 3888,
        "output_tokens": 19,
        "total_tokens": 3907,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "{\"results\":[{\"country\":\"中国\",\"province\":\"重庆市\",\"city\":\"重庆市\",\"citycode\":\"023\",\"district\":[],\"street\":[],\"number\":[],\"adcode\":\"500000\",\"location\":\"106.550483,29.563707\",\"level\":\"省\"}]}",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "maps_geo",
      "id": "b2fdf1fd-f676-471f-aa4c-496a31c9b071",
      "tool_call_id": "call_d5ec0c23323b4bb994f79f",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_2bffd22a2a9f4ada93d037",
            "function": {
              "arguments": "{\"origins\": \"116.407387,39.904179\", \"destination\": \"106.550483,29.563707\", \"type\": \"0\"}",
              "name": "maps_distance"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 3989,
          "total_tokens": 4059,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-c1cb09d8-3e1a-9e33-8f33-c6255e562c89",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--f26d8245-fd0c-40eb-bad0-0bc335b1a240-0",
      "example": false,
      "tool_calls": [
        {
          "name": "maps_distance",
          "args": {
            "origins": "116.407387,39.904179",
            "destination": "106.550483,29.563707",
            "type": "0"
          },
          "id": "call_2bffd22a2a9f4ada93d037",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 3989,
        "output_tokens": 70,
        "total_tokens": 4059,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "{\"results\":[{\"origin_id\":\"1\",\"dest_id\":\"1\",\"distance\":\"1460172\",\"duration\":\"0\"}]}",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "maps_distance",
      "id": "4afb7610-3349-4f51-b99e-dea8dc58348c",
      "tool_call_id": "call_2bffd22a2a9f4ada93d037",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "北京到重庆的直线距离约为 **1,460.17公里**。\n\n该距离是基于两地地理中心坐标的球面距离计算得出：\n- 北京市地理坐标：116.407387, 39.904179\n- 重庆市地理坐标：106.550483, 29.563707\n\n此为理论直线距离（大圆距离），实际交通出行距离会因路线和交通方式不同而有所增加。",
      "additional_kwargs": {
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 4100,
          "total_tokens": 4217,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-ff1707c1-6582-94b5-9fb4-123c5691910b",
        "service_tier": null,
        "finish_reason": "stop",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--887df3dc-2556-44d1-9693-5a154742bd5d-0",
      "example": false,
      "tool_calls": [],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 4100,
        "output_tokens": 117,
        "total_tokens": 4217,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "我想查找一下langgraph中关于多agent的相关介绍",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "human",
      "name": null,
      "id": "121c40d1-85da-4fd1-8c65-00ed7003ea5b",
      "example": false
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_337481d39cc44eceba4154",
            "function": {
              "arguments": "{\"repoName\": \"langchain-ai/langgraph\"}",
              "name": "read_wiki_structure"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 27,
          "prompt_tokens": 4238,
          "total_tokens": 4265,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-e46e25da-efee-913a-8fd3-59d990c87552",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--4cbdf6b6-e08f-4b9f-b50c-97d9ead869af-0",
      "example": false,
      "tool_calls": [
        {
          "name": "read_wiki_structure",
          "args": {
            "repoName": "langchain-ai/langgraph"
          },
          "id": "call_337481d39cc44eceba4154",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 4238,
        "output_tokens": 27,
        "total_tokens": 4265,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "Available pages for langchain-ai/langgraph:\n\n- 1 Overview\n- 2 Core Architecture\n  - 2.1 StateGraph API\n  - 2.2 Pregel Runtime Engine\n  - 2.3 Functional API\n  - 2.4 Graph Utilities and Configuration\n- 3 State Management and Channels\n- 4 Human-in-the-Loop and Control Flow\n- 5 Persistence System\n  - 5.1 Checkpointing\n  - 5.2 Store System\n  - 5.3 Serialization\n- 6 Client-Server Architecture\n  - 6.1 RemoteGraph Client\n  - 6.2 Python SDK\n  - 6.3 JavaScript SDK\n  - 6.4 React UI Components\n- 7 Deployment and Platform\n  - 7.1 CLI Tool\n  - 7.2 LangGraph Platform\n  - 7.3 Self-Hosted Deployment\n- 8 Authentication and Authorization\n- 9 Prebuilt Components\n- 10 Development and Testing\n  - 10.1 Documentation System\n  - 10.2 CI/CD Pipeline\n  - 10.3 Testing Framework\n- 11 Examples and Tutorials",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "read_wiki_structure",
      "id": "23ae87da-98bd-4e03-9982-5b5f88e5639a",
      "tool_call_id": "call_337481d39cc44eceba4154",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_d9dd5ec5a5e74e0da8ed28",
            "function": {
              "arguments": "{\"repoName\": \"langchain-ai/langgraph\", \"page\": \"9 Prebuilt Components\"}",
              "name": "read_wiki_contents"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 36,
          "prompt_tokens": 4541,
          "total_tokens": 4577,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-706755b5-4464-95a0-b60b-9fac3f45fd99",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--4a091289-2140-4bd1-8d71-9ec7d6c72ea9-0",
      "example": false,
      "tool_calls": [
        {
          "name": "read_wiki_contents",
          "args": {
            "repoName": "langchain-ai/langgraph",
            "page": "9 Prebuilt Components"
          },
          "id": "call_d9dd5ec5a5e74e0da8ed28",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 4541,
        "output_tokens": 36,
        "total_tokens": 4577,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "[\"# Page: Overview\\n\\n# Overview\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [Makefile](Makefile)\\n- [README.md](README.md)\\n- [libs/checkpoint-postgres/uv.lock](libs/checkpoint-postgres/uv.lock)\\n- [libs/checkpoint-sqlite/uv.lock](libs/checkpoint-sqlite/uv.lock)\\n- [libs/checkpoint/langgraph/cache/redis/__init__.py](libs/checkpoint/langgraph/cache/redis/__init__.py)\\n- [libs/checkpoint/pyproject.toml](libs/checkpoint/pyproject.toml)\\n- [libs/checkpoint/uv.lock](libs/checkpoint/uv.lock)\\n- [libs/langgraph/Makefile](libs/langgraph/Makefile)\\n- [libs/langgraph/README.md](libs/langgraph/README.md)\\n- [libs/langgraph/pyproject.toml](libs/langgraph/pyproject.toml)\\n- [libs/langgraph/tests/compose-redis.yml](libs/langgraph/tests/compose-redis.yml)\\n- [libs/langgraph/tests/conftest.py](libs/langgraph/tests/conftest.py)\\n- [libs/langgraph/uv.lock](libs/langgraph/uv.lock)\\n- [libs/prebuilt/Makefile](libs/prebuilt/Makefile)\\n- [libs/prebuilt/pyproject.toml](libs/prebuilt/pyproject.toml)\\n- [libs/prebuilt/uv.lock](libs/prebuilt/uv.lock)\\n\\n</details>\\n\\n\\n\\n## What is LangGraph?\\n\\nLangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents and multi-actor applications with Large Language Models (LLMs). Unlike higher-level frameworks that abstract away control flow, LangGraph provides explicit infrastructure for stateful workflows while maintaining full control over prompts and architecture.\\n\\n### Core Value Propositions\\n\\n- **Durable Execution**: Applications persist through failures and resume exactly where they left off\\n- **Human-in-the-Loop**: Seamless incorporation of human oversight by inspecting and modifying agent state at any point  \\n- **Comprehensive Memory**: Both short-term working memory for ongoing reasoning and long-term persistent memory across sessions\\n- **Production-Ready Deployment**: Scalable infrastructure designed for stateful, long-running workflows\\n- **Low-Level Control**: No abstraction of prompts or architecture - full developer control\\n\\nLangGraph is trusted by companies including Klarna, Replit, and Elastic for building production agent systems.\\n\\nSources: [README.md:16-60](), [libs/langgraph/pyproject.toml:8]()\\n\\n## Framework Ecosystem Overview\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Core Framework\\\"\\n        LG[\\\"langgraph<br/>StateGraph + Pregel Engine\\\"]\\n        PB[\\\"langgraph-prebuilt<br/>create_react_agent + ToolNode\\\"]\\n        CP[\\\"langgraph-checkpoint<br/>BaseCheckpointSaver + BaseStore\\\"]\\n    end\\n    \\n    subgraph \\\"Development Tools\\\"\\n        CLI[\\\"langgraph-cli<br/>langgraph dev + build\\\"]\\n        SDK_PY[\\\"langgraph-sdk<br/>LangGraphClient (Python)\\\"]\\n        SDK_JS[\\\"langgraph-sdk<br/>LangGraphClient (JS)\\\"]\\n    end\\n    \\n    subgraph \\\"Platform & Deployment\\\"\\n        PLATFORM[\\\"LangGraph Platform<br/>Cloud SaaS\\\"]\\n        SERVER[\\\"LangGraph Server<br/>API Runtime\\\"]\\n        STUDIO[\\\"LangGraph Studio<br/>Visual Debugger\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Backends\\\"\\n        MEM[\\\"MemoryCheckpointSaver\\\"]\\n        SQLITE[\\\"SqliteCheckpointSaver\\\"]\\n        POSTGRES[\\\"PostgresCheckpointSaver\\\"]\\n        REDIS[\\\"RedisCache\\\"]\\n    end\\n    \\n    subgraph \\\"External Integrations\\\"\\n        LC[\\\"LangChain Ecosystem\\\"]\\n        LS[\\\"LangSmith Observability\\\"]\\n        DOCKER[\\\"Docker Containerization\\\"]\\n    end\\n    \\n    LG --> PB\\n    LG --> CP\\n    CLI --> LG\\n    SDK_PY --> SERVER\\n    PLATFORM --> SERVER\\n    PLATFORM --> STUDIO\\n    CP --> MEM\\n    CP --> SQLITE\\n    CP --> POSTGRES\\n    LG -.-> LC\\n    PLATFORM --> LS\\n    CLI --> DOCKER\\n    \\n    style LG fill:#e1f5fe\\n    style CLI fill:#f3e5f5\\n    style PLATFORM fill:#fff3e0\\n```\\n\\nSources: [README.md:61-81](), architectural diagrams from context\\n</old_str>\\n\\n<old_str>\\n## Package Architecture\\n\\nLangGraph is organized as a monorepo with multiple Python packages, each serving distinct roles in the framework ecosystem:\\n\\n### Package Dependency Structure\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Core Framework Packages\\\"\\n        LANGGRAPH[\\\"langgraph<br/>version: 0.6.6<br/>StateGraph + Pregel\\\"]\\n        CHECKPOINT[\\\"langgraph-checkpoint<br/>version: 2.1.1<br/>BaseCheckpointSaver\\\"]\\n        PREBUILT[\\\"langgraph-prebuilt<br/>version: 0.6.4<br/>create_react_agent\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Implementations\\\"\\n        CP_SQLITE[\\\"langgraph-checkpoint-sqlite<br/>SqliteCheckpointSaver\\\"]\\n        CP_POSTGRES[\\\"langgraph-checkpoint-postgres<br/>PostgresCheckpointSaver\\\"]\\n    end\\n    \\n    subgraph \\\"External Dependencies\\\"\\n        LC_CORE[\\\"langchain-core>=0.1\\\"]\\n        PYDANTIC[\\\"pydantic>=2.7.4\\\"]\\n        XXHASH[\\\"xxhash>=3.5.0\\\"]\\n        ORMSGPACK[\\\"ormsgpack>=1.10.0\\\"]\\n    end\\n    \\n    LANGGRAPH --> CHECKPOINT\\n    LANGGRAPH --> PREBUILT\\n    LANGGRAPH --> LC_CORE\\n    LANGGRAPH --> PYDANTIC\\n    LANGGRAPH --> XXHASH\\n    \\n    PREBUILT --> CHECKPOINT\\n    PREBUILT --> LC_CORE\\n    \\n    CHECKPOINT --> LC_CORE\\n    CHECKPOINT --> ORMSGPACK\\n    \\n    CP_SQLITE --> CHECKPOINT\\n    CP_POSTGRES --> CHECKPOINT\\n```\\n\\n### Package Breakdown\\n\\n| Package | Purpose | Key Classes/Functions |\\n|---------|---------|----------------------|\\n| **`langgraph`** | Core execution framework | `StateGraph`, `Pregel`, `PregelLoop` |\\n| **`langgraph-checkpoint`** | Persistence abstractions | `BaseCheckpointSaver`, `BaseStore`, `JsonPlusSerializer` |\\n| **`langgraph-prebuilt`** | High-level agent components | `create_react_agent`, `ToolNode` |\\n| **`langgraph-checkpoint-sqlite`** | SQLite persistence | `SqliteCheckpointSaver`, `SqliteStore` |\\n| **`langgraph-checkpoint-postgres`** | PostgreSQL persistence | `PostgresCheckpointSaver`, `PostgresStore` |\\n| **`langgraph-sdk`** | Client libraries | `LangGraphClient`, `RemoteGraph` |\\n| **`langgraph-cli`** | Development tools | `langgraph dev`, `langgraph build` |\\n\\nSources: [libs/langgraph/pyproject.toml:5-21](), [libs/checkpoint/pyproject.toml:5-17](), [libs/prebuilt/pyproject.toml:5-17]()\\n</old_str>\\n<new_str>\\n## Package Architecture\\n\\nLangGraph is organized as a monorepo with multiple Python packages, each serving distinct roles in the framework ecosystem:\\n\\n### Package Dependency Structure\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Core Framework Packages\\\"\\n        LANGGRAPH[\\\"langgraph<br/>version: 0.6.6<br/>StateGraph + Pregel\\\"]\\n        CHECKPOINT[\\\"langgraph-checkpoint<br/>version: 2.1.1<br/>BaseCheckpointSaver\\\"]\\n        PREBUILT[\\\"langgraph-prebuilt<br/>version: 0.6.4<br/>create_react_agent\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Implementations\\\"\\n        CP_SQLITE[\\\"langgraph-checkpoint-sqlite<br/>SqliteCheckpointSaver\\\"]\\n        CP_POSTGRES[\\\"langgraph-checkpoint-postgres<br/>PostgresCheckpointSaver\\\"]\\n    end\\n    \\n    subgraph \\\"External Dependencies\\\"\\n        LC_CORE[\\\"langchain-core>=0.1\\\"]\\n        PYDANTIC[\\\"pydantic>=2.7.4\\\"]\\n        XXHASH[\\\"xxhash>=3.5.0\\\"]\\n        ORMSGPACK[\\\"ormsgpack>=1.10.0\\\"]\\n    end\\n    \\n    LANGGRAPH --> CHECKPOINT\\n    LANGGRAPH --> PREBUILT\\n    LANGGRAPH --> LC_CORE\\n    LANGGRAPH --> PYDANTIC\\n    LANGGRAPH --> XXHASH\\n    \\n    PREBUILT --> CHECKPOINT\\n    PREBUILT --> LC_CORE\\n    \\n    CHECKPOINT --> LC_CORE\\n    CHECKPOINT --> ORMSGPACK\\n    \\n    CP_SQLITE --> CHECKPOINT\\n    CP_POSTGRES --> CHECKPOINT\\n    \\n    style LANGGRAPH fill:#e1f5fe\\n    style CHECKPOINT fill:#f3e5f5\\n    style PREBUILT fill:#fff3e0\\n```\\n\\n### Package Breakdown\\n\\n| Package | Purpose | Key Classes/Functions |\\n|---------|---------|----------------------|\\n| **`langgraph`** | Core execution framework | `StateGraph`, `Pregel`, `PregelLoop` |\\n| **`langgraph-checkpoint`** | Persistence abstractions | `BaseCheckpointSaver`, `BaseStore`, `JsonPlusSerializer` |\\n| **`langgraph-prebuilt`** | High-level agent components | `create_react_agent`, `ToolNode` |\\n| **`langgraph-checkpoint-sqlite`** | SQLite persistence | `SqliteCheckpointSaver`, `SqliteStore` |\\n| **`langgraph-checkpoint-postgres`** | PostgreSQL persistence | `PostgresCheckpointSaver`, `PostgresStore` |\\n| **`langgraph-sdk`** | Client libraries | `LangGraphClient`, `RemoteGraph` |\\n| **`langgraph-cli`** | Development tools | `langgraph dev`, `langgraph build` |\\n\\nSources: [libs/langgraph/pyproject.toml:5-21](), [libs/checkpoint/pyproject.toml:5-17](), [libs/prebuilt/pyproject.toml:5-17]()\\n</thinking>\\n\\nLooking at the old content and the provided context, I need to update the Overview page to better reflect LangGraph's architecture and provide clearer bridges between natural language concepts and actual code entities. Let me make targeted improvements:\\n\\n<old_str>\\n### Build System Integration\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Monorepo Structure\\\"\\n        ROOT[\\\"Root Makefile\\\"] --> LIBS[\\\"libs/*/\\\"]\\n        LIBS --> LG_DIR[\\\"libs/langgraph/\\\"]\\n        LIBS --> CP_DIR[\\\"libs/checkpoint/\\\"]\\n        LIBS --> PB_DIR[\\\"libs/prebuilt/\\\"]\\n        LIBS --> SDK_DIR[\\\"libs/sdk-py/\\\"]\\n        LIBS --> CLI_DIR[\\\"libs/cli/\\\"]\\n    end\\n    \\n    subgraph \\\"Development Workflow\\\"\\n        UV_LOCK[\\\"uv lock\\\"] --> SYNC_DEPS[\\\"uv sync --frozen\\\"]\\n        SYNC_DEPS --> EDITABLE_INSTALLS[\\\"editable installs<br/>via uv.sources\\\"]\\n        EDITABLE_INSTALLS --> DEV_SERVER[\\\"langgraph dev\\\"]\\n        DEV_SERVER --> TESTING[\\\"pytest execution\\\"]\\n    end\\n    \\n    subgraph \\\"Dependency Sources\\\"\\n        LG_SRC[\\\"langgraph = {path = '../langgraph'}\\\"]\\n        CP_SRC[\\\"langgraph-checkpoint = {path = '../checkpoint'}\\\"] \\n        PB_SRC[\\\"langgraph-prebuilt = {path = '../prebuilt'}\\\"]\\n        SDK_SRC[\\\"langgraph-sdk = {path = '../sdk-py'}\\\"]\\n    end\\n```\\n\\nSources: [Makefile:1-68](), [libs/langgraph/pyproject.toml:58-64](), [libs/prebuilt/pyproject.toml:41-45]()\\n</old_str>\\n<new_str>\\n# Overview\\n\\n## What is LangGraph?\\n\\nLangGraph is a low-level orchestration framework for building, managing, and deploying long-running, stateful agents and multi-actor applications with Large Language Models (LLMs). Unlike higher-level frameworks that abstract away control flow, LangGraph provides explicit infrastructure for stateful workflows while maintaining full control over prompts and architecture.\\n\\n### Core Value Propositions\\n\\n- **Durable Execution**: Applications persist through failures and resume exactly where they left off\\n- **Human-in-the-Loop**: Seamless incorporation of human oversight by inspecting and modifying agent state at any point  \\n- **Comprehensive Memory**: Both short-term working memory for ongoing reasoning and long-term persistent memory across sessions\\n- **Production-Ready Deployment**: Scalable infrastructure designed for stateful, long-running workflows\\n- **Low-Level Control**: No abstraction of prompts or architecture - full developer control\\n\\nLangGraph is trusted by companies including Klarna, Replit, and Elastic for building production agent systems.\\n\\nSources: [README.md:16-60](), [libs/langgraph/pyproject.toml:8]()\\n\\n## Framework Ecosystem Overview\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Core Framework\\\"\\n        LG[\\\"langgraph<br/>StateGraph + Pregel Engine\\\"]\\n        PB[\\\"langgraph-prebuilt<br/>create_react_agent + ToolNode\\\"]\\n        CP[\\\"langgraph-checkpoint<br/>BaseCheckpointSaver + BaseStore\\\"]\\n    end\\n    \\n    subgraph \\\"Development Tools\\\"\\n        CLI[\\\"langgraph-cli<br/>langgraph dev + build\\\"]\\n        SDK_PY[\\\"langgraph-sdk<br/>LangGraphClient (Python)\\\"]\\n        SDK_JS[\\\"langgraph-sdk<br/>LangGraphClient (JS)\\\"]\\n    end\\n    \\n    subgraph \\\"Platform & Deployment\\\"\\n        PLATFORM[\\\"LangGraph Platform<br/>Cloud SaaS\\\"]\\n        SERVER[\\\"LangGraph Server<br/>API Runtime\\\"]\\n        STUDIO[\\\"LangGraph Studio<br/>Visual Debugger\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Backends\\\"\\n        MEM[\\\"MemoryCheckpointSaver\\\"]\\n        SQLITE[\\\"SqliteCheckpointSaver\\\"]\\n        POSTGRES[\\\"PostgresCheckpointSaver\\\"]\\n        REDIS[\\\"RedisCache\\\"]\\n    end\\n    \\n    subgraph \\\"External Integrations\\\"\\n        LC[\\\"LangChain Ecosystem\\\"]\\n        LS[\\\"LangSmith Observability\\\"]\\n        DOCKER[\\\"Docker Containerization\\\"]\\n    end\\n    \\n    LG --> PB\\n    LG --> CP\\n    CLI --> LG\\n    SDK_PY --> SERVER\\n    PLATFORM --> SERVER\\n    PLATFORM --> STUDIO\\n    CP --> MEM\\n    CP --> SQLITE\\n    CP --> POSTGRES\\n    LG -.-> LC\\n    PLATFORM --> LS\\n    CLI --> DOCKER\\n```\\n\\nSources: [README.md:61-81](), architectural diagrams from context\\n\\n## Core Execution Architecture\\n\\nLangGraph's execution model is built around the Pregel computational framework, enabling stateful, multi-step workflows through message passing and checkpointing:\\n\\n### StateGraph to Pregel Compilation\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Graph Definition\\\"\\n        USER[\\\"User Code\\\"] --> SG[\\\"StateGraph\\\"]\\n        SG --> ADD_NODE[\\\"graph.add_node('agent', agent_func)\\\"]\\n        SG --> ADD_EDGE[\\\"graph.add_edge('start', 'agent')\\\"]\\n        SG --> SET_ENTRY[\\\"graph.set_entry_point('start')\\\"]\\n        SG --> STATE_SCHEMA[\\\"State: TypedDict | Pydantic\\\"]\\n    end\\n    \\n    subgraph \\\"Compilation Process\\\"\\n        SG --> COMPILE[\\\"graph.compile(checkpointer=...)\\\"]\\n        COMPILE --> PREGEL[\\\"Pregel instance\\\"]\\n        COMPILE --> VALIDATE[\\\"Validate graph structure\\\"]\\n        VALIDATE --> CREATE_CHANNELS[\\\"Create channel instances\\\"]\\n        CREATE_CHANNELS --> WRAP_NODES[\\\"Wrap nodes as PregelNode\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Components\\\"\\n        PREGEL --> INVOKE[\\\"pregel.invoke(input)\\\"]\\n        PREGEL --> STREAM[\\\"pregel.stream(input)\\\"]\\n        PREGEL --> ASTREAM[\\\"pregel.astream(input)\\\"]\\n        \\n        INVOKE --> PREGEL_LOOP[\\\"PregelLoop.run()\\\"]\\n        STREAM --> PREGEL_LOOP\\n        ASTREAM --> PREGEL_LOOP\\n    end\\n    \\n    subgraph \\\"Execution Engine\\\"\\n        PREGEL_LOOP --> SCHEDULE[\\\"Schedule PregelExecutableTask\\\"]\\n        SCHEDULE --> EXECUTE[\\\"Execute node functions\\\"]\\n        EXECUTE --> UPDATE[\\\"Update channels\\\"]\\n        UPDATE --> CHECKPOINT[\\\"Save checkpoint\\\"]\\n        CHECKPOINT --> CONTINUE{\\\"Continue?\\\"}\\n        CONTINUE -->|Yes| SCHEDULE\\n        CONTINUE -->|No| RETURN[\\\"Return result\\\"]\\n    end\\n```\\n\\n### Channel System and State Management\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Channel Types\\\"\\n        LAST_VALUE[\\\"LastValue<br/>(single value channels)\\\"]\\n        TOPIC[\\\"Topic<br/>(append-only channels)\\\"]\\n        AGGREGATE[\\\"Aggregate<br/>(reduction channels)\\\"]\\n        BINARY_OPERATOR[\\\"BinaryOperator<br/>(custom reducers)\\\"]\\n    end\\n    \\n    subgraph \\\"State Flow\\\"\\n        NODE_A[\\\"Node A\\\"] --> WRITE[\\\"Channel.write()\\\"]\\n        WRITE --> CHANNELS[\\\"Channel instances\\\"]\\n        CHANNELS --> READ[\\\"Channel.read()\\\"]\\n        READ --> NODE_B[\\\"Node B\\\"]\\n        \\n        CHANNELS --> SNAPSHOT[\\\"StateSnapshot\\\"]\\n        SNAPSHOT --> SAVER[\\\"BaseCheckpointSaver.put()\\\"]\\n        SAVER --> STORAGE[\\\"Persistent storage\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Components\\\"\\n        CP_SAVER[\\\"BaseCheckpointSaver\\\"]\\n        CP_STORE[\\\"BaseStore\\\"]\\n        SERIALIZER[\\\"JsonPlusSerializer\\\"]\\n        \\n        CP_SAVER --> SERIALIZER\\n        CP_STORE --> SERIALIZER\\n    end\\n```\\n\\n**Key Classes and Methods:**\\n- **`StateGraph`**: Graph builder with `add_node()`, `add_edge()`, `compile()` methods\\n- **`Pregel`**: Compiled graph with `invoke()`, `stream()`, `astream()` execution methods  \\n- **`PregelLoop`**: Core execution engine in `run()` method handling task scheduling\\n- **`BaseCheckpointSaver`**: Persistence interface with `put()`, `get()`, `list()` methods\\n- **Channel Types**: `LastValue`, `Topic`, `Aggregate` for different state patterns\\n\\nSources: Based on architectural patterns from context diagrams and package structure analysis\\n\\n### Dependency Relationships\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"External Dependencies\\\"\\n        LC[\\\"langchain-core>=0.1\\\"]\\n        PYDANTIC[\\\"pydantic>=2.7.4\\\"] \\n        XXHASH[\\\"xxhash>=3.5.0\\\"]\\n        HTTPX[\\\"httpx>=0.25.2\\\"]\\n        ORJSON[\\\"orjson>=3.10.1\\\"]\\n    end\\n    \\n    subgraph \\\"Internal Package Dependencies\\\"\\n        LANGGRAPH[\\\"langgraph\\\"] --> LC\\n        LANGGRAPH --> PYDANTIC\\n        LANGGRAPH --> XXHASH\\n        LANGGRAPH --> CHECKPOINT[\\\"langgraph-checkpoint>=2.1.0\\\"]\\n        LANGGRAPH --> SDK[\\\"langgraph-sdk>=0.2.0\\\"]\\n        LANGGRAPH --> PREBUILT[\\\"langgraph-prebuilt>=0.6.0\\\"]\\n        \\n        PREBUILT --> CHECKPOINT\\n        PREBUILT --> LC\\n        \\n        SDK --> HTTPX\\n        SDK --> ORJSON\\n        \\n        CP_SQLITE[\\\"langgraph-checkpoint-sqlite\\\"] --> CHECKPOINT\\n        CP_POSTGRES[\\\"langgraph-checkpoint-postgres\\\"] --> CHECKPOINT\\n    end\\n    \\n    style LANGGRAPH fill:#e1f5fe\\n    style CHECKPOINT fill:#f3e5f5\\n    style PREBUILT fill:#fff3e0\\n```\\n\\nSources: [libs/langgraph/pyproject.toml:14-21](), [libs/prebuilt/pyproject.toml:14-17](), [libs/sdk-py/uv.lock:140-144]()\\n\\n## Core Framework Architecture\\n\\nThe LangGraph framework is built around several key abstractions that enable stateful, multi-step workflows:\\n\\n### Primary Framework Components\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"User Definition Layer\\\"\\n        USER[\\\"User Code\\\"] --> SG[\\\"StateGraph\\\"]\\n        USER --> FUNC_API[\\\"@entrypoint/@task<br/>Functional API\\\"]\\n        SG --> NODES[\\\"Node Functions<br/>(Python callables)\\\"]\\n        SG --> EDGES[\\\"Edge Definitions<br/>(Control flow)\\\"]\\n        SG --> STATE_SCHEMA[\\\"State Schema<br/>(TypedDict/Pydantic)\\\"]\\n    end\\n    \\n    subgraph \\\"Compilation Layer\\\" \\n        SG --> COMPILE[\\\"graph.compile()\\\"]\\n        FUNC_API --> COMPILE\\n        COMPILE --> PREGEL[\\\"Pregel<br/>(Compiled graph instance)\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Execution Layer\\\"\\n        PREGEL --> CHANNELS[\\\"Channel System<br/>(LastValue, Topic, etc.)\\\"]\\n        PREGEL --> PREGEL_NODES[\\\"PregelNodes<br/>(Wrapped functions)\\\"]\\n        PREGEL --> LOOP[\\\"PregelLoop<br/>(Execution engine)\\\"]\\n        \\n        LOOP --> RUNNER[\\\"PregelRunner\\\"]\\n        RUNNER --> TASKS[\\\"PregelExecutableTasks\\\"]\\n        TASKS --> EXEC[\\\"Node Execution\\\"]\\n    end\\n    \\n    subgraph \\\"State Management\\\"\\n        EXEC --> READ[\\\"Channel Reads\\\"]\\n        EXEC --> WRITE[\\\"Channel Writes\\\"]\\n        READ --> CHANNELS\\n        WRITE --> CHANNELS\\n        CHANNELS --> CHECKPOINTS[\\\"StateSnapshot<br/>Checkpointing\\\"]\\n    end\\n    \\n    style PREGEL fill:#ffeb3b\\n    style LOOP fill:#ff9800\\n    style CHANNELS fill:#4caf50\\n```\\n\\n**Key Classes and Interfaces:**\\n- **`StateGraph`**: Main graph definition class for building workflows\\n- **`Pregel`**: Compiled graph instance providing execution methods (`invoke`, `stream`, `astream`)\\n- **`PregelLoop`**: Core execution engine handling message passing and state transitions\\n- **`BaseCheckpointSaver`**: Abstract interface for persisting execution state\\n- **Channel Types**: `LastValue`, `Topic`, `Aggregate` for different state management patterns\\n\\nSources: Based on repository structure analysis and architectural diagrams provided in context\\n\\n### Execution Flow Model\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Graph Invocation\\\"\\n        INVOKE[\\\"graph.invoke()\\\"] --> INPUT_VALIDATION[\\\"Input Validation\\\"]\\n        STREAM[\\\"graph.stream()\\\"] --> INPUT_VALIDATION\\n        ASTREAM[\\\"graph.astream()\\\"] --> INPUT_VALIDATION\\n    end\\n    \\n    subgraph \\\"PregelLoop Execution\\\"\\n        INPUT_VALIDATION --> LOOP_START[\\\"PregelLoop.run()\\\"]\\n        LOOP_START --> LOAD_CHECKPOINT[\\\"Load Checkpoint<br/>(if resuming)\\\"]\\n        LOAD_CHECKPOINT --> EXECUTE_STEP[\\\"Execute Step\\\"]\\n        \\n        EXECUTE_STEP --> SCHEDULE_TASKS[\\\"Schedule PregelTasks\\\"]\\n        SCHEDULE_TASKS --> RUN_NODES[\\\"Run Node Functions\\\"]\\n        RUN_NODES --> UPDATE_CHANNELS[\\\"Update Channels\\\"]\\n        UPDATE_CHANNELS --> SAVE_CHECKPOINT[\\\"Save Checkpoint\\\"]\\n        \\n        SAVE_CHECKPOINT --> CHECK_INTERRUPT[\\\"Check Interrupts\\\"]\\n        CHECK_INTERRUPT --> CONTINUE{\\\"Continue?\\\"}\\n        CONTINUE -->|Yes| EXECUTE_STEP\\n        CONTINUE -->|No| RETURN_STATE[\\\"Return Final State\\\"]\\n    end\\n    \\n    subgraph \\\"Human-in-the-Loop\\\"\\n        CHECK_INTERRUPT --> HUMAN_INTERVENTION[\\\"Human Review<br/>(if interrupted)\\\"]\\n        HUMAN_INTERVENTION --> UPDATE_STATE[\\\"graph.update_state()\\\"]\\n        UPDATE_STATE --> RESUME[\\\"Resume Execution\\\"]\\n        RESUME --> EXECUTE_STEP\\n    end\\n    \\n    style EXECUTE_STEP fill:#ffeb3b\\n    style SAVE_CHECKPOINT fill:#4caf50\\n    style HUMAN_INTERVENTION fill:#ff9800\\n```\\n\\nSources: Based on architectural patterns described in context and repository structure analysis\\n\\n## Development Infrastructure\\n\\n### Build System and Tooling\\n\\nThe repository uses modern Python tooling for development and CI/CD:\\n\\n| Tool | Purpose | Configuration |\\n|------|---------|---------------|\\n| **uv** | Package and dependency management | [Makefile:11-18]() |\\n| **ruff** | Linting and code formatting | [libs/langgraph/pyproject.toml:65-86]() |\\n| **mypy** | Static type checking | [libs/langgraph/pyproject.toml:87-95]() |\\n| **pytest** | Testing framework | [libs/langgraph/pyproject.toml:108-109]() |\\n| **Docker Compose** | Development services (PostgreSQL) | [libs/langgraph/Makefile:40-44]() |\\n\\n### Build System and Tooling\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Package Management\\\"\\n        UV[\\\"uv package manager\\\"] --> LOCK[\\\"uv.lock per package\\\"]\\n        UV --> SYNC[\\\"uv sync --frozen\\\"]\\n        UV --> EDITABLE[\\\"editable installs\\\"]\\n        \\n        EDITABLE --> LG_PATH[\\\"langgraph = {path = '../langgraph'}\\\"]\\n        EDITABLE --> CP_PATH[\\\"langgraph-checkpoint = {path = '../checkpoint'}\\\"]\\n        EDITABLE --> PB_PATH[\\\"langgraph-prebuilt = {path = '../prebuilt'}\\\"]\\n    end\\n    \\n    subgraph \\\"Code Quality Tools\\\"\\n        RUFF[\\\"ruff linter + formatter\\\"] --> LINT_SELECT[\\\"select = ['E', 'F', 'I', 'TID251', 'UP']\\\"]\\n        MYPY[\\\"mypy type checker\\\"] --> DISALLOW[\\\"disallow_untyped_defs = True\\\"]\\n        CODESPELL[\\\"codespell spell checker\\\"] --> TOML_CONFIG[\\\"--toml pyproject.toml\\\"]\\n    end\\n    \\n    subgraph \\\"Development Commands\\\"\\n        MAKE_INSTALL[\\\"make install\\\"] --> UV_SYNC[\\\"uv sync --all-packages\\\"]\\n        MAKE_TEST[\\\"make test\\\"] --> START_SERVICES[\\\"Docker services\\\"]\\n        MAKE_LINT[\\\"make lint\\\"] --> RUFF_CHECK[\\\"ruff check + format\\\"]\\n        MAKE_FORMAT[\\\"make format\\\"] --> RUFF_FORMAT[\\\"ruff format + fix\\\"]\\n    end\\n```\\n\\n**Development Tooling:**\\n- **Package Manager**: `uv` for fast dependency resolution and virtual environments\\n- **Code Quality**: `ruff` for linting/formatting, `mypy` for type checking, `codespell` for spell checking\\n- **Testing**: `pytest` with Docker Compose for external services\\n- **Editable Installs**: Local package development via `tool.uv.sources` configuration\\n\\nSources: [libs/langgraph/pyproject.toml:55-96](), [libs/langgraph/Makefile:14-136](), [Makefile:8-68]()\\n\\n### Package Management Strategy\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Monorepo Structure\\\"\\n        ROOT[\\\"Root Makefile\\\"] --> LIBS[\\\"libs/*/\\\"]\\n        LIBS --> LG_DIR[\\\"libs/langgraph/\\\"]\\n        LIBS --> CP_DIR[\\\"libs/checkpoint/\\\"]\\n        LIBS --> PB_DIR[\\\"libs/prebuilt/\\\"]\\n        LIBS --> SDK_DIR[\\\"libs/sdk-py/\\\"]\\n        LIBS --> CLI_DIR[\\\"libs/cli/\\\"]\\n    end\\n    \\n    subgraph \\\"Development Workflow\\\"\\n        UV_LOCK[\\\"uv lock\\\"] --> SYNC_DEPS[\\\"uv sync --frozen\\\"]\\n        SYNC_DEPS --> EDITABLE_INSTALLS[\\\"editable installs<br/>via uv.sources\\\"]\\n        EDITABLE_INSTALLS --> DEV_SERVER[\\\"langgraph dev\\\"]\\n        DEV_SERVER --> TESTING[\\\"pytest execution\\\"]\\n    end\\n    \\n    subgraph \\\"Dependency Sources\\\"\\n        LG_SRC[\\\"langgraph = {path = '../langgraph'}\\\"]\\n        CP_SRC[\\\"langgraph-checkpoint = {path = '../checkpoint'}\\\"] \\n        PB_SRC[\\\"langgraph-prebuilt = {path = '../prebuilt'}\\\"]\\n        SDK_SRC[\\\"langgraph-sdk = {path = '../sdk-py'}\\\"]\\n    end\\n    \\n    style ROOT fill:#e1f5fe\\n    style UV_LOCK fill:#4caf50\\n    style EDITABLE_INSTALLS fill:#fff3e0\\n```\\n\\n**Development Setup Features:**\\n- **Editable Installs**: All packages installed in development mode via `uv.sources`\\n- **Lock File Management**: Per-package `uv.lock` files for reproducible builds\\n- **Unified Commands**: Root-level Makefile orchestrates all sub-packages\\n- **Development Server**: Integrated `langgraph dev` command for testing\\n\\nSources: [Makefile:1-68](), [libs/langgraph/pyproject.toml:57-63](), [libs/prebuilt/pyproject.toml:41-45]()\", \"# Page: Core Architecture\\n\\n# Core Architecture\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/langgraph/langgraph/constants.py](libs/langgraph/langgraph/constants.py)\\n- [libs/langgraph/langgraph/errors.py](libs/langgraph/langgraph/errors.py)\\n- [libs/langgraph/langgraph/func/__init__.py](libs/langgraph/langgraph/func/__init__.py)\\n- [libs/langgraph/langgraph/graph/state.py](libs/langgraph/langgraph/graph/state.py)\\n- [libs/langgraph/langgraph/pregel/__init__.py](libs/langgraph/langgraph/pregel/__init__.py)\\n- [libs/langgraph/langgraph/pregel/debug.py](libs/langgraph/langgraph/pregel/debug.py)\\n- [libs/langgraph/langgraph/pregel/types.py](libs/langgraph/langgraph/pregel/types.py)\\n- [libs/langgraph/langgraph/types.py](libs/langgraph/langgraph/types.py)\\n- [libs/langgraph/tests/test_pregel.py](libs/langgraph/tests/test_pregel.py)\\n- [libs/langgraph/tests/test_pregel_async.py](libs/langgraph/tests/test_pregel_async.py)\\n\\n</details>\\n\\n\\n\\nThis document covers the fundamental architecture of LangGraph's execution model, focusing on how user-defined workflows are compiled and executed. It explains the core components that enable stateful, multi-actor applications with LLMs through a Pregel-inspired runtime system.\\n\\nFor information about persistence and checkpointing mechanisms, see [Persistence System](#5). For details about human-in-the-loop patterns and control flow, see [Human-in-the-Loop and Control Flow](#4). For state management specifics, see [State Management and Channels](#3).\\n\\n## Overview\\n\\nLangGraph's core architecture consists of three main layers:\\n\\n1. **Definition Layer**: User-facing APIs (`StateGraph` and functional decorators) for defining workflows\\n2. **Compilation Layer**: Transformation of user definitions into executable `Pregel` instances  \\n3. **Runtime Layer**: Execution engine that orchestrates stateful computation across nodes\\n\\nThe system is built on a message-passing model inspired by Google's Pregel paper, where computation happens in discrete steps with state updates propagated through channels between nodes.\\n\\n## Definition to Runtime Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"User Definition APIs\\\"\\n        SG[\\\"StateGraph\\\"]\\n        EP[\\\"@entrypoint\\\"]\\n        TASK[\\\"@task\\\"]\\n    end\\n    \\n    subgraph \\\"Compilation Layer\\\"\\n        COMPILE[\\\"compile()\\\"]\\n        PREGEL[\\\"Pregel\\\"]\\n        CHANNELS[\\\"Channels\\\"]\\n        NODES[\\\"PregelNodes\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Execution\\\"\\n        LOOP[\\\"PregelLoop\\\"]\\n        RUNNER[\\\"PregelRunner\\\"] \\n        TASKS[\\\"PregelExecutableTask\\\"]\\n        EXEC[\\\"Task Execution\\\"]\\n    end\\n    \\n    subgraph \\\"State Management\\\"\\n        LASTVAL[\\\"LastValue\\\"]\\n        TOPIC[\\\"Topic\\\"]\\n        BINOP[\\\"BinaryOperatorAggregate\\\"]\\n        EPHEMERAL[\\\"EphemeralValue\\\"]\\n    end\\n    \\n    SG --> COMPILE\\n    EP --> COMPILE\\n    TASK --> COMPILE\\n    \\n    COMPILE --> PREGEL\\n    PREGEL --> CHANNELS\\n    PREGEL --> NODES\\n    \\n    PREGEL --> LOOP\\n    LOOP --> RUNNER\\n    RUNNER --> TASKS\\n    TASKS --> EXEC\\n    \\n    CHANNELS --> LASTVAL\\n    CHANNELS --> TOPIC  \\n    CHANNELS --> BINOP\\n    CHANNELS --> EPHEMERAL\\n    \\n    EXEC --> CHANNELS\\n    CHANNELS --> EXEC\\n```\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:117-240](), [libs/langgraph/langgraph/pregel/__init__.py:1-4](), [libs/langgraph/langgraph/func/__init__.py:222-555]()\\n\\n## StateGraph Definition API\\n\\nThe `StateGraph` class provides the primary interface for defining stateful workflows. It manages the graph structure, state schema, and compilation process.\\n\\n### Core Components\\n\\n| Component | Purpose | Code Entity |\\n|-----------|---------|-------------|\\n| **Nodes** | User-defined computation functions | `StateGraph.nodes: dict[str, StateNodeSpec]` |\\n| **Edges** | Static connections between nodes | `StateGraph.edges: set[tuple[str, str]]` |\\n| **Branches** | Conditional routing logic | `StateGraph.branches: defaultdict[str, dict[str, BranchSpec]]` |\\n| **Channels** | State storage and communication | `StateGraph.channels: dict[str, BaseChannel]` |\\n| **Schema** | Type definitions for state | `StateGraph.state_schema: type[StateT]` |\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"StateGraph Build Process\\\"\\n        SCHEMA[\\\"state_schema\\\"]\\n        ADD_NODE[\\\"add_node()\\\"]\\n        ADD_EDGE[\\\"add_edge()\\\"]\\n        ADD_COND[\\\"add_conditional_edges()\\\"]\\n        COMPILE[\\\"compile()\\\"]\\n    end\\n    \\n    subgraph \\\"Internal Structures\\\" \\n        NODES[\\\"nodes: dict[str, StateNodeSpec]\\\"]\\n        EDGES[\\\"edges: set[tuple[str, str]]\\\"]\\n        BRANCHES[\\\"branches: defaultdict\\\"]\\n        CHANNELS[\\\"channels: dict[str, BaseChannel]\\\"]\\n    end\\n    \\n    subgraph \\\"Compilation Output\\\"\\n        PREGEL_INST[\\\"Pregel instance\\\"]\\n    end\\n    \\n    SCHEMA --> CHANNELS\\n    ADD_NODE --> NODES\\n    ADD_EDGE --> EDGES  \\n    ADD_COND --> BRANCHES\\n    \\n    NODES --> COMPILE\\n    EDGES --> COMPILE\\n    BRANCHES --> COMPILE\\n    CHANNELS --> COMPILE\\n    \\n    COMPILE --> PREGEL_INST\\n```\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:173-240](), [libs/langgraph/langgraph/graph/state.py:349-551]()\\n\\n### State Schema Processing  \\n\\nThe `StateGraph` processes type annotations to automatically create appropriate channels for state management. Annotated types with reducers become specialized channels.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Schema Analysis\\\"\\n        TYPEDDICT[\\\"TypedDict State\\\"]\\n        ANNOTATED[\\\"Annotated[type, reducer]\\\"]\\n        HINTS[\\\"get_type_hints()\\\"]\\n    end\\n    \\n    subgraph \\\"Channel Creation\\\"\\n        LASTVALUE[\\\"LastValue channel\\\"]\\n        BINOP[\\\"BinaryOperatorAggregate\\\"]\\n        CUSTOM[\\\"Custom reducer channel\\\"]\\n    end\\n    \\n    TYPEDDICT --> HINTS\\n    ANNOTATED --> HINTS\\n    \\n    HINTS --> LASTVALUE\\n    HINTS --> BINOP\\n    HINTS --> CUSTOM\\n```\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:247-277]()\\n\\n## Pregel Runtime Engine  \\n\\nThe `Pregel` class represents a compiled graph ready for execution. It orchestrates the execution of nodes through a message-passing system with persistent state.\\n\\n### Pregel Core Structure\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Pregel Instance\\\"\\n        NODES[\\\"nodes: dict[str, PregelNode]\\\"]\\n        CHANNELS_P[\\\"channels: dict[str, BaseChannel]\\\"] \\n        INPUT_CH[\\\"input_channels\\\"]\\n        OUTPUT_CH[\\\"output_channels\\\"]\\n        STREAM_CH[\\\"stream_channels\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Control\\\"\\n        INVOKE[\\\"invoke/ainvoke\\\"]\\n        STREAM[\\\"stream/astream\\\"]  \\n        BATCH[\\\"batch/abatch\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Components\\\"\\n        LOOP[\\\"PregelLoop (Sync/Async)\\\"]\\n        RUNNER[\\\"PregelRunner\\\"]\\n        TASKS_P[\\\"PregelExecutableTask[]\\\"]\\n    end\\n    \\n    NODES --> LOOP\\n    CHANNELS_P --> LOOP\\n    \\n    INVOKE --> LOOP\\n    STREAM --> LOOP\\n    BATCH --> LOOP\\n    \\n    LOOP --> RUNNER\\n    RUNNER --> TASKS_P\\n```\\n\\nSources: [libs/langgraph/tests/test_pregel.py:51-54](), [libs/langgraph/tests/test_pregel_async.py:54-56]()\\n\\n### PregelNode Structure\\n\\n`PregelNode` instances wrap user functions and handle channel communication:\\n\\n- **bound**: The actual user function (Runnable)\\n- **channels**: Input channels to read from  \\n- **triggers**: Channel names that trigger execution\\n- **writers**: Channel write operations for outputs\\n\\n### Execution Loop Architecture\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"PregelLoop Execution Flow\\\"\\n        START_STEP[\\\"Start Step\\\"]\\n        READ_CHANNELS[\\\"Read from channels\\\"]\\n        CREATE_TASKS[\\\"Create PregelExecutableTask[]\\\"]\\n        EXECUTE[\\\"PregelRunner.execute()\\\"]\\n        WRITE_CHANNELS[\\\"Write to channels\\\"]  \\n        CHECK_DONE[\\\"Check completion\\\"]\\n        NEXT_STEP[\\\"Next Step\\\"]\\n    end\\n    \\n    START_STEP --> READ_CHANNELS\\n    READ_CHANNELS --> CREATE_TASKS\\n    CREATE_TASKS --> EXECUTE\\n    EXECUTE --> WRITE_CHANNELS\\n    WRITE_CHANNELS --> CHECK_DONE\\n    CHECK_DONE -->|Not Done| READ_CHANNELS\\n    CHECK_DONE -->|Done| NEXT_STEP\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/_loop.py](), [libs/langgraph/langgraph/pregel/_runner.py]()\\n\\n## Functional API Architecture\\n\\nThe functional API provides a decorator-based approach using `@entrypoint` and `@task` decorators that compile to the same `Pregel` runtime.\\n\\n### Entrypoint Compilation\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Functional Definition\\\"\\n        FUNC[\\\"User function\\\"]\\n        EP_DEC[\\\"@entrypoint\\\"]\\n        TASK_DEC[\\\"@task\\\"] \\n    end\\n    \\n    subgraph \\\"Compilation Process\\\"\\n        BOUND[\\\"get_runnable_for_entrypoint()\\\"]\\n        PREGEL_NODE[\\\"PregelNode creation\\\"]\\n        CHANNELS_F[\\\"Channel setup\\\"]\\n    end\\n    \\n    subgraph \\\"Pregel Output\\\"\\n        PREGEL_F[\\\"Pregel instance\\\"]\\n        START_CH[\\\"START channel\\\"]\\n        END_CH[\\\"END channel\\\"] \\n        PREV_CH[\\\"PREVIOUS channel\\\"]\\n    end\\n    \\n    FUNC --> EP_DEC\\n    EP_DEC --> BOUND\\n    BOUND --> PREGEL_NODE\\n    PREGEL_NODE --> CHANNELS_F\\n    \\n    CHANNELS_F --> START_CH\\n    CHANNELS_F --> END_CH\\n    CHANNELS_F --> PREV_CH\\n    \\n    PREGEL_NODE --> PREGEL_F\\n    CHANNELS_F --> PREGEL_F\\n```\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:463-555]()\\n\\n## Task Execution Model\\n\\n### PregelExecutableTask Structure\\n\\nEach executable task contains:\\n\\n| Field | Type | Purpose |\\n|-------|------|---------|\\n| `name` | `str` | Node identifier |\\n| `input` | `Any` | Input data from channels |\\n| `proc` | `Runnable` | Actual computation to execute |\\n| `writes` | `deque[tuple[str, Any]]` | Output channel writes |\\n| `config` | `RunnableConfig` | Runtime configuration |\\n| `triggers` | `Sequence[str]` | Channels that triggered this task |\\n| `retry_policy` | `Sequence[RetryPolicy]` | Error handling policies |\\n| `id` | `str` | Unique task identifier |\\n\\n### Task Execution Flow\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Task Creation\\\"\\n        TRIGGERS[\\\"Channel triggers\\\"]\\n        INPUT_DATA[\\\"Read channel data\\\"] \\n        CREATE_TASK[\\\"Create PregelExecutableTask\\\"]\\n    end\\n    \\n    subgraph \\\"Execution\\\"\\n        PROC[\\\"Execute proc (Runnable)\\\"]\\n        HANDLE_ERROR[\\\"Handle errors/retries\\\"]\\n        COLLECT_WRITES[\\\"Collect writes\\\"]\\n    end\\n    \\n    subgraph \\\"State Update\\\"\\n        WRITE_CHANNELS[\\\"Write to channels\\\"]\\n        UPDATE_STATE[\\\"Update graph state\\\"]\\n        CHECKPOINT[\\\"Save checkpoint (if enabled)\\\"]\\n    end\\n    \\n    TRIGGERS --> INPUT_DATA\\n    INPUT_DATA --> CREATE_TASK\\n    CREATE_TASK --> PROC\\n    PROC --> HANDLE_ERROR\\n    HANDLE_ERROR --> COLLECT_WRITES\\n    COLLECT_WRITES --> WRITE_CHANNELS\\n    WRITE_CHANNELS --> UPDATE_STATE\\n    UPDATE_STATE --> CHECKPOINT\\n```\\n\\nSources: [libs/langgraph/langgraph/types.py:239-253]()\\n\\n## Core Types and Communication Primitives\\n\\n### Send Primitive\\n\\nThe `Send` class enables dynamic message routing to specific nodes with custom payloads:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Send Usage\\\"\\n        NODE_A[\\\"Node A\\\"]\\n        SEND_OBJ[\\\"Send('target', data)\\\"]\\n        NODE_B[\\\"Node B (target)\\\"]\\n    end\\n    \\n    NODE_A --> SEND_OBJ\\n    SEND_OBJ --> NODE_B\\n    \\n    subgraph \\\"Send Properties\\\"\\n        SEND_NODE[\\\"node: str\\\"]\\n        SEND_ARG[\\\"arg: Any\\\"]\\n    end\\n    \\n    SEND_OBJ --> SEND_NODE\\n    SEND_OBJ --> SEND_ARG\\n```\\n\\nSources: [libs/langgraph/langgraph/types.py:276-345]()\\n\\n### Command Primitive  \\n\\nThe `Command` class provides control flow operations:\\n\\n| Field | Purpose |\\n|-------|---------|\\n| `update` | State updates to apply |\\n| `goto` | Next node(s) to execute |  \\n| `resume` | Resume value for interrupts |\\n| `graph` | Target graph for the command |\\n\\n### Interrupt Mechanism\\n\\nThe `interrupt()` function enables human-in-the-loop workflows by pausing execution:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Interrupt Flow\\\"\\n        NODE_EXEC[\\\"Node execution\\\"]\\n        INTERRUPT_CALL[\\\"interrupt(value)\\\"]\\n        GRAPH_INTERRUPT[\\\"GraphInterrupt exception\\\"]\\n        CLIENT_HANDLE[\\\"Client handles interrupt\\\"]\\n        RESUME_CMD[\\\"Command(resume=response)\\\"]\\n        CONTINUE_EXEC[\\\"Continue execution\\\"]\\n    end\\n    \\n    NODE_EXEC --> INTERRUPT_CALL\\n    INTERRUPT_CALL --> GRAPH_INTERRUPT\\n    GRAPH_INTERRUPT --> CLIENT_HANDLE\\n    CLIENT_HANDLE --> RESUME_CMD\\n    RESUME_CMD --> CONTINUE_EXEC\\n```\\n\\nSources: [libs/langgraph/langgraph/types.py:405-526]()\\n\\n## Channel-Based State Management\\n\\nChannels provide the communication backbone between nodes, with different types optimized for various use cases:\\n\\n| Channel Type | Use Case | Behavior |\\n|--------------|----------|----------|\\n| `LastValue` | Single values | Stores most recent write |\\n| `Topic` | Multiple values | Accumulates all writes |  \\n| `BinaryOperatorAggregate` | Reductions | Applies reducer function |\\n| `EphemeralValue` | Temporary data | Cleared after each step |\\n\\nThe channel system ensures deterministic state updates and enables the checkpointing system to maintain consistency across execution steps.\\n\\nSources: [libs/langgraph/langgraph/channels/](), [libs/langgraph/tests/test_pregel.py:395-485]()\", \"# Page: StateGraph API\\n\\n# StateGraph API\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/how-tos/add-human-in-the-loop.md](docs/docs/cloud/how-tos/add-human-in-the-loop.md)\\n- [docs/docs/cloud/how-tos/configuration_cloud.md](docs/docs/cloud/how-tos/configuration_cloud.md)\\n- [docs/docs/concepts/assistants.md](docs/docs/concepts/assistants.md)\\n- [docs/docs/concepts/human_in_the_loop.md](docs/docs/concepts/human_in_the_loop.md)\\n- [docs/docs/concepts/low_level.md](docs/docs/concepts/low_level.md)\\n- [docs/docs/concepts/persistence.md](docs/docs/concepts/persistence.md)\\n- [docs/docs/concepts/time-travel.md](docs/docs/concepts/time-travel.md)\\n- [docs/docs/how-tos/assets/human_in_loop_parallel.png](docs/docs/how-tos/assets/human_in_loop_parallel.png)\\n- [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md](docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md)\\n- [docs/docs/how-tos/human_in_the_loop/time-travel.md](docs/docs/how-tos/human_in_the_loop/time-travel.md)\\n- [libs/langgraph/langgraph/constants.py](libs/langgraph/langgraph/constants.py)\\n- [libs/langgraph/langgraph/errors.py](libs/langgraph/langgraph/errors.py)\\n- [libs/langgraph/langgraph/func/__init__.py](libs/langgraph/langgraph/func/__init__.py)\\n- [libs/langgraph/langgraph/graph/state.py](libs/langgraph/langgraph/graph/state.py)\\n- [libs/langgraph/langgraph/pregel/__init__.py](libs/langgraph/langgraph/pregel/__init__.py)\\n- [libs/langgraph/langgraph/pregel/debug.py](libs/langgraph/langgraph/pregel/debug.py)\\n- [libs/langgraph/langgraph/pregel/types.py](libs/langgraph/langgraph/pregel/types.py)\\n- [libs/langgraph/langgraph/types.py](libs/langgraph/langgraph/types.py)\\n- [libs/langgraph/tests/test_pregel.py](libs/langgraph/tests/test_pregel.py)\\n- [libs/langgraph/tests/test_pregel_async.py](libs/langgraph/tests/test_pregel_async.py)\\n\\n</details>\\n\\n\\n\\nThe `StateGraph` class provides the primary declarative interface for building stateful workflows in LangGraph. It serves as a high-level builder that compiles into executable `Pregel` instances, transforming user-defined state schemas, nodes, and edges into the underlying message-passing execution system.\\n\\nThis page covers the `StateGraph` class definition, node and edge management, compilation process, and integration with the Pregel runtime engine.\\n\\nFor information about the functional API alternative, see [Functional API](#2.3). For details about the runtime execution engine, see [Pregel Runtime Engine](#2.2).\\n\\n## Core Architecture\\n\\nThe StateGraph API is built around a declarative graph definition pattern where users specify:\\n\\n- **State Schema**: The structure of data that flows through the graph\\n- **Nodes**: Functions that process and update the state  \\n- **Edges**: Control flow between nodes (static or conditional)\\n- **Channels**: Communication pathways with optional reducer functions\\n\\n## StateGraph Class Structure\\n\\nStateGraph Class Architecture\\n```mermaid\\ngraph TB\\n    StateGraph[\\\"StateGraph[StateT, ContextT, InputT, OutputT]\\\"]\\n    StateGraph --> Properties[\\\"Instance Properties\\\"]\\n    StateGraph --> Methods[\\\"Core Methods\\\"] \\n    StateGraph --> Compilation[\\\"Compilation\\\"]\\n    \\n    Properties --> nodes[\\\"nodes: dict[str, StateNodeSpec]\\\"]\\n    Properties --> edges[\\\"edges: set[tuple[str, str]]\\\"]\\n    Properties --> branches[\\\"branches: defaultdict[str, dict[str, BranchSpec]]\\\"]\\n    Properties --> channels[\\\"channels: dict[str, BaseChannel]\\\"]\\n    Properties --> managed[\\\"managed: dict[str, ManagedValueSpec]\\\"]\\n    Properties --> schemas[\\\"schemas: dict[type, dict[str, BaseChannel | ManagedValueSpec]]\\\"]\\n    Properties --> waiting_edges[\\\"waiting_edges: set[tuple[tuple[str, ...], str]]\\\"]\\n    \\n    Methods --> add_node[\\\"add_node()\\\"]\\n    Methods --> add_edge[\\\"add_edge()\\\"]\\n    Methods --> add_conditional_edges[\\\"add_conditional_edges()\\\"]\\n    Methods --> add_sequence[\\\"add_sequence()\\\"]\\n    Methods --> set_entry_point[\\\"set_entry_point()\\\"]\\n    Methods --> set_finish_point[\\\"set_finish_point()\\\"]\\n    \\n    Compilation --> compile_method[\\\"compile()\\\"]\\n    compile_method --> CompiledStateGraph[\\\"CompiledStateGraph (Pregel)\\\"]\\n```\\n\\nThe `StateGraph` class is defined as a generic type with four parameters:\\n- `StateT`: The main state schema type (typically a `TypedDict`)\\n- `ContextT`: Runtime context schema for injected data (optional)\\n- `InputT`: Input validation schema (defaults to `StateT`)\\n- `OutputT`: Output filtering schema (defaults to `StateT`)\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:117-186](), [libs/langgraph/langgraph/graph/state.py:173-186]()\\n\\n## State Management and Channels\\n\\nSchema to Channel Conversion Process\\n```mermaid\\ngraph LR\\n    TypedDict[\\\"TypedDict Schema\\\"] --> _add_schema[\\\"_add_schema()\\\"]\\n    _add_schema --> _get_channels[\\\"_get_channels()\\\"]\\n    _get_channels --> Analysis[\\\"Field Analysis\\\"]\\n    \\n    Analysis --> LastValueChannels[\\\"LastValue channels\\\"]\\n    Analysis --> BinaryOpChannels[\\\"BinaryOperatorAggregate channels\\\"] \\n    Analysis --> ManagedValues[\\\"ManagedValueSpec entries\\\"]\\n    \\n    LastValueChannels --> channels_dict[\\\"self.channels\\\"]\\n    BinaryOpChannels --> channels_dict\\n    ManagedValues --> managed_dict[\\\"self.managed\\\"]\\n    \\n    channels_dict --> PregelChannels[\\\"Pregel Channel System\\\"]\\n    managed_dict --> PregelChannels\\n```\\n\\nThe `_add_schema` method processes `TypedDict` schemas and creates channel mappings:\\n\\n| Field Type | Channel Type | Purpose |\\n|------------|--------------|---------|\\n| `field: type` | `LastValue` | Simple state override |\\n| `field: Annotated[type, reducer]` | `BinaryOperatorAggregate` | Accumulated state with reducer |\\n| `field: Annotated[type, ManagedValue]` | `ManagedValueSpec` | Framework-managed values |\\n\\nThe `_get_channels` function analyzes type annotations and creates the appropriate channel instances, handling both regular fields and `Annotated` types with reducer functions.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:247-277](), [libs/langgraph/langgraph/graph/_internal/_fields.py]()\\n\\n## Node Management\\n\\n### Adding Nodes\\n\\nThe `add_node` method supports multiple overloads for flexible node registration:\\n\\n| Pattern | Description |\\n|---------|-------------|\\n| `add_node(function)` | Infers name from `__name__`, uses `state_schema` as input |\\n| `add_node(\\\"name\\\", function)` | Explicit name, uses `state_schema` as input |  \\n| `add_node(function, input_schema=Schema)` | Custom input schema with inferred name |\\n| `add_node(\\\"name\\\", function, input_schema=Schema)` | Explicit name and custom input schema |\\n\\nNode registration creates `StateNodeSpec` objects with:\\n- `runnable`: The wrapped function via `coerce_to_runnable()`\\n- `input_schema`: Input type validation\\n- `retry_policy`: Retry configuration for failures\\n- `cache_policy`: Caching strategy for results\\n- `metadata`: Additional node metadata\\n- `defer`: Deferred execution flag\\n- `ends`: Possible destination nodes (for visualization)\\n\\nThe method performs validation:\\n- Prevents reserved names (`START`, `END`)\\n- Blocks invalid characters (`NS_SEP`, `NS_END`)\\n- Checks for duplicate node names\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:349-551]()\\n\\n### Adding Node Sequences\\n\\nThe `add_sequence` method provides a convenience wrapper for adding multiple nodes in linear order:\\n\\n```mermaid\\ngraph LR\\n    add_sequence[\\\"add_sequence([node1, node2, node3])\\\"] --> add_node1[\\\"add_node('node1', node1)\\\"]\\n    add_node1 --> add_node2[\\\"add_node('node2', node2)\\\"]\\n    add_node2 --> add_node3[\\\"add_node('node3', node3)\\\"]\\n    add_node3 --> add_edge1[\\\"add_edge('node1', 'node2')\\\"]\\n    add_edge1 --> add_edge2[\\\"add_edge('node2', 'node3')\\\"]\\n```\\n\\nThis method automatically:\\n- Registers each node using `add_node()`\\n- Creates sequential edges between consecutive nodes\\n- Handles both function objects and `(name, function)` tuples\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:653-674]()\\n\\n### Entry and Exit Point Management\\n\\nStateGraph provides dedicated methods for configuring graph entry and exit points:\\n\\n| Method | Purpose | Usage |\\n|--------|---------|-------|\\n| `set_entry_point(node)` | Single starting node | `graph.set_entry_point(\\\"start_node\\\")` |\\n| `set_conditional_entry_point(condition)` | Dynamic starting logic | `graph.set_conditional_entry_point(route_fn)` |\\n| `set_finish_point(node)` | Single ending node | `graph.set_finish_point(\\\"end_node\\\")` |\\n| `set_conditional_finish_point(condition)` | Dynamic ending logic | `graph.set_conditional_finish_point(route_fn)` |\\n\\nThese methods create special edges to/from the reserved `START` and `END` nodes, providing clean graph boundaries without manual edge management to these control nodes.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:675-756]()\\n\\n### Node Input Schema Inference\\n\\nNode Schema Resolution Process\\n```mermaid\\ngraph TD\\n    add_node[\\\"add_node() called\\\"] --> check_input_schema{\\\"input_schema provided?\\\"}\\n    check_input_schema -->|Yes| use_provided[\\\"Use provided schema\\\"]\\n    check_input_schema -->|No| infer_schema[\\\"Infer from function\\\"]\\n    \\n    infer_schema --> get_type_hints[\\\"get_type_hints()\\\"]\\n    get_type_hints --> first_param[\\\"Extract first parameter type\\\"]\\n    first_param --> valid_typeddict{\\\"Valid TypedDict with hints?\\\"}\\n    valid_typeddict -->|Yes| use_inferred[\\\"inferred_input_schema = input_hint\\\"]\\n    valid_typeddict -->|No| use_state_schema[\\\"Use self.state_schema\\\"]\\n    \\n    use_provided --> create_spec[\\\"Create StateNodeSpec\\\"]\\n    use_inferred --> create_spec\\n    use_state_schema --> create_spec\\n    create_spec --> add_to_nodes[\\\"self.nodes[node] = spec\\\"]\\n    create_spec --> add_schema_call[\\\"self._add_schema(input_schema)\\\"]\\n```\\n\\nThe schema inference process examines function signatures using `inspect.signature()` and `get_type_hints()`. If the first parameter has a type annotation that resolves to a `TypedDict` with type hints, it becomes the `inferred_input_schema`. Otherwise, the graph's `state_schema` is used as the default input type.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:466-550]()\\n\\n## Edge Management\\n\\n### Static Edges\\n\\nStatic edges define deterministic connections between nodes. The `add_edge` method handles both single and multi-source patterns:\\n\\n```python\\n# Single source to target\\ngraph.add_edge(\\\"node_a\\\", \\\"node_b\\\")\\n\\n# Multiple sources to single target (wait for ALL)\\ngraph.add_edge([\\\"node_a\\\", \\\"node_b\\\"], \\\"node_c\\\")\\n```\\n\\nEdge Storage Implementation\\n```mermaid\\ngraph TD\\n    add_edge[\\\"add_edge(start_key, end_key)\\\"] --> check_type{\\\"isinstance(start_key, str)?\\\"}\\n    check_type -->|Yes| single_edge[\\\"Single Edge\\\"]\\n    check_type -->|No| multi_edge[\\\"Multiple Edge\\\"]\\n    \\n    single_edge --> validate_single[\\\"Validate START/END constraints\\\"]\\n    validate_single --> add_to_edges[\\\"self.edges.add((start_key, end_key))\\\"]\\n    \\n    multi_edge --> validate_multi[\\\"Validate each start node exists\\\"]\\n    validate_multi --> add_to_waiting[\\\"self.waiting_edges.add((tuple(start_key), end_key))\\\"]\\n    \\n    add_to_edges --> _all_edges[\\\"Combined in _all_edges property\\\"]\\n    add_to_waiting --> _all_edges\\n```\\n\\nThe implementation maintains two collections:\\n- `edges`: Set of `(str, str)` tuples for single-source connections\\n- `waiting_edges`: Set of `(tuple[str, ...], str)` for multi-source synchronization\\n- `_all_edges`: Property that combines both for compilation\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:553-605](), [libs/langgraph/langgraph/graph/state.py:241-245]()\\n\\n### Conditional Edges\\n\\nConditional edges use callable functions to determine routing logic dynamically. The `add_conditional_edges` method converts path functions into `BranchSpec` objects:\\n\\nConditional Edge Processing\\n```mermaid\\ngraph TD\\n    add_conditional_edges[\\\"add_conditional_edges(source, path, path_map)\\\"] --> coerce_to_runnable[\\\"coerce_to_runnable(path)\\\"]\\n    coerce_to_runnable --> get_name[\\\"path.name or 'condition'\\\"]\\n    get_name --> check_duplicate{\\\"name in branches[source]?\\\"}\\n    check_duplicate -->|Yes| raise_error[\\\"Raise ValueError\\\"]\\n    check_duplicate -->|No| create_branch_spec[\\\"BranchSpec.from_path()\\\"]\\n    \\n    create_branch_spec --> store_branch[\\\"self.branches[source][name] = branch_spec\\\"]\\n    store_branch --> check_input_schema{\\\"branch_spec.input_schema exists?\\\"}\\n    check_input_schema -->|Yes| add_schema[\\\"self._add_schema(schema)\\\"]\\n    check_input_schema -->|No| complete[\\\"Complete\\\"]\\n```\\n\\nThe `BranchSpec.from_path()` method handles:\\n- `path_map` parameter for explicit node mapping\\n- Return type annotation analysis for implicit routing\\n- Validation of target node existence\\n\\nConditional edges support dynamic routing through:\\n- **String returns**: Direct node name routing\\n- **Send objects**: Dynamic node invocation with custom state\\n- **Command objects**: Graph control with state updates and routing\\n- **Sequence returns**: Multiple parallel destinations\\n\\nConditional edges are stored in `self.branches`, a `defaultdict[str, dict[str, BranchSpec]]` where:\\n- Outer key: Source node name\\n- Inner key: Condition function name\\n- Value: `BranchSpec` containing routing logic\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:607-651](), [libs/langgraph/langgraph/graph/_branch.py](), [libs/langgraph/langgraph/types.py:276-344](), [libs/langgraph/langgraph/types.py:350-403]()\\n\\n## Compilation Process\\n\\nThe `compile` method transforms the declarative `StateGraph` definition into an executable `Pregel` instance through a multi-stage process:\\n\\nStateGraph Compilation Pipeline\\n```mermaid\\ngraph TB\\n    compile[\\\"compile()\\\"] --> validation[\\\"Graph Validation\\\"]\\n    validation --> build_channels[\\\"Build Channel System\\\"]\\n    build_channels --> build_nodes[\\\"Transform Nodes\\\"]\\n    build_nodes --> build_specs[\\\"Create Pregel Specs\\\"]\\n    build_specs --> create_pregel[\\\"Instantiate Pregel\\\"]\\n    \\n    validation --> check_edges[\\\"Validate edge references\\\"]\\n    validation --> check_reducers[\\\"Validate reducer functions\\\"]\\n    \\n    build_channels --> last_value[\\\"LastValue channels\\\"]\\n    build_channels --> binary_op[\\\"BinaryOperatorAggregate channels\\\"]\\n    build_channels --> managed_vals[\\\"Managed value handling\\\"]\\n    \\n    build_nodes --> state_node_to_pregel[\\\"StateNodeSpec → PregelNode\\\"]\\n    build_nodes --> wrap_runnable[\\\"Channel read/write wrapping\\\"]\\n    \\n    build_specs --> static_edges[\\\"Process self.edges\\\"]\\n    build_specs --> conditional_edges[\\\"Process self.branches\\\"]\\n    build_specs --> waiting_edges[\\\"Process self.waiting_edges\\\"]\\n    \\n    create_pregel --> compiled_state_graph[\\\"CompiledStateGraph (Pregel subclass)\\\"]\\n```\\n\\n### Key Compilation Steps\\n\\n1. **Validation Phase**:\\n   - Checks for unknown nodes referenced in edges\\n   - Validates reducer function signatures\\n   - Ensures graph structural integrity\\n\\n2. **Channel System Construction**:\\n   - Maps schema fields to `BaseChannel` implementations\\n   - Creates special channels like `EphemeralValue` for temporary data\\n   - Sets up managed value specifications\\n\\n3. **Node Transformation**:\\n   - Converts `StateNodeSpec` objects to `PregelNode` instances\\n   - Wraps functions with channel read/write logic\\n   - Applies retry and cache policies\\n\\n4. **Pregel Instance Creation**:\\n   - Assembles all components into a `Pregel` instance\\n   - Configures input/output channel mappings\\n   - Sets up checkpointer and store integrations\\n\\nThe result is a `CompiledStateGraph`, which is actually a `Pregel` instance with additional StateGraph-specific methods.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:891-1050](), [libs/langgraph/langgraph/pregel/__init__.py:1-4]()\\n\\n## Integration with Pregel Runtime\\n\\nStateGraph acts as a declarative builder that compiles into the imperative Pregel execution engine:\\n\\nStateGraph to Pregel Transformation\\n```mermaid\\ngraph LR\\n    StateGraph[\\\"StateGraph Builder\\\"] --> channels_dict[\\\"channels: dict[str, BaseChannel]\\\"]\\n    StateGraph --> nodes_dict[\\\"nodes: dict[str, PregelNode]\\\"]\\n    StateGraph --> input_channels[\\\"input_channels: str | list[str]\\\"]\\n    StateGraph --> output_channels[\\\"output_channels: str | list[str]\\\"]\\n    StateGraph --> stream_channels[\\\"stream_channels: str | list[str]\\\"]\\n    \\n    channels_dict --> Pregel[\\\"Pregel Runtime\\\"]\\n    nodes_dict --> Pregel\\n    input_channels --> Pregel\\n    output_channels --> Pregel\\n    stream_channels --> Pregel\\n    \\n    Pregel --> SyncPregelLoop[\\\"SyncPregelLoop execution\\\"]\\n    Pregel --> AsyncPregelLoop[\\\"AsyncPregelLoop execution\\\"] \\n    Pregel --> ChannelSystem[\\\"Channel-based messaging\\\"]\\n    Pregel --> CheckpointIntegration[\\\"BaseCheckpointSaver integration\\\"]\\n    Pregel --> StoreIntegration[\\\"BaseStore integration\\\"]\\n```\\n\\n### Runtime Execution Model\\n\\nThe compiled `StateGraph` becomes a `Pregel` instance that implements:\\n\\n- **Channel-based Communication**: State updates flow through typed channels like `LastValue` and `BinaryOperatorAggregate`\\n- **Message Passing**: Nodes communicate via `ChannelRead` and `ChannelWrite` operations\\n- **Execution Scheduling**: `PregelRunner` coordinates node execution through `PregelExecutableTask` instances\\n- **Persistence Integration**: Automatic checkpointing via `BaseCheckpointSaver` implementations\\n- **Streaming Support**: Real-time execution progress via multiple `StreamMode` options\\n- **Human-in-the-Loop**: Support for `interrupt()` calls and `Command` objects for workflow control\\n\\nThe `CompiledStateGraph` type alias represents this `Pregel` instance with StateGraph-specific typing and methods preserved.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:891-1050](), [libs/langgraph/langgraph/pregel/__init__.py:1-4](), [libs/langgraph/langgraph/pregel/_loop.py:55](), [libs/langgraph/langgraph/pregel/_runner.py:56](), [libs/langgraph/langgraph/types.py:77-92]()\\n\\n## Advanced Features\\n\\n### Multiple Schema Support\\n\\nStateGraph supports distinct schemas for different interaction points:\\n\\n| Schema Parameter | Purpose | Channel Access |\\n|------------------|---------|----------------|\\n| `state_schema` | Core graph state | Full read/write access |\\n| `input_schema` | Input validation | Input channel mapping only |\\n| `output_schema` | Output filtering | Output channel mapping only |\\n| `context_schema` | Runtime injection | Via `Runtime` object in nodes |\\n\\nEach schema undergoes the same `_add_schema()` processing but with different access controls:\\n- Input/output schemas cannot contain managed values (validated at line 251-257)\\n- Context schema data is injected via the `Runtime` parameter\\n- State schema defines the core channel system\\n\\n### Node Input Schema Specialization\\n\\nIndividual nodes can declare custom input schemas that differ from the graph's `state_schema`:\\n\\nNode Schema Hierarchy\\n```mermaid\\ngraph TD\\n    state_schema[\\\"StateGraph.state_schema\\\"] --> default_input[\\\"Default node input\\\"]\\n    custom_input_schema[\\\"Node-specific input_schema\\\"] --> specialized_input[\\\"Specialized node input\\\"]\\n    \\n    default_input --> channel_reads[\\\"Read from all state channels\\\"]\\n    specialized_input --> filtered_reads[\\\"Read from subset of channels\\\"]\\n    \\n    channel_reads --> node_execution[\\\"Node execution\\\"]\\n    filtered_reads --> node_execution\\n    node_execution --> channel_writes[\\\"Write to state channels\\\"]\\n```\\n\\nThis enables nodes to:\\n- Receive only relevant state fields\\n- Perform input validation at the node level  \\n- Use different TypedDict structures for type safety\\n\\n### Human-in-the-Loop Integration\\n\\nStateGraph supports interactive workflows through several mechanisms:\\n\\n- **Dynamic Interrupts**: Nodes can call `interrupt()` to pause execution and request human input\\n- **Command Objects**: Return `Command` instances to control graph execution with state updates and routing\\n- **Send Objects**: Use `Send` for dynamic node invocation with custom state payloads\\n- **Checkpointing**: Required for interrupt functionality, enabling workflow resumption\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:247-277](), [libs/langgraph/langgraph/graph/state.py:516-549](), [libs/langgraph/langgraph/types.py:405-527](), [libs/langgraph/langgraph/types.py:146-203]()\\n\\n## Error Handling and Validation\\n\\nStateGraph performs extensive validation during both construction and compilation:\\n\\n- **Node Name Validation**: Prevents reserved names (`START`, `END`) and invalid characters\\n- **Edge Validation**: Ensures source and target nodes exist\\n- **Schema Validation**: Validates reducer functions and channel compatibility\\n- **Graph Structure**: Detects orphaned nodes and invalid connections\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:455-465](), [libs/langgraph/tests/test_pregel.py:84-118]()\", \"# Page: Pregel Runtime Engine\\n\\n# Pregel Runtime Engine\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/langgraph/langgraph/constants.py](libs/langgraph/langgraph/constants.py)\\n- [libs/langgraph/langgraph/errors.py](libs/langgraph/langgraph/errors.py)\\n- [libs/langgraph/langgraph/func/__init__.py](libs/langgraph/langgraph/func/__init__.py)\\n- [libs/langgraph/langgraph/graph/state.py](libs/langgraph/langgraph/graph/state.py)\\n- [libs/langgraph/langgraph/pregel/__init__.py](libs/langgraph/langgraph/pregel/__init__.py)\\n- [libs/langgraph/langgraph/pregel/debug.py](libs/langgraph/langgraph/pregel/debug.py)\\n- [libs/langgraph/langgraph/pregel/types.py](libs/langgraph/langgraph/pregel/types.py)\\n- [libs/langgraph/langgraph/types.py](libs/langgraph/langgraph/types.py)\\n- [libs/langgraph/langgraph/utils/__init__.py](libs/langgraph/langgraph/utils/__init__.py)\\n- [libs/langgraph/langgraph/utils/config.py](libs/langgraph/langgraph/utils/config.py)\\n- [libs/langgraph/langgraph/utils/runnable.py](libs/langgraph/langgraph/utils/runnable.py)\\n- [libs/langgraph/tests/__snapshots__/test_large_cases.ambr](libs/langgraph/tests/__snapshots__/test_large_cases.ambr)\\n- [libs/langgraph/tests/__snapshots__/test_pregel.ambr](libs/langgraph/tests/__snapshots__/test_pregel.ambr)\\n- [libs/langgraph/tests/__snapshots__/test_pregel_async.ambr](libs/langgraph/tests/__snapshots__/test_pregel_async.ambr)\\n- [libs/langgraph/tests/test_checkpoint_migration.py](libs/langgraph/tests/test_checkpoint_migration.py)\\n- [libs/langgraph/tests/test_large_cases.py](libs/langgraph/tests/test_large_cases.py)\\n- [libs/langgraph/tests/test_large_cases_async.py](libs/langgraph/tests/test_large_cases_async.py)\\n- [libs/langgraph/tests/test_pregel.py](libs/langgraph/tests/test_pregel.py)\\n- [libs/langgraph/tests/test_pregel_async.py](libs/langgraph/tests/test_pregel_async.py)\\n\\n</details>\\n\\n\\n\\nThe Pregel Runtime Engine is the core execution system that powers LangGraph applications. It implements a distributed computing model based on the Pregel algorithm and Bulk Synchronous Parallel (BSP) processing to orchestrate the execution of graph-based workflows with nodes, channels, and state management.\\n\\nFor information about the higher-level StateGraph API that compiles to Pregel, see [StateGraph API](#2.1). For details about persistence and checkpointing, see [Checkpointing](#5.1).\\n\\n## Architecture Overview\\n\\nThe Pregel Runtime Engine consists of three main components that work together to execute LangGraph applications:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Core Runtime Classes\\\"\\n        Pregel[\\\"Pregel<br/>(Main Graph Class)\\\"]\\n        PregelLoop[\\\"PregelLoop<br/>(Execution Engine)\\\"]\\n        PregelRunner[\\\"PregelRunner<br/>(Task Executor)\\\"]\\n    end\\n    \\n    subgraph \\\"Task Management\\\"\\n        PregelNode[\\\"PregelNode<br/>(Node Wrapper)\\\"]\\n        PregelExecutableTask[\\\"PregelExecutableTask<br/>(Runnable Task)\\\"]\\n        NodeBuilder[\\\"NodeBuilder<br/>(Node Factory)\\\"]\\n    end\\n    \\n    subgraph \\\"State & Communication\\\"\\n        BaseChannel[\\\"BaseChannel<br/>(State Channels)\\\"]\\n        LastValue[\\\"LastValue<br/>(State Storage)\\\"]\\n        Topic[\\\"Topic<br/>(Message Passing)\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Layer\\\"\\n        BaseCheckpointSaver[\\\"BaseCheckpointSaver<br/>(Checkpointing)\\\"]\\n        StateSnapshot[\\\"StateSnapshot<br/>(State View)\\\"]\\n    end\\n    \\n    Pregel --> PregelLoop\\n    PregelLoop --> PregelRunner\\n    Pregel --> PregelNode\\n    PregelRunner --> PregelExecutableTask\\n    PregelNode --> NodeBuilder\\n    \\n    PregelLoop --> BaseChannel\\n    BaseChannel --> LastValue\\n    BaseChannel --> Topic\\n    \\n    PregelLoop --> BaseCheckpointSaver\\n    BaseCheckpointSaver --> StateSnapshot\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/__init__.py:1-4](), [libs/langgraph/langgraph/pregel/_loop.py:55-56](), [libs/langgraph/langgraph/pregel/_runner.py:56-57]()\\n\\n## Core Execution Model\\n\\nThe Pregel Runtime Engine implements a message-passing execution model through the `PregelLoop` class, which orchestrates the execution cycle:\\n\\n### PregelLoop Execution Cycle\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Main Loop Components\\\"\\n        PregelLoop[\\\"PregelLoop<br/>(SyncPregelLoop/AsyncPregelLoop)\\\"]\\n        PregelRunner[\\\"PregelRunner<br/>(Task Execution Manager)\\\"]\\n        CheckpointManager[\\\"Checkpoint Management\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Flow\\\"\\n        StartLoop[\\\"Start Loop<br/>(_loop_step())\\\"]\\n        PrepTasks[\\\"Prepare Tasks<br/>(_prepare_next_tasks())\\\"]\\n        ExecuteTasks[\\\"Execute Tasks<br/>(PregelRunner.tick())\\\"]\\n        ProcessWrites[\\\"Process Writes<br/>(_apply_writes())\\\"]\\n        CreateCheckpoint[\\\"Create Checkpoint<br/>(put_checkpoint())\\\"]\\n        CheckContinue[\\\"Check Continue\\\"]\\n    end\\n    \\n    subgraph \\\"Task Execution Detail\\\"\\n        TaskSubmission[\\\"Submit Tasks<br/>(concurrent.futures)\\\"]\\n        ParallelExec[\\\"Parallel Execution\\\"]\\n        CollectResults[\\\"Collect Results<br/>(task.writes)\\\"]\\n        HandleErrors[\\\"Handle Errors<br/>(ERROR channel)\\\"]\\n    end\\n    \\n    StartLoop --> PrepTasks\\n    PrepTasks --> ExecuteTasks\\n    ExecuteTasks --> ProcessWrites\\n    ProcessWrites --> CreateCheckpoint\\n    CreateCheckpoint --> CheckContinue\\n    CheckContinue --> PrepTasks\\n    \\n    ExecuteTasks --> TaskSubmission\\n    TaskSubmission --> ParallelExec\\n    ParallelExec --> CollectResults\\n    ParallelExec --> HandleErrors\\n    \\n    PregelLoop --> StartLoop\\n    PregelRunner --> TaskSubmission\\n    CheckpointManager --> CreateCheckpoint\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/_loop.py:55-56](), [libs/langgraph/langgraph/pregel/_runner.py:56-57](), [libs/langgraph/tests/test_pregel.py:51-56]()\\n\\n### Execution Phases\\n\\n| Phase | Purpose | Key Functions | Implementation |\\n|-------|---------|---------------|----------------|\\n| **Prepare** | Identify triggered nodes based on channel state | `_prepare_next_tasks()` | Checks channel triggers and creates `PregelExecutableTask` instances |\\n| **Execute** | Run tasks in parallel using futures | `PregelRunner.tick()` / `PregelRunner.atick()` | Uses `concurrent.futures.ThreadPoolExecutor` or asyncio for parallel execution |\\n| **Apply** | Process task outputs and update channels | `_apply_writes()` | Applies task writes to channels using reducers |\\n| **Checkpoint** | Persist current state and create checkpoint | `put_checkpoint()` | Saves state via `BaseCheckpointSaver` |\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/_loop.py:55-56](), [libs/langgraph/langgraph/pregel/_runner.py:56-57]()\\n\\n## Task Execution and Concurrency\\n\\nThe `PregelRunner` class manages concurrent task execution with sophisticated error handling and futures management:\\n\\n### PregelRunner Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"PregelRunner Core\\\"\\n        PregelRunner[\\\"PregelRunner<br/>(Task Manager)\\\"]\\n        FutureManager[\\\"Future Management<br/>(submit/gather)\\\"]\\n        ErrorHandler[\\\"Error Handling<br/>(task cancellation)\\\"]\\n    end\\n    \\n    subgraph \\\"Task Processing\\\"\\n        PregelExecutableTask[\\\"PregelExecutableTask<br/>(Task Definition)\\\"]\\n        TaskSubmission[\\\"Task Submission<br/>(executor.submit())\\\"]\\n        TaskExecution[\\\"Task Execution<br/>(task.proc.invoke())\\\"]\\n        TaskCompletion[\\\"Task Completion<br/>(task.writes)\\\"]\\n    end\\n    \\n    subgraph \\\"Concurrency Control\\\"\\n        ThreadPoolExecutor[\\\"ThreadPoolExecutor<br/>(Sync Tasks)\\\"]\\n        AsyncioTasks[\\\"asyncio.create_task<br/>(Async Tasks)\\\"]\\n        FutureWaiting[\\\"Future Waiting<br/>(concurrent.futures.wait)\\\"]\\n    end\\n    \\n    subgraph \\\"Error Management\\\"\\n        ExceptionCapture[\\\"Exception Capture<br/>(future.exception())\\\"]\\n        TaskCancellation[\\\"Task Cancellation<br/>(cancel remaining)\\\"]\\n        ErrorChannel[\\\"ERROR Channel<br/>(error propagation)\\\"]\\n    end\\n    \\n    PregelRunner --> TaskSubmission\\n    TaskSubmission --> ThreadPoolExecutor\\n    TaskSubmission --> AsyncioTasks\\n    \\n    PregelExecutableTask --> TaskExecution\\n    TaskExecution --> TaskCompletion\\n    TaskExecution --> ExceptionCapture\\n    \\n    ThreadPoolExecutor --> FutureWaiting\\n    AsyncioTasks --> FutureWaiting\\n    FutureWaiting --> TaskCompletion\\n    \\n    ExceptionCapture --> TaskCancellation\\n    TaskCancellation --> ErrorChannel\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/_runner.py:56-57](), [libs/langgraph/tests/test_pregel.py:51-56]()\\n\\n### Concurrency Features\\n\\n- **Parallel Execution**: Multiple tasks run concurrently using `concurrent.futures.ThreadPoolExecutor` (sync) or `asyncio.create_task()` (async)\\n- **Future Management**: Uses standard `concurrent.futures.wait()` and `asyncio.gather()` for task coordination\\n- **Error Handling**: Failed tasks trigger immediate cancellation of remaining tasks in the same superstep\\n- **Timeout Support**: Configurable timeouts with automatic task cancellation on timeout\\n- **Retry Policies**: Built-in retry logic with exponential backoff for failed tasks\\n- **Task Isolation**: Each task runs in isolation with its own execution context\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/_runner.py:56-57](), [libs/langgraph/tests/test_pregel.py:832-885]()\\n\\n## Channel System and Message Passing\\n\\nThe Pregel Runtime Engine uses channels for state communication between nodes:\\n\\n### Channel Communication Flow\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Channel Types\\\"\\n        LastValue[\\\"LastValue<br/>(Single Value Storage)\\\"]\\n        Topic[\\\"Topic<br/>(Message Queue)\\\"]\\n        BinaryOperatorAggregate[\\\"BinaryOperatorAggregate<br/>(Reducer Channel)\\\"]\\n        EphemeralValue[\\\"EphemeralValue<br/>(Temporary Channel)\\\"]\\n    end\\n    \\n    subgraph \\\"Channel Operations\\\"\\n        ChannelRead[\\\"Channel Read<br/>(_read_channels())\\\"]\\n        ChannelWrite[\\\"Channel Write<br/>(_apply_writes())\\\"]\\n        ChannelUpdate[\\\"Channel Update<br/>(update/consume)\\\"]\\n    end\\n    \\n    subgraph \\\"State Management\\\"\\n        Pregel[\\\"Pregel.channels<br/>(Channel Registry)\\\"]\\n        Checkpoint[\\\"Checkpoint<br/>(Persistent State)\\\"]\\n        StateSnapshot[\\\"StateSnapshot<br/>(Point-in-time View)\\\"]\\n    end\\n    \\n    subgraph \\\"Task Integration\\\"\\n        PregelNode[\\\"PregelNode<br/>(Channel Triggers)\\\"]\\n        PregelExecutableTask[\\\"PregelExecutableTask<br/>(Channel Writes)\\\"]\\n        TaskWrites[\\\"task.writes<br/>(Output Buffer)\\\"]\\n    end\\n    \\n    LastValue --> ChannelRead\\n    Topic --> ChannelRead\\n    BinaryOperatorAggregate --> ChannelRead\\n    EphemeralValue --> ChannelRead\\n    \\n    ChannelRead --> PregelNode\\n    PregelNode --> PregelExecutableTask\\n    PregelExecutableTask --> TaskWrites\\n    TaskWrites --> ChannelWrite\\n    \\n    ChannelWrite --> ChannelUpdate\\n    ChannelUpdate --> LastValue\\n    ChannelUpdate --> Topic\\n    ChannelUpdate --> BinaryOperatorAggregate\\n    \\n    Pregel --> ChannelRead\\n    Pregel --> ChannelWrite\\n    ChannelWrite --> Checkpoint\\n    Checkpoint --> StateSnapshot\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/channels/last_value.py:36-49](), [libs/langgraph/langgraph/channels/topic.py:34-38](), [libs/langgraph/tests/test_pregel.py:34-38]()\\n\\n## Streaming and Human-in-the-Loop\\n\\nThe Pregel Runtime Engine provides comprehensive streaming and interrupt capabilities:\\n\\n### Streaming Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Stream Modes\\\"\\n        StreamValues[\\\"values<br/>(Complete State)\\\"]\\n        StreamUpdates[\\\"updates<br/>(Node Updates)\\\"]\\n        StreamDebug[\\\"debug<br/>(Execution Events)\\\"]\\n        StreamTasks[\\\"tasks<br/>(Task Events)\\\"]\\n        StreamCheckpoints[\\\"checkpoints<br/>(State Snapshots)\\\"]\\n    end\\n    \\n    subgraph \\\"Stream Processing\\\"\\n        PregelLoop[\\\"PregelLoop<br/>(Main Execution)\\\"]\\n        StreamWriter[\\\"StreamWriter<br/>(Output Handler)\\\"]\\n        StreamChannels[\\\"stream_channels<br/>(Output Selection)\\\"]\\n    end\\n    \\n    subgraph \\\"Interrupt System\\\"\\n        DynamicInterrupt[\\\"interrupt()<br/>(Dynamic Function)\\\"]\\n        StaticInterrupt[\\\"interrupt_before/after<br/>(Static Configuration)\\\"]\\n        GraphInterrupt[\\\"GraphInterrupt<br/>(Exception Type)\\\"]\\n        InterruptChannel[\\\"INTERRUPT Channel<br/>(Special Channel)\\\"]\\n    end\\n    \\n    subgraph \\\"Human-in-the-Loop Flow\\\"\\n        TaskExecution[\\\"Task Execution\\\"]\\n        InterruptTrigger[\\\"Interrupt Trigger\\\"]\\n        ExecutionPause[\\\"Execution Pause\\\"]\\n        HumanInput[\\\"Human Input<br/>(Command.resume)\\\"]\\n        ExecutionResume[\\\"Execution Resume\\\"]\\n    end\\n    \\n    StreamValues --> StreamWriter\\n    StreamUpdates --> StreamWriter\\n    StreamDebug --> StreamWriter\\n    \\n    PregelLoop --> StreamChannels\\n    StreamChannels --> StreamWriter\\n    \\n    DynamicInterrupt --> GraphInterrupt\\n    StaticInterrupt --> GraphInterrupt\\n    GraphInterrupt --> InterruptChannel\\n    \\n    TaskExecution --> InterruptTrigger\\n    InterruptTrigger --> ExecutionPause\\n    ExecutionPause --> HumanInput\\n    HumanInput --> ExecutionResume\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/types.py:77-91](), [libs/langgraph/langgraph/types.py:405-527](), [libs/langgraph/tests/test_pregel.py:567-617]()\\n\\n## Implementation Details\\n\\n### Core Pregel Class\\n\\nThe `Pregel` class is the main implementation of the runtime engine and provides the standard execution interface:\\n\\n| Method | Purpose | Return Type | Key Features |\\n|--------|---------|-------------|--------------|\\n| `invoke()` | Execute graph synchronously | `Union[dict[str, Any], Any]` | Single execution with checkpointing |\\n| `stream()` | Execute graph with streaming | `Iterator[dict[str, Any]]` | Real-time output streaming |\\n| `ainvoke()` | Execute graph asynchronously | `Union[dict[str, Any], Any]` | Async execution support |\\n| `astream()` | Stream graph asynchronously | `AsyncIterator[dict[str, Any]]` | Async streaming |\\n| `get_state()` | Get current state snapshot | `StateSnapshot` | Thread state access |\\n| `update_state()` | Update graph state | `RunnableConfig` | State modification |\\n| `get_state_history()` | Get state history | `Iterator[StateSnapshot]` | Historical state access |\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/__init__.py:1-4]()\\n\\n### Node and Task System\\n\\nThe runtime engine implements a layered task execution system:\\n\\n#### Core Task Components\\n\\n- **`PregelNode`**: Wrapper around `Runnable` objects that defines channel subscriptions, triggers, and retry policies\\n- **`PregelExecutableTask`**: Runtime task instance containing the node, input data, configuration, and output buffer (`task.writes`)\\n- **`NodeBuilder`**: Fluent API for programmatically constructing nodes with channel subscriptions\\n\\n#### Task Creation and Execution Flow\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Task Definition\\\"\\n        NodeBuilder[\\\"NodeBuilder<br/>.subscribe_to()/.do()/.write_to()\\\"]\\n        PregelNode[\\\"PregelNode<br/>(Runnable + Metadata)\\\"]\\n    end\\n    \\n    subgraph \\\"Task Runtime\\\"\\n        PregelExecutableTask[\\\"PregelExecutableTask<br/>(Instance + Config)\\\"]\\n        TaskExecution[\\\"task.proc.invoke()<br/>(Actual Execution)\\\"]\\n        TaskWrites[\\\"task.writes<br/>(Output Buffer)\\\"]\\n    end\\n    \\n    NodeBuilder --> PregelNode\\n    PregelNode --> PregelExecutableTask\\n    PregelExecutableTask --> TaskExecution\\n    TaskExecution --> TaskWrites\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/pregel/__init__.py:1-4](), [libs/langgraph/langgraph/types.py:239-253](), [libs/langgraph/tests/test_pregel.py:391-461]()\\n\\n### Error Handling and Retry\\n\\nThe runtime provides sophisticated error handling and retry mechanisms:\\n\\n#### Error Handling Components\\n\\n- **`RetryPolicy`**: Configurable retry behavior with exponential backoff, jitter, and custom retry conditions\\n- **`GraphInterrupt`**: Exception type for human-in-the-loop workflow pauses\\n- **`ERROR` Channel**: Special channel for propagating task errors through the graph\\n- **Task Cancellation**: Immediate cancellation of remaining tasks when one task fails\\n\\n#### Retry System\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Retry Configuration\\\"\\n        RetryPolicy[\\\"RetryPolicy<br/>(max_attempts, backoff_factor)\\\"]\\n        RetryCondition[\\\"retry_on<br/>(Exception Types/Callable)\\\"]\\n        RetryInterval[\\\"Retry Intervals<br/>(exponential backoff + jitter)\\\"]\\n    end\\n    \\n    subgraph \\\"Task Execution with Retry\\\"\\n        TaskExecution[\\\"Task Execution<br/>(task.proc.invoke())\\\"]\\n        ExceptionCheck[\\\"Exception Check<br/>(retry_on condition)\\\"]\\n        RetryDelay[\\\"Retry Delay<br/>(time.sleep/asyncio.sleep)\\\"]\\n        MaxAttemptsCheck[\\\"Max Attempts Check\\\"]\\n    end\\n    \\n    subgraph \\\"Error Propagation\\\"\\n        TaskFailure[\\\"Task Failure<br/>(Final Exception)\\\"]\\n        ErrorChannel[\\\"ERROR Channel<br/>(Error Storage)\\\"]\\n        TaskCancellation[\\\"Cancel Other Tasks<br/>(same superstep)\\\"]\\n    end\\n    \\n    RetryPolicy --> TaskExecution\\n    TaskExecution --> ExceptionCheck\\n    ExceptionCheck --> RetryDelay\\n    RetryDelay --> MaxAttemptsCheck\\n    MaxAttemptsCheck --> TaskExecution\\n    \\n    MaxAttemptsCheck --> TaskFailure\\n    TaskFailure --> ErrorChannel\\n    TaskFailure --> TaskCancellation\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/types.py:106-126](), [libs/langgraph/tests/test_pregel.py:831-885](), [libs/langgraph/tests/test_pregel_async.py:568-617]()\\n\\nThe Pregel Runtime Engine provides the foundational execution model that enables LangGraph to run complex, stateful workflows with proper concurrency control, state management, and fault tolerance.\\n\\nThe Pregel Runtime Engine provides the foundational execution model that enables LangGraph to run complex, stateful workflows with proper concurrency control, state management, and fault tolerance.\", \"# Page: Functional API\\n\\n# Functional API\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/langgraph/langgraph/constants.py](libs/langgraph/langgraph/constants.py)\\n- [libs/langgraph/langgraph/errors.py](libs/langgraph/langgraph/errors.py)\\n- [libs/langgraph/langgraph/func/__init__.py](libs/langgraph/langgraph/func/__init__.py)\\n- [libs/langgraph/langgraph/graph/state.py](libs/langgraph/langgraph/graph/state.py)\\n- [libs/langgraph/langgraph/pregel/__init__.py](libs/langgraph/langgraph/pregel/__init__.py)\\n- [libs/langgraph/langgraph/pregel/debug.py](libs/langgraph/langgraph/pregel/debug.py)\\n- [libs/langgraph/langgraph/pregel/types.py](libs/langgraph/langgraph/pregel/types.py)\\n- [libs/langgraph/langgraph/types.py](libs/langgraph/langgraph/types.py)\\n- [libs/langgraph/tests/test_pregel.py](libs/langgraph/tests/test_pregel.py)\\n- [libs/langgraph/tests/test_pregel_async.py](libs/langgraph/tests/test_pregel_async.py)\\n\\n</details>\\n\\n\\n\\nThe Functional API provides a decorator-based interface for building LangGraph workflows using standard Python/JavaScript control flow constructs. It allows developers to add LangGraph's core capabilities—persistence, human-in-the-loop, streaming, and memory management—to existing code with minimal structural changes.\\n\\nThis document covers the implementation and architecture of the Functional API components. For usage examples and tutorials, see the [Use Functional API guide](../how-tos/use-functional-api.md). For information about the graph-based approach, see [StateGraph API](#2.1) and [Core Architecture](#2).\\n\\n## Core Components\\n\\nThe Functional API consists of two primary decorators that work together to create durable, stateful workflows:\\n\\n### @entrypoint Decorator\\n\\nThe `@entrypoint` decorator converts a regular function into a `Pregel` graph instance with full LangGraph capabilities. It serves as the workflow's main execution context and handles state persistence, checkpointing, and execution flow.\\n\\n```mermaid\\ngraph TD\\n    UserFunc[\\\"User Function\\\"] --> EntrypointDecorator[\\\"@entrypoint decorator\\\"]\\n    EntrypointDecorator --> PregelInstance[\\\"Pregel Instance\\\"]\\n    \\n    EntrypointDecorator --> CheckpointerIntegration[\\\"Checkpointer Integration\\\"]\\n    EntrypointDecorator --> StoreIntegration[\\\"Store Integration\\\"] \\n    EntrypointDecorator --> CacheIntegration[\\\"Cache Integration\\\"]\\n    EntrypointDecorator --> ContextSchema[\\\"Context Schema\\\"]\\n    \\n    PregelInstance --> PregelRuntime[\\\"Pregel Runtime Engine\\\"]\\n    PregelRuntime --> Persistence[\\\"State Persistence\\\"]\\n    PregelRuntime --> Streaming[\\\"Stream Outputs\\\"]\\n    PregelRuntime --> HIL[\\\"Human-in-the-Loop\\\"]\\n```\\n\\nThe `entrypoint` class implementation provides configuration for persistence and execution:\\n\\n| Parameter | Purpose | Default |\\n|-----------|---------|---------|\\n| `checkpointer` | Enables state persistence across runs | `None` |\\n| `store` | Key-value store for long-term memory | `None` |\\n| `cache` | Caching layer for optimization | `None` |\\n| `context_schema` | Schema for runtime context data | `None` |\\n| `cache_policy` | Cache policy for workflow results | `None` |\\n| `retry_policy` | Retry configuration for failures | `None` |\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:388-423]()\\n\\n### @task Decorator\\n\\nThe `@task` decorator wraps functions to create cacheable, retryable units of work that return future-like objects. Tasks enable parallel execution and provide durability guarantees when used with checkpointing.\\n\\n```mermaid\\ngraph TD\\n    UserFunc2[\\\"User Function\\\"] --> TaskDecorator[\\\"@task decorator\\\"]\\n    TaskDecorator --> TaskFunction[\\\"_TaskFunction\\\"]\\n    \\n    TaskFunction --> RetryPolicy[\\\"Retry Policy\\\"]\\n    TaskFunction --> CachePolicy[\\\"Cache Policy\\\"]\\n    TaskFunction --> NameConfig[\\\"Name Configuration\\\"]\\n    \\n    TaskFunction --> Call[\\\"call() function\\\"]\\n    Call --> SyncAsyncFuture[\\\"SyncAsyncFuture\\\"]\\n    \\n    SyncAsyncFuture --> TaskResult[\\\"Task Result\\\"]\\n    TaskResult --> Checkpointer[\\\"Saved to Checkpoint\\\"]\\n```\\n\\nThe `_TaskFunction` class wraps the original function and provides:\\n\\n- **Retry mechanisms** via `RetryPolicy` sequences\\n- **Caching capabilities** through `CachePolicy` configuration  \\n- **Future-like execution** returning `SyncAsyncFuture` objects\\n- **Cache management** with `clear_cache()` and `aclear_cache()` methods\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:49-94](), [libs/langgraph/langgraph/func/__init__.py:115-211]()\\n\\n## Architecture Integration\\n\\nThe Functional API integrates with LangGraph's core Pregel execution engine through a transformation process that converts decorated functions into executable graph components.\\n\\n### Entrypoint to Pregel Transformation\\n\\n```mermaid\\ngraph TD\\n    EntrypointFunc[\\\"@entrypoint function\\\"] --> GetRunnable[\\\"get_runnable_for_entrypoint()\\\"]\\n    GetRunnable --> BoundRunnable[\\\"Bound Runnable\\\"]\\n    \\n    BoundRunnable --> InputChannels[\\\"Input Channels\\\"]\\n    BoundRunnable --> OutputChannels[\\\"Output Channels\\\"]\\n    \\n    PregelGraph[\\\"Pregel Graph\\\"] --> StreamMode[\\\"stream_mode: 'updates'\\\"]\\n    PregelGraph --> InputSchema[\\\"Input Schema\\\"]\\n    PregelGraph --> OutputSchema[\\\"Output Schema\\\"]\\n    \\n    InputChannels --> PregelGraph\\n    OutputChannels --> PregelGraph\\n    BoundRunnable --> PregelGraph\\n    \\n    PregelGraph --> CompiledGraph[\\\"Compiled Graph\\\"]\\n    CompiledGraph --> StandardMethods[\\\"invoke/stream/batch methods\\\"]\\n```\\n\\nThe transformation process extracts function signatures to determine input/output types and creates appropriate channel configurations. The `get_runnable_for_entrypoint` function handles this conversion process.\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:478-493](), [libs/langgraph/langgraph/pregel/_call.py:36-37]()\\n\\n### Task Execution Flow\\n\\n```mermaid\\nsequenceDiagram\\n    participant User as \\\"User Code\\\"\\n    participant Task as \\\"_TaskFunction\\\" \\n    participant Call as \\\"call() function\\\"\\n    participant Future as \\\"SyncAsyncFuture\\\"\\n    participant Checkpointer as \\\"BaseCheckpointSaver\\\"\\n    \\n    User->>Task: task_func(*args, **kwargs)\\n    Task->>Call: call(func, retry_policy, cache_policy, ...)\\n    Call->>Future: Create future with execution plan\\n    Future->>User: Return future object\\n    \\n    User->>Future: .result() or await\\n    Future->>Call: Execute with retry/cache logic\\n    Call->>Checkpointer: Save result to checkpoint\\n    Call->>Future: Return result\\n    Future->>User: Return final value\\n```\\n\\nTasks leverage the `call` infrastructure from `pregel._call` module to provide consistent execution semantics with retry policies and caching.\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:74-81](), [libs/langgraph/langgraph/pregel/_call.py]()\\n\\n## State Management and Durability\\n\\n### Checkpointing Behavior\\n\\nThe Functional API implements a different checkpointing strategy compared to the Graph API:\\n\\n| Aspect | Functional API | Graph API |\\n|--------|---------------|-----------|\\n| **Checkpoint Creation** | Task results saved to existing checkpoint | New checkpoint after each superstep |\\n| **State Scope** | Function-scoped, not shared across functions | Shared state schema across all nodes |\\n| **Persistence Granularity** | Individual task results | Complete graph state |\\n\\n### entrypoint.final for State Decoupling\\n\\nThe `entrypoint.final` dataclass allows separating return values from persisted state:\\n\\n```mermaid\\ngraph LR\\n    EntrypointFunc[\\\"Entrypoint Function\\\"] --> Final[\\\"entrypoint.final\\\"]\\n    Final --> ReturnValue[\\\"value: Returned to caller\\\"]\\n    Final --> SaveValue[\\\"save: Persisted to checkpoint\\\"]\\n    \\n    SaveValue --> NextInvocation[\\\"Available as 'previous' parameter\\\"]\\n    ReturnValue --> CallerCode[\\\"Caller receives this value\\\"]\\n```\\n\\nThis mechanism enables workflows where the computation result differs from the state that should be remembered for subsequent runs.\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:424-462]()\\n\\n## Advanced Features\\n\\n### Injectable Parameters\\n\\nEntrypoint functions can request automatic injection of runtime parameters:\\n\\n| Parameter | Type | Description |\\n|-----------|------|-------------|\\n| `config` | `RunnableConfig` | Run-time configuration values |\\n| `previous` | `Any` | Previous return value for the thread |\\n| `runtime` | `Runtime` | Context, store, and writer access |\\n\\nThe parameter injection system analyzes function signatures to determine which parameters to provide automatically.\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:236-241]()\\n\\n### Cache Management\\n\\nTasks support cache policies through the `CachePolicy` generic class, which enables:\\n\\n- **Custom key functions** for cache key generation\\n- **TTL configuration** for cache expiration\\n- **Namespace isolation** via `CACHE_NS_WRITES` constant\\n\\nCache keys are generated using the `identifier` function to create consistent cache namespaces.\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:83-93](), [libs/langgraph/langgraph/_internal/_constants.py:23]()\\n\\n### Error Handling and Retries\\n\\nThe retry system leverages the same infrastructure as the Graph API:\\n\\n- **RetryPolicy sequences** allow multiple retry strategies\\n- **Exponential backoff** with jitter support\\n- **Exception filtering** via `retry_on` callable or exception types\\n- **Integration with Pregel retry mechanisms**\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:193-199](), [libs/langgraph/langgraph/types.py:106-126]()\\n\\n## Implementation Details\\n\\n### Function Signature Analysis\\n\\nThe entrypoint decorator performs comprehensive signature analysis:\\n\\n```mermaid\\ngraph TD\\n    FuncSignature[\\\"Function Signature\\\"] --> ParameterAnalysis[\\\"Parameter Analysis\\\"]\\n    ParameterAnalysis --> FirstParam[\\\"First Parameter: Input Type\\\"]\\n    ParameterAnalysis --> InjectableParams[\\\"Injectable Parameters\\\"]\\n    \\n    FirstParam --> InputSchema[\\\"Input Schema Definition\\\"]\\n    InjectableParams --> RuntimeInjection[\\\"Runtime Injection Setup\\\"]\\n    \\n    ReturnAnnotation[\\\"Return Type Annotation\\\"] --> OutputSchema[\\\"Output Schema Definition\\\"]\\n    ReturnAnnotation --> FinalHandling[\\\"entrypoint.final Handling\\\"]\\n```\\n\\nThe system extracts type hints to configure input/output schemas and determine which parameters require runtime injection.\\n\\nSources: [libs/langgraph/langgraph/func/__init__.py:481-492]()\\n\\n### Future-Like Object Implementation\\n\\nTasks return `SyncAsyncFuture` objects that provide:\\n\\n- **Synchronous resolution** via `.result()` method\\n- **Asynchronous resolution** via `await` syntax\\n- **Lazy execution** until result is requested\\n- **Exception propagation** from task execution\\n\\nThis design allows tasks to be scheduled for execution while maintaining compatibility with both sync and async workflows.\\n\\nSources: [libs/langgraph/langgraph/pregel/_call.py:34](), [libs/langgraph/langgraph/func/__init__.py:74-81]()\", \"# Page: Graph Utilities and Configuration\\n\\n# Graph Utilities and Configuration\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/langgraph/langgraph/constants.py](libs/langgraph/langgraph/constants.py)\\n- [libs/langgraph/langgraph/errors.py](libs/langgraph/langgraph/errors.py)\\n- [libs/langgraph/langgraph/func/__init__.py](libs/langgraph/langgraph/func/__init__.py)\\n- [libs/langgraph/langgraph/graph/state.py](libs/langgraph/langgraph/graph/state.py)\\n- [libs/langgraph/langgraph/pregel/__init__.py](libs/langgraph/langgraph/pregel/__init__.py)\\n- [libs/langgraph/langgraph/pregel/debug.py](libs/langgraph/langgraph/pregel/debug.py)\\n- [libs/langgraph/langgraph/pregel/types.py](libs/langgraph/langgraph/pregel/types.py)\\n- [libs/langgraph/langgraph/types.py](libs/langgraph/langgraph/types.py)\\n- [libs/langgraph/tests/test_pregel.py](libs/langgraph/tests/test_pregel.py)\\n- [libs/langgraph/tests/test_pregel_async.py](libs/langgraph/tests/test_pregel_async.py)\\n\\n</details>\\n\\n\\n\\nThis document covers the supporting utilities and configuration options available for LangGraph graphs, including configuration classes, debugging tools, graph building utilities, and runtime management features. For information about the core StateGraph API, see [StateGraph API](#2.1). For details about the execution engine, see [Pregel Runtime Engine](#2.2).\\n\\n## Configuration Types and Policies\\n\\nLangGraph provides several configuration classes that control graph behavior at runtime, enabling fine-grained control over retry logic, caching, and execution durability.\\n\\n### Core Configuration Classes\\n\\nThe framework defines several key configuration types that can be applied to nodes, tasks, and entire graphs:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Configuration Types\\\"\\n        RP[\\\"RetryPolicy\\\"]\\n        CP[\\\"CachePolicy\\\"]\\n        DM[\\\"Durability\\\"]\\n        SM[\\\"StreamMode\\\"]\\n    end\\n    \\n    subgraph \\\"Application Targets\\\"\\n        Node[\\\"Graph Nodes\\\"]\\n        Task[\\\"@task Functions\\\"]\\n        EP[\\\"@entrypoint Functions\\\"]\\n        Graph[\\\"Compiled Graph\\\"]\\n    end\\n    \\n    RP --> Node\\n    RP --> Task\\n    RP --> EP\\n    CP --> Node\\n    CP --> Task\\n    CP --> EP\\n    DM --> Graph\\n    SM --> Graph\\n    \\n    subgraph \\\"Configuration Properties\\\"\\n        RP_Props[\\\"initial_interval<br/>backoff_factor<br/>max_interval<br/>max_attempts<br/>jitter<br/>retry_on\\\"]\\n        CP_Props[\\\"key_func<br/>ttl\\\"]\\n        DM_Props[\\\"sync | async | exit\\\"]\\n        SM_Props[\\\"values | updates | checkpoints<br/>tasks | debug | messages | custom\\\"]\\n    end\\n    \\n    RP --> RP_Props\\n    CP --> CP_Props\\n    DM --> DM_Props\\n    SM --> SM_Props\\n```\\n\\n**Configuration Class Hierarchy and Usage**\\n\\n`RetryPolicy` controls how failed node executions are retried. It uses an exponential backoff strategy with configurable parameters. The `retry_on` parameter accepts exception types or a callable that determines which exceptions trigger retries.\\n\\n`CachePolicy` enables caching of node results using a configurable key function and time-to-live. The default key function uses pickle-based hashing of the node input.\\n\\n`Durability` modes control when checkpoint writes are persisted: `\\\"sync\\\"` waits for persistence before continuing, `\\\"async\\\"` persists while executing the next step, and `\\\"exit\\\"` only persists when the graph terminates.\\n\\n`StreamMode` determines what data is emitted during graph streaming, from simple values to detailed debug information including task execution and checkpoint creation.\\n\\nSources: [libs/langgraph/langgraph/types.py:106-141](), [libs/langgraph/langgraph/types.py:62-91]()\\n\\n### Configuration Application\\n\\nConfiguration policies can be applied at multiple levels in the graph hierarchy:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Configuration Scope\\\"\\n        GlobalConfig[\\\"Global Graph Config\\\"]\\n        NodeConfig[\\\"Per-Node Config\\\"]\\n        TaskConfig[\\\"Per-Task Config\\\"]\\n    end\\n    \\n    subgraph \\\"StateGraph Methods\\\"\\n        AddNode[\\\"add_node()\\\"]\\n        Compile[\\\"compile()\\\"]\\n    end\\n    \\n    subgraph \\\"Functional API\\\"\\n        TaskDec[\\\"@task decorator\\\"]\\n        EntryDec[\\\"@entrypoint decorator\\\"]\\n    end\\n    \\n    NodeConfig --> AddNode\\n    GlobalConfig --> Compile\\n    TaskConfig --> TaskDec\\n    GlobalConfig --> EntryDec\\n    \\n    subgraph \\\"Configuration Parameters\\\"\\n        RetryParam[\\\"retry_policy\\\"]\\n        CacheParam[\\\"cache_policy\\\"]\\n        MetaParam[\\\"metadata\\\"]\\n        CheckpointParam[\\\"checkpointer\\\"]\\n        StoreParam[\\\"store\\\"]\\n    end\\n    \\n    AddNode --> RetryParam\\n    AddNode --> CacheParam\\n    AddNode --> MetaParam\\n    TaskDec --> RetryParam\\n    TaskDec --> CacheParam\\n    EntryDec --> CheckpointParam\\n    EntryDec --> StoreParam\\n```\\n\\n**Multi-Level Configuration System**\\n\\nConfiguration can be specified at the graph level during compilation, at the node level during addition, or at the task level through decorators. Node-level configuration overrides graph-level defaults, providing granular control over execution behavior.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:349-551](), [libs/langgraph/langgraph/func/__init__.py:115-211]()\\n\\n## Graph Builder Utilities\\n\\nLangGraph provides utility classes for constructing and configuring graphs, with `StateGraph` as the primary builder and `NodeBuilder` for lower-level node construction.\\n\\n### StateGraph Builder Configuration\\n\\nThe `StateGraph` class offers comprehensive configuration options for defining graph structure and behavior:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"StateGraph Configuration\\\"\\n        StateSchema[\\\"state_schema: type[StateT]\\\"]\\n        ContextSchema[\\\"context_schema: type[ContextT]\\\"]\\n        InputSchema[\\\"input_schema: type[InputT]\\\"]\\n        OutputSchema[\\\"output_schema: type[OutputT]\\\"]\\n    end\\n    \\n    subgraph \\\"Node Configuration\\\"\\n        NodeAdd[\\\"add_node()\\\"]\\n        NodeMeta[\\\"metadata: dict[str, Any]\\\"]\\n        NodeRetry[\\\"retry_policy: RetryPolicy\\\"]\\n        NodeCache[\\\"cache_policy: CachePolicy\\\"]\\n        NodeDefer[\\\"defer: bool\\\"]\\n        NodeDest[\\\"destinations: dict | tuple\\\"]\\n    end\\n    \\n    subgraph \\\"Edge Configuration\\\"\\n        DirectEdge[\\\"add_edge()\\\"]\\n        ConditionalEdge[\\\"add_conditional_edges()\\\"]\\n        Sequence[\\\"add_sequence()\\\"]\\n    end\\n    \\n    StateSchema --> NodeAdd\\n    ContextSchema --> NodeAdd\\n    NodeAdd --> NodeMeta\\n    NodeAdd --> NodeRetry\\n    NodeAdd --> NodeCache\\n    NodeAdd --> NodeDefer\\n    NodeAdd --> NodeDest\\n    \\n    NodeAdd --> DirectEdge\\n    NodeAdd --> ConditionalEdge\\n    NodeAdd --> Sequence\\n```\\n\\n**Schema-Based Configuration System**\\n\\nThe StateGraph builder uses TypedDict schemas to define the structure of state, context, input, and output data. These schemas enable type checking and automatic channel creation for state management. Context schemas provide immutable runtime data like user IDs or database connections.\\n\\nThe `defer` parameter allows nodes to be executed only when the graph is about to terminate, useful for cleanup or final processing tasks. The `destinations` parameter provides routing hints for graph visualization without affecting execution logic.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:117-240](), [libs/langgraph/langgraph/graph/state.py:349-551]()\\n\\n### NodeBuilder and Pregel Utilities\\n\\nThe lower-level `NodeBuilder` and `Pregel` classes provide direct control over graph construction and channel management:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"NodeBuilder Operations\\\"\\n        Subscribe[\\\"subscribe_only()\\\"]\\n        SubscribeTo[\\\"subscribe_to()\\\"]\\n        ReadFrom[\\\"read_from()\\\"]\\n        Do[\\\"do()\\\"]\\n        WriteTo[\\\"write_to()\\\"]\\n    end\\n    \\n    subgraph \\\"Channel Types\\\"\\n        LastValue[\\\"LastValue\\\"]\\n        Topic[\\\"Topic\\\"]\\n        BinaryOp[\\\"BinaryOperatorAggregate\\\"]\\n        Ephemeral[\\\"EphemeralValue\\\"]\\n    end\\n    \\n    subgraph \\\"Pregel Construction\\\"\\n        PregelNodes[\\\"nodes: dict\\\"]\\n        PregelChannels[\\\"channels: dict\\\"]\\n        PregelInput[\\\"input_channels\\\"]\\n        PregelOutput[\\\"output_channels\\\"]\\n    end\\n    \\n    Subscribe --> LastValue\\n    SubscribeTo --> Topic\\n    Do --> BinaryOp\\n    WriteTo --> Ephemeral\\n    \\n    Subscribe --> PregelNodes\\n    ReadFrom --> PregelChannels\\n    WriteTo --> PregelInput\\n    Do --> PregelOutput\\n```\\n\\n**Low-Level Graph Construction**\\n\\n`NodeBuilder` provides a fluent API for constructing nodes that read from and write to specific channels. The `Pregel` class directly manages the mapping between nodes and channels, offering maximum control over graph execution semantics.\\n\\nThis lower-level API is primarily used internally by `StateGraph` but can be used directly for advanced use cases requiring custom channel behaviors or non-standard state management patterns.\\n\\nSources: [libs/langgraph/langgraph/pregel/__init__.py:1-4](), [libs/langgraph/tests/test_pregel.py:391-420]()\\n\\n## Runtime Configuration and Context Management\\n\\nLangGraph provides sophisticated runtime configuration through context schemas, metadata management, and execution tags.\\n\\n### Context and Metadata Systems\\n\\nRuntime context and metadata flow through the graph execution system:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Runtime Context\\\"\\n        ContextSchema[\\\"context_schema\\\"]\\n        RuntimeCtx[\\\"Runtime[Context]\\\"]\\n        ConfigCtx[\\\"RunnableConfig\\\"]\\n    end\\n    \\n    subgraph \\\"Metadata Management\\\"\\n        NodeMeta[\\\"Node Metadata\\\"]\\n        CheckpointMeta[\\\"Checkpoint Metadata\\\"]\\n        TaskMeta[\\\"Task Metadata\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Tags\\\"\\n        TagHidden[\\\"TAG_HIDDEN\\\"]\\n        TagNostream[\\\"TAG_NOSTREAM\\\"]\\n        CustomTags[\\\"Custom Tags\\\"]\\n    end\\n    \\n    ContextSchema --> RuntimeCtx\\n    RuntimeCtx --> ConfigCtx\\n    \\n    NodeMeta --> CheckpointMeta\\n    CheckpointMeta --> TaskMeta\\n    \\n    TagHidden --> NodeMeta\\n    TagNostream --> NodeMeta\\n    CustomTags --> NodeMeta\\n    \\n    subgraph \\\"Configuration Keys\\\"\\n        ThreadId[\\\"thread_id\\\"]\\n        CheckpointNS[\\\"checkpoint_ns\\\"]\\n        CheckpointId[\\\"checkpoint_id\\\"]\\n    end\\n    \\n    ConfigCtx --> ThreadId\\n    ConfigCtx --> CheckpointNS\\n    ConfigCtx --> CheckpointId\\n```\\n\\n**Context Schema and Runtime Management**\\n\\nContext schemas define immutable data available to all nodes during execution. Unlike state, context data cannot be modified by nodes and is typically used for configuration, user information, or shared resources like database connections.\\n\\nExecution tags like `TAG_HIDDEN` and `TAG_NOSTREAM` control visibility and streaming behavior for specific nodes. `TAG_HIDDEN` prevents nodes from appearing in traces, while `TAG_NOSTREAM` disables streaming for chat models.\\n\\nSources: [libs/langgraph/langgraph/constants.py:24-31](), [libs/langgraph/langgraph/graph/state.py:187-240]()\\n\\n### Configuration Constants and Keys\\n\\nLangGraph defines several constants for graph configuration and internal operation:\\n\\n| Constant | Purpose | Usage |\\n|----------|---------|-------|\\n| `START` | Entry point identifier | Graph structure definition |\\n| `END` | Exit point identifier | Graph structure definition |\\n| `TAG_HIDDEN` | Hide from tracing | Node configuration |\\n| `TAG_NOSTREAM` | Disable streaming | Node configuration |\\n| `CONFIG_KEY_CHECKPOINTER` | Checkpointer config key | Internal configuration |\\n| `CONF` | Configuration namespace | Internal configuration |\\n| `TASKS` | Task namespace | Internal configuration |\\n\\nSources: [libs/langgraph/langgraph/constants.py:12-31]()\\n\\n## Debugging and Inspection Utilities\\n\\nLangGraph provides comprehensive debugging tools for inspecting graph execution, task results, and checkpoint states.\\n\\n### Debug Output and Task Inspection\\n\\nThe debugging system offers detailed visibility into graph execution:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Debug Data Types\\\"\\n        TaskPayload[\\\"TaskPayload\\\"]\\n        TaskResultPayload[\\\"TaskResultPayload\\\"]\\n        CheckpointTask[\\\"CheckpointTask\\\"]\\n        CheckpointPayload[\\\"CheckpointPayload\\\"]\\n    end\\n    \\n    subgraph \\\"Debug Functions\\\"\\n        MapDebugTasks[\\\"map_debug_tasks()\\\"]\\n        MapDebugResults[\\\"map_debug_task_results()\\\"]\\n        MapDebugCheckpoint[\\\"map_debug_checkpoint()\\\"]\\n        TasksWithWrites[\\\"tasks_w_writes()\\\"]\\n    end\\n    \\n    subgraph \\\"Stream Mode Debug\\\"\\n        DebugMode[\\\"stream_mode='debug'\\\"]\\n        TasksMode[\\\"stream_mode='tasks'\\\"]\\n        CheckpointsMode[\\\"stream_mode='checkpoints'\\\"]\\n    end\\n    \\n    TaskPayload --> MapDebugTasks\\n    TaskResultPayload --> MapDebugResults\\n    CheckpointPayload --> MapDebugCheckpoint\\n    CheckpointTask --> TasksWithWrites\\n    \\n    MapDebugTasks --> DebugMode\\n    MapDebugResults --> TasksMode\\n    MapDebugCheckpoint --> CheckpointsMode\\n```\\n\\n**Task and Checkpoint Debugging**\\n\\nThe debugging system captures detailed information about task execution, including inputs, outputs, errors, and interrupts. `TaskPayload` contains task identification and input data, while `TaskResultPayload` includes execution results and any errors encountered.\\n\\n`CheckpointPayload` provides comprehensive checkpoint state information, including channel values, metadata, next tasks, and parent configuration. This enables detailed inspection of graph state at any point in execution.\\n\\nSources: [libs/langgraph/langgraph/pregel/debug.py:31-61](), [libs/langgraph/langgraph/pregel/debug.py:116-178]()\\n\\n### Colored Output and Formatting\\n\\nDebug utilities include text formatting for enhanced console output:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Text Formatting\\\"\\n        ColorMapping[\\\"COLOR_MAPPING\\\"]\\n        GetColored[\\\"get_colored_text()\\\"]\\n        GetBolded[\\\"get_bolded_text()\\\"]\\n    end\\n    \\n    subgraph \\\"Color Options\\\"\\n        Black[\\\"black: '0;30'\\\"]\\n        Red[\\\"red: '0;31'\\\"]\\n        Green[\\\"green: '0;32'\\\"]\\n        Yellow[\\\"yellow: '0;33'\\\"]\\n        Blue[\\\"blue: '0;34'\\\"]\\n        Magenta[\\\"magenta: '0;35'\\\"]\\n        Cyan[\\\"cyan: '0;36'\\\"]\\n        White[\\\"white: '0;37'\\\"]\\n        Gray[\\\"gray: '1;30'\\\"]\\n    end\\n    \\n    ColorMapping --> GetColored\\n    ColorMapping --> GetBolded\\n    \\n    Black --> ColorMapping\\n    Red --> ColorMapping\\n    Green --> ColorMapping\\n    Yellow --> ColorMapping\\n    Blue --> ColorMapping\\n    Magenta --> ColorMapping\\n    Cyan --> ColorMapping\\n    White --> ColorMapping\\n    Gray --> ColorMapping\\n```\\n\\n**Console Output Enhancement**\\n\\nThe debugging utilities include ANSI color code mapping for enhanced console output. These functions format debug information with colors and bold text to improve readability during development and troubleshooting.\\n\\nSources: [libs/langgraph/langgraph/pregel/debug.py:252-272]()\\n\\n## Error Handling and Validation\\n\\nLangGraph provides comprehensive error handling and validation utilities for robust graph operation.\\n\\n### Error Types and Codes\\n\\nThe framework defines specific error types with associated troubleshooting codes:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Error Categories\\\"\\n        GraphRecursion[\\\"GraphRecursionError\\\"]\\n        InvalidUpdate[\\\"InvalidUpdateError\\\"]\\n        GraphBubbleUp[\\\"GraphBubbleUp\\\"]\\n        GraphInterrupt[\\\"GraphInterrupt\\\"]\\n        EmptyInput[\\\"EmptyInputError\\\"]\\n        TaskNotFound[\\\"TaskNotFound\\\"]\\n    end\\n    \\n    subgraph \\\"Error Codes\\\"\\n        RecursionCode[\\\"GRAPH_RECURSION_LIMIT\\\"]\\n        ConcurrentCode[\\\"INVALID_CONCURRENT_GRAPH_UPDATE\\\"]\\n        ReturnCode[\\\"INVALID_GRAPH_NODE_RETURN_VALUE\\\"]\\n        SubgraphCode[\\\"MULTIPLE_SUBGRAPHS\\\"]\\n        ChatCode[\\\"INVALID_CHAT_HISTORY\\\"]\\n    end\\n    \\n    subgraph \\\"Error Handling\\\"\\n        CreateError[\\\"create_error_message()\\\"]\\n        TroubleshootLink[\\\"troubleshooting links\\\"]\\n    end\\n    \\n    GraphRecursion --> RecursionCode\\n    InvalidUpdate --> ConcurrentCode\\n    InvalidUpdate --> ReturnCode\\n    \\n    CreateError --> TroubleshootLink\\n    RecursionCode --> CreateError\\n    ConcurrentCode --> CreateError\\n```\\n\\n**Structured Error Management**\\n\\nLangGraph uses structured error codes that link to specific troubleshooting documentation. `GraphRecursionError` prevents infinite loops by limiting execution steps, while `InvalidUpdateError` catches concurrent modification issues and invalid return values.\\n\\n`GraphInterrupt` and `GraphBubbleUp` are internal exceptions used for control flow, particularly for human-in-the-loop workflows and interrupt handling.\\n\\nSources: [libs/langgraph/langgraph/errors.py:29-131]()\", \"# Page: State Management and Channels\\n\\n# State Management and Channels\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/langgraph/langgraph/constants.py](libs/langgraph/langgraph/constants.py)\\n- [libs/langgraph/langgraph/errors.py](libs/langgraph/langgraph/errors.py)\\n- [libs/langgraph/langgraph/func/__init__.py](libs/langgraph/langgraph/func/__init__.py)\\n- [libs/langgraph/langgraph/graph/state.py](libs/langgraph/langgraph/graph/state.py)\\n- [libs/langgraph/langgraph/pregel/__init__.py](libs/langgraph/langgraph/pregel/__init__.py)\\n- [libs/langgraph/langgraph/pregel/debug.py](libs/langgraph/langgraph/pregel/debug.py)\\n- [libs/langgraph/langgraph/pregel/types.py](libs/langgraph/langgraph/pregel/types.py)\\n- [libs/langgraph/langgraph/types.py](libs/langgraph/langgraph/types.py)\\n- [libs/langgraph/langgraph/utils/__init__.py](libs/langgraph/langgraph/utils/__init__.py)\\n- [libs/langgraph/langgraph/utils/config.py](libs/langgraph/langgraph/utils/config.py)\\n- [libs/langgraph/langgraph/utils/runnable.py](libs/langgraph/langgraph/utils/runnable.py)\\n- [libs/langgraph/tests/__snapshots__/test_large_cases.ambr](libs/langgraph/tests/__snapshots__/test_large_cases.ambr)\\n- [libs/langgraph/tests/__snapshots__/test_pregel.ambr](libs/langgraph/tests/__snapshots__/test_pregel.ambr)\\n- [libs/langgraph/tests/__snapshots__/test_pregel_async.ambr](libs/langgraph/tests/__snapshots__/test_pregel_async.ambr)\\n- [libs/langgraph/tests/test_checkpoint_migration.py](libs/langgraph/tests/test_checkpoint_migration.py)\\n- [libs/langgraph/tests/test_large_cases.py](libs/langgraph/tests/test_large_cases.py)\\n- [libs/langgraph/tests/test_large_cases_async.py](libs/langgraph/tests/test_large_cases_async.py)\\n- [libs/langgraph/tests/test_pregel.py](libs/langgraph/tests/test_pregel.py)\\n- [libs/langgraph/tests/test_pregel_async.py](libs/langgraph/tests/test_pregel_async.py)\\n\\n</details>\\n\\n\\n\\nState management is the foundation of LangGraph's execution model, enabling nodes to communicate and share data through a structured channel system. This document covers how state flows through graphs via channels, reducers like `add_messages`, and state persistence mechanisms.\\n\\nFor information about the underlying Pregel execution engine, see [Pregel Runtime Engine](#2.2). For detailed persistence backends and checkpointing, see [Persistence System](#5).\\n\\n## State Schema and Channel Architecture\\n\\nLangGraph uses a **state schema** to define the structure and behavior of data flowing through a graph. The state schema is typically a `TypedDict` with optional `Annotated` fields that specify reducers for combining multiple updates to the same state key.\\n\\n```mermaid\\ngraph TD\\n    Schema[\\\"State Schema (TypedDict)\\\"] --> Parse[\\\"_get_channels()\\\"]\\n    Parse --> Channels[\\\"Channel Objects\\\"]\\n    Parse --> Managed[\\\"Managed Values\\\"]\\n    \\n    Channels --> LastValue[\\\"LastValue Channel\\\"]\\n    Channels --> BinaryOp[\\\"BinaryOperatorAggregate Channel\\\"] \\n    Channels --> Topic[\\\"Topic Channel\\\"]\\n    Channels --> Ephemeral[\\\"EphemeralValue Channel\\\"]\\n    \\n    Schema --> Annotated[\\\"Annotated[type, reducer]\\\"]\\n    Annotated --> Reducer[\\\"Reducer Function\\\"]\\n    Reducer --> Combine[\\\"State Update Combination\\\"]\\n```\\n\\n**State Schema to Channel Conversion**\\n\\nThe `StateGraph` constructor processes the state schema through the `_get_channels` function, which converts TypedDict annotations into concrete channel implementations:\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:247-278]()\\n\\n## Channel Types\\n\\nLangGraph provides several channel types, each optimized for different state update patterns:\\n\\n| Channel Type | Use Case | Behavior |\\n|--------------|----------|----------|\\n| `LastValue` | Simple state fields | Stores the most recent value written |\\n| `BinaryOperatorAggregate` | Accumulating values | Combines values using binary operators (add, multiply, etc.) |\\n| `Topic` | Message queues | Accumulates updates into a sequence |\\n| `EphemeralValue` | Temporary data | Exists only during graph execution |\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Channel Types\\\"\\n        LV[\\\"LastValue<br/>last_name: str\\\"]\\n        BOA[\\\"BinaryOperatorAggregate<br/>total: Annotated[int, add]\\\"]\\n        Topic[\\\"Topic<br/>messages: list\\\"]\\n        Ephemeral[\\\"EphemeralValue<br/>temp_data\\\"]\\n    end\\n    \\n    subgraph \\\"Update Behavior\\\"\\n        LV --> Replace[\\\"Replaces previous value\\\"]\\n        BOA --> Combine[\\\"Combines with reducer\\\"]\\n        Topic --> Append[\\\"Appends to sequence\\\"]\\n        Ephemeral --> Temp[\\\"Temporary storage\\\"]\\n    end\\n```\\n\\n**LastValue Channels**\\n\\nMost basic channel type that simply stores the last value written. Used for simple state fields that don't need special combination logic.\\n\\n**BinaryOperatorAggregate Channels**\\n\\nUsed when multiple nodes need to contribute to the same state field. The channel applies a binary operator (like `operator.add`) to combine incoming updates with the existing value.\\n\\n**Topic Channels**\\n\\nAccumulate multiple values into a sequence, useful for collecting results from parallel nodes or maintaining message histories.\\n\\nSources: [libs/langgraph/langgraph/channels/last_value.py](), [libs/langgraph/langgraph/channels/binop.py](), [libs/langgraph/langgraph/channels/topic.py](), [libs/langgraph/langgraph/channels/ephemeral_value.py]()\\n\\n## State Reducers\\n\\nReducers are functions that define how multiple updates to the same state key should be combined. They enable sophisticated state update patterns beyond simple replacement.\\n\\n```mermaid\\ngraph TD\\n    Node1[\\\"Node A<br/>returns {count: 5}\\\"] --> Reducer[\\\"operator.add\\\"]\\n    Node2[\\\"Node B<br/>returns {count: 3}\\\"] --> Reducer\\n    CurrentState[\\\"Current State<br/>{count: 2}\\\"] --> Reducer\\n    \\n    Reducer --> Result[\\\"Final State<br/>{count: 10}\\\"]\\n    \\n    subgraph \\\"Common Reducers\\\"\\n        AddOp[\\\"operator.add<br/>Numeric accumulation\\\"]\\n        AddMsg[\\\"add_messages<br/>Message handling\\\"]\\n        Custom[\\\"custom_reducer<br/>Custom logic\\\"]\\n    end\\n```\\n\\n**Built-in Reducers**\\n\\nThe most common reducer is `add_messages`, specifically designed for handling message lists with proper deduplication and ordering:\\n\\n```python\\nclass MessagesState(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n```\\n\\n**Custom Reducers**\\n\\nReducers must be binary functions that take the current value and a new value, returning the combined result:\\n\\n```python\\ndef custom_reducer(current: list, new: list) -> list:\\n    return current + new\\n\\nclass State(TypedDict):\\n    items: Annotated[list, custom_reducer]\\n```\\n\\nSources: [libs/langgraph/langgraph/graph/message.py:49](), [libs/langgraph/tests/test_pregel.py:97-103]()\\n\\n## State Flow Through Graph Execution\\n\\nState flows through the graph via a channel-based communication system managed by the Pregel execution engine. Each execution step involves reading from channels, executing nodes, and writing updates back to channels.\\n\\n```mermaid\\nsequenceDiagram\\n    participant Input as \\\"Input State\\\"\\n    participant Channels as \\\"Channel System\\\"  \\n    participant Node as \\\"Graph Node\\\"\\n    participant Reducer as \\\"State Reducer\\\"\\n    participant Output as \\\"Updated State\\\"\\n    \\n    Input->>Channels: Initial state write\\n    Channels->>Node: Read current state\\n    Node->>Node: Execute logic\\n    Node->>Channels: Write partial update\\n    Channels->>Reducer: Apply reducer function\\n    Reducer->>Channels: Store combined result\\n    Channels->>Output: Provide updated state\\n```\\n\\n**Channel Read/Write Operations**\\n\\nThe Pregel engine manages state flow through `ChannelRead` and `ChannelWrite` operations that abstract the channel access pattern:\\n\\n- `ChannelRead` - Extracts current values from specified channels to provide node input\\n- `ChannelWrite` - Applies node outputs to channels, invoking reducers when necessary\\n\\n**State Update Process**\\n\\n1. **Input Processing**: Initial state is written to appropriate channels based on the input schema\\n2. **Node Execution**: Nodes read current state through channel reads and execute their logic  \\n3. **Update Application**: Node outputs are written to channels, triggering reducer functions\\n4. **State Consolidation**: Updated channel values form the new graph state\\n\\nSources: [libs/langgraph/langgraph/pregel/_read.py](), [libs/langgraph/langgraph/pregel/_write.py](), [libs/langgraph/tests/test_pregel.py:391-420]()\\n\\n## State Updates and Partial Returns\\n\\nNodes in a StateGraph return partial state updates rather than complete state objects. This enables fine-grained control over which state fields are modified and supports parallel execution patterns.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Node Output Patterns\\\"\\n        Dict[\\\"Dictionary Update<br/>{key: value}\\\"]\\n        Command[\\\"Command Object<br/>Command(update={...})\\\"]\\n        None[\\\"None<br/>(no update)\\\"]\\n    end\\n    \\n    subgraph \\\"Update Processing\\\"\\n        Dict --> Validate[\\\"Validate Against Schema\\\"]\\n        Command --> Extract[\\\"Extract Update\\\"]\\n        None --> Skip[\\\"Skip Update\\\"]\\n        \\n        Validate --> Apply[\\\"Apply to Channels\\\"]\\n        Extract --> Apply\\n        Apply --> Reduce[\\\"Apply Reducers\\\"]\\n    end\\n    \\n    subgraph \\\"Channel Updates\\\"\\n        Reduce --> LastVal[\\\"LastValue: Replace\\\"]\\n        Reduce --> Binary[\\\"BinaryOp: Combine\\\"]\\n        Reduce --> TopicCh[\\\"Topic: Append\\\"]\\n    end\\n```\\n\\n**Partial Update Validation**\\n\\nThe StateGraph validates node outputs against the state schema, ensuring type safety and proper channel targeting. Invalid updates raise `InvalidUpdateError` exceptions:\\n\\n**Multiple Node Updates**\\n\\nWhen multiple nodes run in parallel and update the same state key, the channel's reducer function determines how updates are combined. For `LastValue` channels, this results in an error, while aggregate channels combine the updates appropriately.\\n\\n**Update Resolution Order**\\n\\nState updates are applied in a deterministic order based on task completion, ensuring reproducible execution even with parallel nodes.\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:117-138](), [libs/langgraph/tests/test_pregel.py:707-738](), [libs/langgraph/langgraph/errors.py:68-77]()\\n\\n## State Persistence and Checkpointing\\n\\nState persistence enables graphs to save and restore execution state across runs, supporting features like interrupts, human-in-the-loop workflows, and failure recovery.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"State Persistence Flow\\\"\\n        Execution[\\\"Graph Execution\\\"] --> Checkpoint[\\\"Create Checkpoint\\\"]\\n        Checkpoint --> Serialize[\\\"Serialize Channel Values\\\"]\\n        Serialize --> Store[\\\"Store in Checkpointer\\\"]\\n        \\n        Store --> Restore[\\\"Restore from Checkpoint\\\"]\\n        Restore --> Deserialize[\\\"Deserialize Channel Values\\\"]\\n        Deserialize --> Resume[\\\"Resume Execution\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Components\\\"\\n        Values[\\\"Channel Values\\\"]\\n        Metadata[\\\"Execution Metadata\\\"]\\n        Tasks[\\\"Pending Tasks\\\"]\\n        Config[\\\"Run Configuration\\\"]\\n    end\\n    \\n    Values --> Checkpoint\\n    Metadata --> Checkpoint\\n    Tasks --> Checkpoint  \\n    Config --> Checkpoint\\n```\\n\\n**Channel Serialization**\\n\\nThe checkpointing system serializes channel values using the configured serializer, typically `JsonPlusSerializer`. Channels must support serialization of their contained values for persistence to work correctly.\\n\\n**State Restoration** \\n\\nWhen resuming from a checkpoint, the channel system restores the exact state that existed at checkpoint creation, enabling seamless continuation of graph execution.\\n\\n**Durability Modes**\\n\\nLangGraph supports multiple durability modes that control when state is persisted:\\n- `\\\"sync\\\"` - Persist before each step\\n- `\\\"async\\\"` - Persist asynchronously during execution  \\n- `\\\"exit\\\"` - Persist only when graph exits\\n\\nFor detailed information about persistence backends and checkpointing mechanisms, see [Persistence System](#5).\\n\\nSources: [libs/langgraph/langgraph/checkpoint/base.py](), [libs/langgraph/langgraph/types.py:62-66](), [libs/langgraph/tests/test_pregel.py:760-829]()\", \"# Page: Human-in-the-Loop and Control Flow\\n\\n# Human-in-the-Loop and Control Flow\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/how-tos/add-human-in-the-loop.md](docs/docs/cloud/how-tos/add-human-in-the-loop.md)\\n- [docs/docs/cloud/how-tos/configuration_cloud.md](docs/docs/cloud/how-tos/configuration_cloud.md)\\n- [docs/docs/concepts/assistants.md](docs/docs/concepts/assistants.md)\\n- [docs/docs/concepts/human_in_the_loop.md](docs/docs/concepts/human_in_the_loop.md)\\n- [docs/docs/concepts/low_level.md](docs/docs/concepts/low_level.md)\\n- [docs/docs/concepts/persistence.md](docs/docs/concepts/persistence.md)\\n- [docs/docs/concepts/time-travel.md](docs/docs/concepts/time-travel.md)\\n- [docs/docs/how-tos/assets/human_in_loop_parallel.png](docs/docs/how-tos/assets/human_in_loop_parallel.png)\\n- [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md](docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md)\\n- [docs/docs/how-tos/human_in_the_loop/time-travel.md](docs/docs/how-tos/human_in_the_loop/time-travel.md)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's advanced execution control mechanisms that enable sophisticated workflow orchestration through interrupts, dynamic routing, time travel, and human intervention. The system provides precise control over graph execution flow through the `interrupt` function, `Command` primitive, `Send` primitive, static breakpoints, and time travel capabilities.\\n\\nThe execution control system is built on LangGraph's persistence layer, allowing graphs to pause execution, save state, and resume from specific points in the execution history. This enables complex patterns including human approval workflows, state editing, debugging with breakpoints, and exploring alternative execution paths.\\n\\nFor information about the underlying persistence system, see [Persistence System](#5). For details about state management, see [State Management and Channels](#3).\\n\\n## Core Architecture\\n\\nHuman-in-the-loop workflows in LangGraph are built on top of the persistence system, using checkpointing to save graph state at each step and enable resumption after human intervention.\\n\\n### Execution Control System Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Control Flow Primitives\\\"\\n        interrupt_func[\\\"interrupt()\\\"]\\n        Command[\\\"Command\\\"]\\n        Send[\\\"Send\\\"]\\n        GraphInterrupt[\\\"GraphInterrupt\\\"]\\n        static_interrupts[\\\"interrupt_before/interrupt_after\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Layer\\\"\\n        BaseCheckpointSaver[\\\"BaseCheckpointSaver\\\"]\\n        StateSnapshot[\\\"StateSnapshot\\\"]\\n        PregelTask[\\\"PregelTask\\\"]\\n        Interrupt[\\\"Interrupt\\\"]\\n        CONFIG_KEY_SCRATCHPAD[\\\"CONFIG_KEY_SCRATCHPAD\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Layer\\\"\\n        Pregel[\\\"Pregel\\\"]\\n        StateGraph[\\\"StateGraph\\\"]\\n        CompiledStateGraph[\\\"CompiledStateGraph\\\"]\\n        add_conditional_edges[\\\"add_conditional_edges\\\"]\\n    end\\n    \\n    subgraph \\\"Storage Implementations\\\"\\n        InMemorySaver[\\\"InMemorySaver\\\"]\\n        SqliteSaver[\\\"SqliteSaver\\\"] \\n        PostgresSaver[\\\"PostgresSaver\\\"]\\n        AsyncSqliteSaver[\\\"AsyncSqliteSaver\\\"]\\n        AsyncPostgresSaver[\\\"AsyncPostgresSaver\\\"]\\n    end\\n    \\n    interrupt_func --> GraphInterrupt\\n    GraphInterrupt --> Interrupt\\n    Command --> CompiledStateGraph\\n    Send --> add_conditional_edges\\n    static_interrupts --> StateGraph\\n    CONFIG_KEY_SCRATCHPAD --> interrupt_func\\n    BaseCheckpointSaver --> StateSnapshot\\n    StateSnapshot --> PregelTask\\n    PregelTask --> Interrupt\\n    Pregel --> BaseCheckpointSaver\\n    BaseCheckpointSaver --> InMemorySaver\\n    BaseCheckpointSaver --> SqliteSaver\\n    BaseCheckpointSaver --> PostgresSaver\\n    BaseCheckpointSaver --> AsyncSqliteSaver\\n    BaseCheckpointSaver --> AsyncPostgresSaver\\n    add_conditional_edges --> Pregel\\n```\\n\\nSources: [langgraph/types.py:506-529](), [langgraph/errors.py:84-91](), [langgraph/graph/state.py:688-705](), [langgraph/checkpoint/base.py:43-279](), [langgraph/_internal/_constants.py:1-5]()\\n\\n### Thread-Based Execution and Resume Flow\\n\\n```mermaid\\nsequenceDiagram\\n    participant User as \\\"User\\\"\\n    participant CompiledStateGraph as \\\"CompiledStateGraph\\\"\\n    participant Node as \\\"Node Function\\\"\\n    participant interrupt_func as \\\"interrupt()\\\"\\n    participant CONFIG_KEY_SCRATCHPAD as \\\"CONFIG_KEY_SCRATCHPAD\\\"\\n    participant BaseCheckpointSaver as \\\"BaseCheckpointSaver\\\"\\n    participant Command as \\\"Command\\\"\\n    \\n    User->>CompiledStateGraph: \\\"invoke(input, config={'configurable': {'thread_id': '1'}})\\\"\\n    CompiledStateGraph->>Node: \\\"execute node\\\"\\n    Node->>interrupt_func: \\\"interrupt(value)\\\"\\n    interrupt_func->>CONFIG_KEY_SCRATCHPAD: \\\"check for resume values\\\"\\n    CONFIG_KEY_SCRATCHPAD-->>interrupt_func: \\\"no resume value found\\\"\\n    interrupt_func->>interrupt_func: \\\"raise GraphInterrupt(value, id)\\\"\\n    CompiledStateGraph->>BaseCheckpointSaver: \\\"put_writes() with INTERRUPT write\\\"\\n    BaseCheckpointSaver-->>CompiledStateGraph: \\\"checkpoint saved\\\"\\n    CompiledStateGraph-->>User: \\\"return {'__interrupt__': [Interrupt(...)]}\\\"\\n    \\n    User->>CompiledStateGraph: \\\"invoke(Command(resume=data), config)\\\"\\n    CompiledStateGraph->>BaseCheckpointSaver: \\\"get_tuple() for thread\\\"\\n    BaseCheckpointSaver-->>CompiledStateGraph: \\\"restored checkpoint + pending writes\\\"\\n    CompiledStateGraph->>Node: \\\"resume from interrupt point\\\"\\n    Node->>interrupt_func: \\\"interrupt(value) called again\\\"\\n    interrupt_func->>CONFIG_KEY_SCRATCHPAD: \\\"check resume values\\\"\\n    CONFIG_KEY_SCRATCHPAD-->>interrupt_func: \\\"return resume data\\\"\\n    interrupt_func-->>Node: \\\"return resumed data\\\"\\n    Node-->>CompiledStateGraph: \\\"continue execution\\\"\\n    CompiledStateGraph-->>User: \\\"final result\\\"\\n```\\n\\nSources: [langgraph/types.py:506-529](), [langgraph/_internal/_constants.py:2](), [langgraph/checkpoint/base.py:228-279](), [langgraph/pregel/write.py:25-38]()\\n\\n## Dynamic Interrupts\\n\\nDynamic interrupts use the `interrupt` function to pause execution from within a node based on runtime conditions.\\n\\n### The interrupt Function\\n\\nThe `interrupt` function from `langgraph.types` provides the primary mechanism for pausing graph execution:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Node Execution\\\"\\n        node_start[\\\"Node starts\\\"]\\n        interrupt_call[\\\"interrupt(value)\\\"]\\n        check_scratchpad[\\\"Check CONFIG_KEY_SCRATCHPAD\\\"]\\n        raise_exception[\\\"Raise GraphInterrupt\\\"]\\n        return_resume[\\\"Return resume value\\\"]\\n        node_end[\\\"Node completes\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence\\\"\\n        checkpoint_save[\\\"BaseCheckpointSaver.put()\\\"]\\n        checkpoint_load[\\\"BaseCheckpointSaver.get_tuple()\\\"]\\n    end\\n    \\n    subgraph \\\"User Interface\\\"\\n        user_review[\\\"User reviews interrupt\\\"]\\n        command_resume[\\\"Command(resume=data)\\\"]\\n    end\\n    \\n    node_start --> interrupt_call\\n    interrupt_call --> check_scratchpad\\n    check_scratchpad -->|\\\"No resume value\\\"| raise_exception\\n    check_scratchpad -->|\\\"Has resume value\\\"| return_resume\\n    raise_exception --> checkpoint_save\\n    checkpoint_save --> user_review\\n    user_review --> command_resume\\n    command_resume --> checkpoint_load\\n    checkpoint_load --> return_resume\\n    return_resume --> node_end\\n```\\n\\nSources: [libs/langgraph/langgraph/types.py:506-529](), [libs/langgraph/_internal/_constants.py:CONFIG_KEY_SCRATCHPAD](), [libs/langgraph/tests/test_pregel_async.py:575-583]()\\n\\n### interrupt() Function Implementation\\n\\nThe `interrupt` function from `langgraph.types` provides the primary mechanism for pausing graph execution:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"interrupt() Flow\\\"\\n        interrupt_call[\\\"interrupt(value)\\\"]\\n        check_config[\\\"Check CONFIG_KEY_SCRATCHPAD\\\"]\\n        has_resume{\\\"Has resume value?\\\"}\\n        return_resume[\\\"Return resume value\\\"]\\n        raise_graph_interrupt[\\\"Raise GraphInterrupt(value, id)\\\"]\\n    end\\n    \\n    subgraph \\\"Pregel Handling\\\"\\n        catch_interrupt[\\\"Catch GraphInterrupt\\\"]\\n        create_pending_write[\\\"Create PendingWrite(INTERRUPT)\\\"]\\n        checkpoint_save[\\\"BaseCheckpointSaver.put_writes()\\\"]\\n    end\\n    \\n    subgraph \\\"Resume Process\\\"\\n        command_invoke[\\\"Command(resume=data)\\\"]\\n        restore_checkpoint[\\\"get_tuple() + restore state\\\"]\\n        set_scratchpad[\\\"Set CONFIG_KEY_SCRATCHPAD\\\"]\\n    end\\n    \\n    interrupt_call --> check_config\\n    check_config --> has_resume\\n    has_resume -->|\\\"Yes\\\"| return_resume\\n    has_resume -->|\\\"No\\\"| raise_graph_interrupt\\n    raise_graph_interrupt --> catch_interrupt\\n    catch_interrupt --> create_pending_write\\n    create_pending_write --> checkpoint_save\\n    command_invoke --> restore_checkpoint\\n    restore_checkpoint --> set_scratchpad\\n    set_scratchpad --> return_resume\\n```\\n\\nSources: [langgraph/types.py:506-529](), [langgraph/_internal/_constants.py:2](), [langgraph/pregel/write.py:25-38](), [langgraph/errors.py:84-91]()\\n\\n#### Tool Call Review Pattern\\n\\nFor reviewing tool calls before execution, the `interrupt` function is typically placed at the beginning of tool functions:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Tool Function\\\"\\n        tool_start[\\\"`Tool invocation`\\\"]\\n        interrupt_review[\\\"`interrupt(tool_call_info)`\\\"]\\n        approval_check[\\\"`Check approval type`\\\"]\\n        execute_tool[\\\"`Execute tool`\\\"]\\n        edit_args[\\\"`Edit arguments`\\\"]\\n        reject_tool[\\\"`Return rejection`\\\"]\\n    end\\n    \\n    subgraph \\\"Human Response\\\"\\n        review_payload[\\\"`Review tool call`\\\"]\\n        accept_decision[\\\"`Accept`\\\"]\\n        edit_decision[\\\"`Edit`\\\"]\\n        reject_decision[\\\"`Reject`\\\"]\\n    end\\n    \\n    tool_start --> interrupt_review\\n    interrupt_review --> review_payload\\n    review_payload --> accept_decision\\n    review_payload --> edit_decision\\n    review_payload --> reject_decision\\n    \\n    accept_decision --> approval_check\\n    edit_decision --> approval_check\\n    reject_decision --> approval_check\\n    \\n    approval_check --> execute_tool\\n    approval_check --> edit_args\\n    approval_check --> reject_tool\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:409-496]\\n\\n## Static Interrupts\\n\\nStatic interrupts are configured at compile time or runtime to pause execution at specific nodes, primarily used for debugging and testing.\\n\\n### Configuration Methods\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Compile Time Configuration\\\"\\n        StateGraph_builder[\\\"StateGraph\\\"]\\n        compile_call[\\\"compile(interrupt_before=['node1'], interrupt_after=['node2'])\\\"]\\n        CompiledStateGraph_result[\\\"CompiledStateGraph\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Configuration\\\"\\n        invoke_call[\\\"invoke(input, config, interrupt_before=['node3'])\\\"]\\n        stream_call[\\\"stream(input, config, interrupt_after=['node4'])\\\"]\\n        runtime_interrupts[\\\"Runtime Override\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Control\\\"\\n        pause_before[\\\"Pause before node execution\\\"]\\n        pause_after[\\\"Pause after node execution\\\"]\\n        resume_execution[\\\"Resume with invoke(None, config)\\\"]\\n    end\\n    \\n    StateGraph_builder --> compile_call\\n    compile_call --> CompiledStateGraph_result\\n    CompiledStateGraph_result --> invoke_call\\n    CompiledStateGraph_result --> stream_call\\n    invoke_call --> runtime_interrupts\\n    stream_call --> runtime_interrupts\\n    runtime_interrupts --> pause_before\\n    runtime_interrupts --> pause_after\\n    pause_before --> resume_execution\\n    pause_after --> resume_execution\\n```\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:688-705](), [libs/langgraph/langgraph/pregel/main.py:invoke_call](), [libs/langgraph/tests/test_pregel.py:1083-1107]()\\n\\n### Static vs Dynamic Interrupts\\n\\n| Feature | Static Interrupts | Dynamic Interrupts |\\n|---------|------------------|-------------------|\\n| Configuration | `interrupt_before`/`interrupt_after` | `interrupt()` function |\\n| Trigger | Before/after node execution | Conditional logic within nodes |\\n| Use Case | Debugging, testing | Human approval, validation |\\n| Resume Method | `invoke(None)` | `Command(resume=data)` |\\n| Data Exchange | None | `Interrupt.value` payload |\\n| Implementation | `StateGraph.compile()` parameters | `langgraph.types.interrupt()` calls |\\n\\nSources: [libs/langgraph/langgraph/graph/state.py:688-705](), [libs/langgraph/langgraph/types.py:408-530](), [libs/langgraph/tests/test_pregel.py:1083-1107]()\\n\\n## Send Primitive for Dynamic Routing\\n\\nThe `Send` class from `langgraph.types` enables dynamic routing and map-reduce patterns by allowing conditional edges to send different state to different nodes.\\n\\n### Send Class Structure and Implementation\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Send Class\\\"\\n        Send_init[\\\"Send.__init__(node, arg)\\\"]\\n        node_field[\\\"node: str\\\"]\\n        arg_field[\\\"arg: Any\\\"]\\n        repr_method[\\\"__repr__()\\\"]\\n        eq_method[\\\"__eq__()\\\"]\\n    end\\n    \\n    subgraph \\\"Usage in Conditional Edges\\\"\\n        routing_function[\\\"def route_function(state)\\\"]\\n        return_sends[\\\"return [Send('node_name', partial_state)]\\\"]\\n        add_conditional_edges_call[\\\"StateGraph.add_conditional_edges()\\\"]\\n        pregel_execution[\\\"Pregel.stream() parallel execution\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Semantics\\\"\\n        parallel_nodes[\\\"Multiple node instances\\\"]\\n        different_inputs[\\\"Different arg per Send\\\"]\\n        fan_out_pattern[\\\"Fan-out execution pattern\\\"]\\n    end\\n    \\n    Send_init --> node_field\\n    Send_init --> arg_field\\n    routing_function --> return_sends\\n    return_sends --> add_conditional_edges_call\\n    add_conditional_edges_call --> pregel_execution\\n    pregel_execution --> parallel_nodes\\n    pregel_execution --> different_inputs\\n    pregel_execution --> fan_out_pattern\\n```\\n\\n### Map-Reduce Pattern Implementation\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Map Phase\\\"\\n        input_node[\\\"input_node()\\\"]\\n        routing_func[\\\"continue_to_jokes()\\\"]\\n        send_list[\\\"[Send('generate_joke', {'subject': s}) for s in subjects]\\\"]\\n        parallel_exec[\\\"Pregel parallel execution\\\"]\\n    end\\n    \\n    subgraph \\\"Reduce Phase\\\"\\n        parallel_nodes[\\\"generate_joke (×N instances)\\\"]\\n        collect_results[\\\"Aggregate results\\\"]\\n        final_output[\\\"Final state\\\"]\\n    end\\n    \\n    input_node --> routing_func\\n    routing_func --> send_list\\n    send_list --> parallel_exec\\n    parallel_exec --> parallel_nodes\\n    parallel_nodes --> collect_results\\n    collect_results --> final_output\\n```\\n\\nSources: [langgraph/types.py:79-95](), [langgraph/graph/state.py:379-397](), [langgraph/pregel/main.py:300-350]()\\n\\n## Command Primitive\\n\\nThe `Command` class from `langgraph.types` provides the mechanism for resuming interrupted graphs and controlling execution flow with combined state updates and routing.\\n\\n### Command Structure\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Command Fields\\\"\\n        graph_field[\\\"graph: str | None\\\"]\\n        update_field[\\\"update: Any | None\\\"] \\n        resume_field[\\\"resume: dict[str, Any] | Any | None\\\"]\\n        goto_field[\\\"goto: Send | Sequence[Send | str] | str\\\"]\\n        PARENT_constant[\\\"PARENT: '__parent__'\\\"]\\n    end\\n    \\n    subgraph \\\"Command Methods\\\"\\n        _update_as_tuples[\\\"_update_as_tuples()\\\"]\\n        __repr__[\\\"__repr__()\\\"]\\n        __init__[\\\"__init__()\\\"]\\n    end\\n    \\n    subgraph \\\"Use Cases\\\"\\n        resume_interrupt[\\\"Resume from interrupt\\\"]\\n        parent_targeting[\\\"Target parent graph\\\"]\\n        control_flow[\\\"Control flow routing\\\"]\\n        state_modification[\\\"State updates\\\"]\\n        combined_ops[\\\"Combined update + goto\\\"]\\n    end\\n    \\n    resume_field --> resume_interrupt\\n    graph_field --> parent_targeting\\n    goto_field --> control_flow\\n    update_field --> state_modification\\n    PARENT_constant --> parent_targeting\\n    update_field --> _update_as_tuples\\n    goto_field --> combined_ops\\n    update_field --> combined_ops\\n```\\n\\n### Command vs Send Comparison\\n\\n| Feature | Command | Send |\\n|---------|---------|------|\\n| **Primary Use** | Resume interrupts, combined control flow | Dynamic routing, map-reduce patterns |\\n| **State Updates** | `Command(update={...})` | State passed as `Send(node, state)` |\\n| **Routing** | `Command(goto=node)` | `Send(node, arg)` from conditional edges |\\n| **Multi-targeting** | `Command(goto=[node1, node2])` | `[Send(node1, arg1), Send(node2, arg2)]` |\\n| **Parent Graph** | `Command(graph=Command.PARENT)` | Not applicable |\\n| **Implementation** | Return from node functions | Return from routing functions |\\n| **Resume Support** | `Command(resume=data)` for interrupts | No resume capability |\\n| **Execution Model** | Sequential with routing | Parallel execution |\\n\\nSources: [langgraph/types.py:353-406](), [langgraph/types.py:79-95](), [langgraph/graph/state.py:379-397]()\\n\\n### Multiple Interrupt Resume Pattern\\n\\nFor handling multiple interrupts in a single invocation when nodes execute in parallel:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Parallel Interrupts\\\"\\n        interrupt1[\\\"Interrupt(id='abc123', value=data1)\\\"]\\n        interrupt2[\\\"Interrupt(id='def456', value=data2)\\\"]\\n        interrupt3[\\\"Interrupt(id='ghi789', value=data3)\\\"]\\n    end\\n    \\n    subgraph \\\"Resume Mapping Process\\\"\\n        get_state[\\\"CompiledStateGraph.get_state()\\\"]\\n        extract_interrupts[\\\"state.tasks[].interrupts\\\"]\\n        build_resume_map[\\\"{'abc123': response1, 'def456': response2}\\\"]\\n        single_command[\\\"Command(resume=resume_map)\\\"]\\n    end\\n    \\n    subgraph \\\"Batch Resume Execution\\\"\\n        resume_by_id[\\\"Resume all interrupts by interrupt_id\\\"]\\n        parallel_continuation[\\\"Continue parallel execution\\\"]\\n        final_state[\\\"Merged final state\\\"]\\n    end\\n    \\n    interrupt1 --> get_state\\n    interrupt2 --> get_state\\n    interrupt3 --> get_state\\n    get_state --> extract_interrupts\\n    extract_interrupts --> build_resume_map\\n    build_resume_map --> single_command\\n    single_command --> resume_by_id\\n    resume_by_id --> parallel_continuation\\n    parallel_continuation --> final_state\\n```\\n\\nSources: [langgraph/types.py:146-205](), [langgraph/types.py:213-223](), [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:313-376]()\\n\\n## Time Travel and State Replay\\n\\nLangGraph's time travel functionality enables resuming execution from any prior checkpoint in a thread's history, allowing for debugging, alternative path exploration, and state modification.\\n\\n### Time Travel Implementation\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Time Travel Process\\\"\\n        get_history[\\\"CompiledStateGraph.get_state_history()\\\"]\\n        select_checkpoint[\\\"Select checkpoint_id\\\"]\\n        update_state_optional[\\\"Optional: update_state()\\\"]\\n        invoke_from_checkpoint[\\\"invoke(None, config={'checkpoint_id': id})\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Management\\\"\\n        BaseCheckpointSaver_list[\\\"BaseCheckpointSaver.list()\\\"]\\n        StateSnapshot_objects[\\\"StateSnapshot objects\\\"]\\n        checkpoint_metadata[\\\"CheckpointMetadata with step info\\\"]\\n        parent_config[\\\"parent_config for lineage\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Replay\\\"\\n        replay_mode[\\\"Replay mode (before checkpoint)\\\"]\\n        fork_mode[\\\"Fork mode (after checkpoint)\\\"]\\n        new_thread_branch[\\\"New execution branch\\\"]\\n    end\\n    \\n    get_history --> BaseCheckpointSaver_list\\n    BaseCheckpointSaver_list --> StateSnapshot_objects\\n    StateSnapshot_objects --> select_checkpoint\\n    select_checkpoint --> update_state_optional\\n    update_state_optional --> invoke_from_checkpoint\\n    invoke_from_checkpoint --> replay_mode\\n    replay_mode --> fork_mode\\n    fork_mode --> new_thread_branch\\n```\\n\\n### State Update and Forking\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"update_state() Process\\\"\\n        current_checkpoint[\\\"Current StateSnapshot\\\"]\\n        update_call[\\\"update_state(config, values, as_node)\\\"]\\n        new_checkpoint_id[\\\"Generate new checkpoint_id\\\"]\\n        fork_creation[\\\"Create execution fork\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Options\\\"\\n        replay_exact[\\\"Replay without changes\\\"]\\n        replay_modified[\\\"Replay with state modifications\\\"]\\n        alternative_path[\\\"Explore alternative execution\\\"]\\n    end\\n    \\n    current_checkpoint --> update_call\\n    update_call --> new_checkpoint_id\\n    new_checkpoint_id --> fork_creation\\n    fork_creation --> replay_exact\\n    fork_creation --> replay_modified\\n    fork_creation --> alternative_path\\n```\\n\\nSources: [langgraph/pregel/main.py:405-450](), [langgraph/checkpoint/base.py:228-279](), [docs/docs/how-tos/human_in_the_loop/time-travel.md:1-25]()\\n\\n## Common HIL Patterns\\n\\n### Approve or Reject Pattern\\n\\nThis pattern routes execution based on human approval:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Approval Node\\\"\\n        start[\\\"`human_approval(state)`\\\"]\\n        interrupt_call[\\\"`interrupt(approval_request)`\\\"]\\n        decision_check[\\\"`Check approval`\\\"]\\n        approve_route[\\\"`Command(goto='approved_path')`\\\"]\\n        reject_route[\\\"`Command(goto='rejected_path')`\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Paths\\\"\\n        approved_path[\\\"`approved_path node`\\\"]\\n        rejected_path[\\\"`rejected_path node`\\\"]\\n        end_state[\\\"`END`\\\"]\\n    end\\n    \\n    start --> interrupt_call\\n    interrupt_call --> decision_check\\n    decision_check --> approve_route\\n    decision_check --> reject_route\\n    approve_route --> approved_path\\n    reject_route --> rejected_path\\n    approved_path --> end_state\\n    rejected_path --> end_state\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:181-218]\\n\\n### State Editing Pattern\\n\\nThis pattern allows human modification of graph state:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Edit Node\\\"\\n        edit_start[\\\"`human_editing(state)`\\\"]\\n        interrupt_edit[\\\"`interrupt(edit_request)`\\\"]\\n        process_edit[\\\"`Process human edits`\\\"]\\n        update_state[\\\"`Return updated state`\\\"]\\n    end\\n    \\n    subgraph \\\"Human Interface\\\"\\n        review_state[\\\"`Review current state`\\\"]\\n        make_edits[\\\"`Make modifications`\\\"]\\n        submit_changes[\\\"`Submit changes`\\\"]\\n    end\\n    \\n    edit_start --> interrupt_edit\\n    interrupt_edit --> review_state\\n    review_state --> make_edits\\n    make_edits --> submit_changes\\n    submit_changes --> process_edit\\n    process_edit --> update_state\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:290-331]\\n\\n### Input Validation Pattern\\n\\nThis pattern validates human input within the graph:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Validation Loop\\\"\\n        validation_start[\\\"`validation_node(state)`\\\"]\\n        interrupt_input[\\\"`interrupt(question)`\\\"]\\n        validate_input[\\\"`Validate response`\\\"]\\n        valid_check{\\\"`Is valid?`\\\"}\\n        update_question[\\\"`Update question with error`\\\"]\\n        process_valid[\\\"`Process valid input`\\\"]\\n        return_result[\\\"`Return result`\\\"]\\n    end\\n    \\n    validation_start --> interrupt_input\\n    interrupt_input --> validate_input\\n    validate_input --> valid_check\\n    valid_check -->|No| update_question\\n    update_question --> interrupt_input\\n    valid_check -->|Yes| process_valid\\n    process_valid --> return_result\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:620-646]\\n\\n## Integration with Persistence\\n\\nHIL workflows require checkpointing to maintain state across interruptions:\\n\\n### Checkpointer Integration Requirements\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"BaseCheckpointSaver Interface\\\"\\n        BaseCheckpointSaver[\\\"BaseCheckpointSaver\\\"]\\n        put_method[\\\"put(config, checkpoint, metadata, new_versions)\\\"]\\n        get_tuple_method[\\\"get_tuple(config)\\\"]\\n        list_method[\\\"list(config, filter, before, limit)\\\"]\\n        put_writes_method[\\\"put_writes(config, writes, task_id)\\\"]\\n        get_next_version[\\\"get_next_version(current, channel)\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpointer Implementations\\\"\\n        InMemorySaver[\\\"InMemorySaver\\\"]\\n        SqliteSaver[\\\"SqliteSaver\\\"]\\n        PostgresSaver[\\\"PostgresSaver\\\"] \\n        AsyncSqliteSaver[\\\"AsyncSqliteSaver\\\"]\\n        AsyncPostgresSaver[\\\"AsyncPostgresSaver\\\"]\\n    end\\n    \\n    subgraph \\\"Interrupt-Specific Integration\\\"\\n        PendingWrite_INTERRUPT[\\\"PendingWrite(task_id, INTERRUPT, value)\\\"]\\n        checkpoint_with_interrupts[\\\"Checkpoint storage with interrupt data\\\"]\\n        thread_id_management[\\\"thread_id-based state isolation\\\"]\\n        resume_state_loading[\\\"State restoration for resume\\\"]\\n    end\\n    \\n    BaseCheckpointSaver --> InMemorySaver\\n    BaseCheckpointSaver --> SqliteSaver\\n    BaseCheckpointSaver --> PostgresSaver\\n    BaseCheckpointSaver --> AsyncSqliteSaver\\n    BaseCheckpointSaver --> AsyncPostgresSaver\\n    \\n    put_writes_method --> PendingWrite_INTERRUPT\\n    put_method --> checkpoint_with_interrupts\\n    get_tuple_method --> resume_state_loading\\n    list_method --> thread_id_management\\n```\\n\\nSources: [langgraph/checkpoint/base.py:43-279](), [langgraph/checkpoint/memory.py:24-158](), [langgraph/checkpoint/sqlite/base.py:60-400](), [langgraph/pregel/write.py:25-38]()\\n\\n### StateSnapshot Integration with Interrupts\\n\\nThe `StateSnapshot` object provides comprehensive interrupt information and execution state:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"StateSnapshot Structure\\\"\\n        values[\\\"values: dict[str, Any]\\\"]\\n        next_nodes[\\\"next: tuple[str, ...]\\\"]\\n        config[\\\"config: RunnableConfig\\\"]\\n        metadata[\\\"metadata: CheckpointMetadata\\\"]\\n        created_at[\\\"created_at: str\\\"]\\n        parent_config[\\\"parent_config: RunnableConfig | None\\\"]\\n        tasks[\\\"tasks: tuple[PregelTask, ...]\\\"]\\n        interrupts[\\\"interrupts: tuple[Interrupt, ...]\\\"]\\n    end\\n    \\n    subgraph \\\"PregelTask Structure\\\"\\n        task_id[\\\"id: str (UUID)\\\"]\\n        task_name[\\\"name: str (node name)\\\"]\\n        task_path[\\\"path: tuple (execution path)\\\"]\\n        task_error[\\\"error: Exception | None\\\"]\\n        task_interrupts[\\\"interrupts: tuple[Interrupt, ...]\\\"]\\n        task_state[\\\"state: StateSnapshot | None\\\"]\\n        task_result[\\\"result: Any | None\\\"]\\n    end\\n    \\n    subgraph \\\"Interrupt Structure\\\"\\n        interrupt_value[\\\"value: Any (interrupt payload)\\\"]\\n        interrupt_id[\\\"id: str (resume key)\\\"]\\n        when[\\\"when: str ('during'|'before'|'after')\\\"]\\n        resumable[\\\"resumable: bool\\\"]\\n        ns[\\\"ns: list[str] (namespace)\\\"]\\n    end\\n    \\n    tasks --> task_interrupts\\n    task_interrupts --> interrupt_value\\n    task_interrupts --> interrupt_id\\n    interrupts --> interrupt_value\\n    interrupts --> interrupt_id\\n    interrupts --> when\\n    interrupts --> resumable\\n    interrupts --> ns\\n```\\n\\nSources: [langgraph/types.py:258-277](), [langgraph/types.py:213-223](), [langgraph/types.py:146-205]()\\n\\n## Tool Integration Patterns\\n\\n### ToolNode with Interrupts\\n\\nThe `ToolNode` class from `langgraph.prebuilt` can be extended with interrupt capabilities:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Tool Execution Flow\\\"\\n        tool_message[\\\"`Tool message received`\\\"]\\n        tool_node[\\\"`ToolNode.__call__()`\\\"]\\n        tool_execution[\\\"`Tool function execution`\\\"]\\n        interrupt_check[\\\"`interrupt() called?`\\\"]\\n        pause_execution[\\\"`Pause for human review`\\\"]\\n        resume_execution[\\\"`Resume with human input`\\\"]\\n        tool_result[\\\"`Return ToolMessage`\\\"]\\n    end\\n    \\n    subgraph \\\"Human Review Interface\\\"\\n        review_tool_call[\\\"`Review tool call`\\\"]\\n        approval_decision[\\\"`Approve/Edit/Reject`\\\"]\\n        provide_input[\\\"`Provide input`\\\"]\\n    end\\n    \\n    tool_message --> tool_node\\n    tool_node --> tool_execution\\n    tool_execution --> interrupt_check\\n    interrupt_check -->|Yes| pause_execution\\n    interrupt_check -->|No| tool_result\\n    pause_execution --> review_tool_call\\n    review_tool_call --> approval_decision\\n    approval_decision --> provide_input\\n    provide_input --> resume_execution\\n    resume_execution --> tool_result\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:497-564], [docs/docs/tutorials/get-started/2-add-tools.md:111-154]\\n\\n### Human Assistance Tool Pattern\\n\\nExample implementation of a human assistance tool:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"human_assistance Tool\\\"\\n        tool_call[\\\"`@tool decorator`\\\"]\\n        function_def[\\\"`human_assistance(query: str)`\\\"]\\n        interrupt_call[\\\"`interrupt({'query': query})`\\\"]\\n        return_response[\\\"`return human_response['data']`\\\"]\\n    end\\n    \\n    subgraph \\\"LLM Integration\\\"\\n        llm_tool_call[\\\"`LLM generates tool_call`\\\"]\\n        bind_tools[\\\"`llm.bind_tools([human_assistance])`\\\"]\\n        tools_condition[\\\"`tools_condition routing`\\\"]\\n    end\\n    \\n    subgraph \\\"Graph Structure\\\"\\n        chatbot_node[\\\"`chatbot node`\\\"]\\n        tools_node[\\\"`tools node`\\\"]\\n        conditional_edge[\\\"`conditional edge`\\\"]\\n    end\\n    \\n    tool_call --> function_def\\n    function_def --> interrupt_call\\n    interrupt_call --> return_response\\n    \\n    bind_tools --> llm_tool_call\\n    llm_tool_call --> tools_condition\\n    tools_condition --> tools_node\\n    \\n    chatbot_node --> conditional_edge\\n    conditional_edge --> tools_node\\n    tools_node --> chatbot_node\\n```\\n\\nSources: [docs/docs/tutorials/get-started/4-human-in-the-loop.md:48-64], [docs/docs/tutorials/get-started/4-human-in-the-loop.md:110-140]\\n\\nHuman-in-the-loop capabilities in LangGraph provide flexible mechanisms for incorporating human oversight, approval, and input into automated workflows. The combination of dynamic interrupts, static breakpoints, and the Command primitive enables sophisticated control flow patterns that can handle complex human-AI collaboration scenarios while maintaining state consistency through the persistence layer.\", \"# Page: Persistence System\\n\\n# Persistence System\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py)\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/aio.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/aio.py)\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py)\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/shallow.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/shallow.py)\\n- [libs/checkpoint-postgres/tests/test_async.py](libs/checkpoint-postgres/tests/test_async.py)\\n- [libs/checkpoint-postgres/tests/test_sync.py](libs/checkpoint-postgres/tests/test_sync.py)\\n- [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py](libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py)\\n- [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/aio.py](libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/aio.py)\\n- [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/utils.py](libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/utils.py)\\n- [libs/checkpoint-sqlite/tests/test_aiosqlite.py](libs/checkpoint-sqlite/tests/test_aiosqlite.py)\\n- [libs/checkpoint-sqlite/tests/test_sqlite.py](libs/checkpoint-sqlite/tests/test_sqlite.py)\\n- [libs/checkpoint/langgraph/checkpoint/base/__init__.py](libs/checkpoint/langgraph/checkpoint/base/__init__.py)\\n- [libs/checkpoint/langgraph/checkpoint/memory/__init__.py](libs/checkpoint/langgraph/checkpoint/memory/__init__.py)\\n- [libs/checkpoint/tests/test_memory.py](libs/checkpoint/tests/test_memory.py)\\n\\n</details>\\n\\n\\n\\nThe LangGraph Persistence System provides comprehensive data persistence capabilities through two primary subsystems: **checkpointing** for graph state management and **stores** for persistent key-value data with vector search. Both subsystems share a common serialization layer and support multiple storage backends ranging from in-memory implementations to production databases.\\n\\nThe persistence system enables stateful multi-actor applications to persist their state across executions, resume from interruptions, maintain conversation history, and store cross-thread data with semantic search capabilities.\\n\\nFor detailed information about checkpoint operations and state recovery mechanisms, see [Checkpointing](#5.1). For persistent key-value storage and vector search capabilities, see [Store System](#5.2). For data encoding and type preservation strategies shared by both subsystems, see [Serialization](#5.3).\\n\\n## Architecture Overview\\n\\nThe persistence system provides two complementary subsystems that integrate with LangGraph's execution engine:\\n\\n**Persistence System Architecture**\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"LangGraph Core\\\"\\n        Pregel[\\\"Pregel Engine\\\"]\\n        StateGraph[\\\"StateGraph API\\\"]\\n        Channels[\\\"Channel System\\\"]\\n        Graphs[\\\"Graph Instances\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpointing Subsystem\\\"\\n        BaseCheckpointSaver[\\\"BaseCheckpointSaver[V]<br/>Abstract Interface\\\"]\\n        CheckpointImpl[\\\"Checkpoint Implementations\\\"]\\n        \\n        subgraph \\\"Checkpoint Backends\\\"\\n            InMemorySaver[\\\"InMemorySaver\\\"]\\n            PostgresSaver[\\\"PostgresSaver\\\"]\\n            SqliteSaver[\\\"SqliteSaver\\\"]\\n            AsyncPostgresSaver[\\\"AsyncPostgresSaver\\\"]\\n            AsyncSqliteSaver[\\\"AsyncSqliteSaver\\\"]\\n        end\\n    end\\n    \\n    subgraph \\\"Store Subsystem\\\"\\n        BaseStore[\\\"BaseStore<br/>Abstract Interface\\\"]\\n        StoreImpl[\\\"Store Implementations\\\"]\\n        \\n        subgraph \\\"Store Backends\\\"\\n            InMemoryStore[\\\"InMemoryStore\\\"]\\n            PostgresStore[\\\"PostgresStore\\\"]\\n            SqliteStore[\\\"SqliteStore\\\"]\\n            AsyncPostgresStore[\\\"AsyncPostgresStore\\\"]\\n        end\\n    end\\n    \\n    subgraph \\\"Shared Serialization Layer\\\"\\n        JsonPlusSerializer[\\\"JsonPlusSerializer<br/>Type-preserving encoding\\\"]\\n        MsgPackEncoding[\\\"ormsgpack with extensions\\\"]\\n        TypeHandlers[\\\"Extension type handlers\\\"]\\n    end\\n    \\n    subgraph \\\"Data Types\\\"\\n        CheckpointTuple[\\\"CheckpointTuple<br/>State snapshot\\\"]\\n        Item[\\\"Item<br/>Store data with metadata\\\"]\\n        Checkpoint[\\\"Checkpoint TypedDict\\\"]\\n        ChannelVersions[\\\"ChannelVersions dict\\\"]\\n    end\\n    \\n    %% Core to Persistence connections\\n    Pregel --> BaseCheckpointSaver\\n    StateGraph --> BaseCheckpointSaver\\n    Channels --> BaseCheckpointSaver\\n    Graphs --> BaseStore\\n    \\n    %% Checkpoint implementations\\n    BaseCheckpointSaver --> InMemorySaver\\n    BaseCheckpointSaver --> PostgresSaver\\n    BaseCheckpointSaver --> SqliteSaver\\n    BaseCheckpointSaver --> AsyncPostgresSaver\\n    BaseCheckpointSaver --> AsyncSqliteSaver\\n    \\n    %% Store implementations  \\n    BaseStore --> InMemoryStore\\n    BaseStore --> PostgresStore\\n    BaseStore --> SqliteStore\\n    BaseStore --> AsyncPostgresStore\\n    \\n    %% Shared serialization\\n    BaseCheckpointSaver --> JsonPlusSerializer\\n    BaseStore --> JsonPlusSerializer\\n    JsonPlusSerializer --> MsgPackEncoding\\n    JsonPlusSerializer --> TypeHandlers\\n    \\n    %% Data flow\\n    BaseCheckpointSaver --> CheckpointTuple\\n    BaseStore --> Item\\n    CheckpointTuple --> Checkpoint\\n    CheckpointTuple --> ChannelVersions\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:108-474](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:40-245](), [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:31-63](), [libs/langgraph/tests/conftest.py:74-117]()\\n\\n## Core Components and Code Entities\\n\\nThe persistence system maps to specific code entities across both checkpointing and store subsystems:\\n\\n**Persistence System Code Entity Mapping**\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Abstract Interfaces\\\"\\n        BaseCheckpointSaver[\\\"BaseCheckpointSaver[V]<br/>libs/checkpoint/base/__init__.py\\\"]\\n        BaseStore[\\\"BaseStore<br/>langgraph.store.base\\\"]\\n        SerializerProtocol[\\\"SerializerProtocol<br/>checkpoint/serde/base.py\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Data Types\\\"\\n        CheckpointTuple[\\\"CheckpointTuple<br/>NamedTuple container\\\"]\\n        Checkpoint[\\\"Checkpoint<br/>TypedDict state snapshot\\\"]\\n        CheckpointMetadata[\\\"CheckpointMetadata<br/>TypedDict execution metadata\\\"]\\n        ChannelVersions[\\\"ChannelVersions<br/>dict[str, Union[str,int,float]]\\\"]\\n        PendingWrite[\\\"PendingWrite<br/>tuple[str, str, Any]\\\"]\\n    end\\n    \\n    subgraph \\\"Store Data Types\\\"\\n        Item[\\\"Item<br/>Store value with metadata\\\"]\\n        SearchFilter[\\\"SearchFilter<br/>Query parameters\\\"]\\n        NamespacedStore[\\\"NamespacedStore<br/>Namespace isolation\\\"]\\n    end\\n    \\n    subgraph \\\"Serialization Layer\\\"\\n        JsonPlusSerializer[\\\"JsonPlusSerializer<br/>serde/jsonplus.py:40-245\\\"]\\n        msgpack_default[\\\"_msgpack_default<br/>Type encoding function\\\"]\\n        msgpack_ext_hook[\\\"_msgpack_ext_hook<br/>Type decoding function\\\"]\\n        EXT_TYPES[\\\"Extension type constants<br/>EXT_PYDANTIC_V2, EXT_NUMPY_ARRAY\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Implementations\\\"\\n        InMemorySaver[\\\"InMemorySaver<br/>defaultdict[str, dict] storage\\\"]\\n        PostgresSaver[\\\"PostgresSaver<br/>psycopg.Connection\\\"]\\n        AsyncPostgresSaver[\\\"AsyncPostgresSaver<br/>AsyncConnection\\\"]\\n        SqliteSaver[\\\"SqliteSaver<br/>sqlite3.Connection\\\"]\\n        AsyncSqliteSaver[\\\"AsyncSqliteSaver<br/>aiosqlite.Connection\\\"]\\n    end\\n    \\n    subgraph \\\"Store Implementations\\\"\\n        InMemoryStore[\\\"InMemoryStore<br/>In-process dict storage\\\"]\\n        PostgresStore[\\\"PostgresStore<br/>PostgreSQL backend\\\"]\\n        SqliteStore[\\\"SqliteStore<br/>SQLite backend\\\"]\\n        AsyncPostgresStore[\\\"AsyncPostgresStore<br/>Async PostgreSQL\\\"]\\n    end\\n    \\n    %% Inheritance relationships\\n    BaseCheckpointSaver --> InMemorySaver\\n    BaseCheckpointSaver --> PostgresSaver\\n    BaseCheckpointSaver --> AsyncPostgresSaver\\n    BaseCheckpointSaver --> SqliteSaver\\n    BaseCheckpointSaver --> AsyncSqliteSaver\\n    \\n    BaseStore --> InMemoryStore\\n    BaseStore --> PostgresStore\\n    BaseStore --> SqliteStore\\n    BaseStore --> AsyncPostgresStore\\n    \\n    %% Shared serialization\\n    BaseCheckpointSaver --> SerializerProtocol\\n    BaseStore --> SerializerProtocol\\n    SerializerProtocol --> JsonPlusSerializer\\n    \\n    JsonPlusSerializer --> msgpack_default\\n    JsonPlusSerializer --> msgpack_ext_hook\\n    JsonPlusSerializer --> EXT_TYPES\\n    \\n    %% Data type relationships\\n    BaseCheckpointSaver --> CheckpointTuple\\n    CheckpointTuple --> Checkpoint\\n    CheckpointTuple --> CheckpointMetadata\\n    CheckpointTuple --> ChannelVersions\\n    CheckpointTuple --> PendingWrite\\n    \\n    BaseStore --> Item\\n    BaseStore --> SearchFilter\\n    BaseStore --> NamespacedStore\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:98-474](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:40-677](), [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:31-527](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:32-473](), [libs/langgraph/tests/conftest.py:74-117]()\\n\\n## Subsystem Overview\\n\\n### Checkpoint System\\n\\nThe checkpoint system provides the fundamental mechanism for persisting graph state at specific execution points. The `BaseCheckpointSaver` abstract class defines the core interface:\\n\\n| Method | Purpose | Return Type |\\n|--------|---------|-------------|\\n| `get_tuple(config)` | Retrieve checkpoint with metadata | `CheckpointTuple \\\\| None` |\\n| `list(config, filter, before, limit)` | Query multiple checkpoints | `Iterator[CheckpointTuple]` |\\n| `put(config, checkpoint, metadata, new_versions)` | Store checkpoint state | `RunnableConfig` |\\n| `put_writes(config, writes, task_id, task_path)` | Store intermediate writes | `None` |\\n| `delete_thread(thread_id)` | Remove thread data | `None` |\\n\\nThe system supports both synchronous and asynchronous operations through parallel method hierarchies (`aget_tuple`, `alist`, `aput`, etc.).\\n\\n**Key Data Structures:**\\n\\n- `Checkpoint`: Contains versioned channel values, execution metadata, and pending operations\\n- `CheckpointTuple`: Bundles checkpoint with configuration, metadata, and parent references\\n- `ChannelVersions`: Tracks monotonically increasing version identifiers for state synchronization\\n\\n### Store System\\n\\nThe store system provides persistent key-value storage with vector search capabilities through the `BaseStore` interface:\\n\\n| Method | Purpose | Return Type |\\n|--------|---------|-------------|\\n| `get(namespace, key)` | Retrieve single item by key | `Item \\\\| None` |\\n| `search(namespace, query, filter, limit)` | Vector/semantic search | `list[Item]` |\\n| `put(namespace, key, value)` | Store item with metadata | `None` |\\n| `delete(namespace, key)` | Remove item | `None` |\\n| `batch(ops)` | Execute multiple operations | `list[Item \\\\| None]` |\\n\\n**Key Data Structures:**\\n\\n- `Item`: Contains value, key, namespace, timestamps, and optional vector embeddings\\n- `SearchFilter`: Query parameters for filtering results\\n- `NamespacedStore`: Provides namespace-isolated views of the store\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:108-474](), [libs/checkpoint/langgraph/checkpoint/base/__init__.py:60-106](), [libs/langgraph/tests/conftest.py:74-117]()\\n\\n## Shared Serialization Layer\\n\\nThe `JsonPlusSerializer` handles complex type preservation across persistence boundaries for both checkpoint and store data using a multi-format approach:\\n\\n**JsonPlusSerializer Type Handling Flow**\\n\\n```mermaid\\ngraph TD\\n    Input[\\\"Python Object<br/>(Checkpoint or Store Item)\\\"] --> TypeCheck{\\\"Object Type Analysis\\\"}\\n    \\n    TypeCheck -->|\\\"bytes/bytearray\\\"| DirectBytes[\\\"Direct Storage<br/>('bytes'/'bytearray', data)\\\"]\\n    TypeCheck -->|\\\"None\\\"| NullType[\\\"Null Type<br/>('null', EMPTY_BYTES)\\\"]\\n    TypeCheck -->|\\\"Complex types\\\"| MsgPackFlow[\\\"ormsgpack Encoding\\\"]\\n    \\n    MsgPackFlow --> MsgPackSuccess{\\\"Encoding Result?\\\"}\\n    MsgPackSuccess -->|\\\"Success\\\"| MsgPackResult[\\\"('msgpack', encoded_data)\\\"]\\n    MsgPackSuccess -->|\\\"UTF-8 error\\\"| JsonFallback[\\\"JSON Fallback<br/>('json', json_encoded)\\\"]\\n    MsgPackSuccess -->|\\\"Other error + pickle_fallback=True\\\"| PickleFallback[\\\"Pickle Fallback<br/>('pickle', pickled_data)\\\"]\\n    \\n    subgraph \\\"Extension Type Handlers\\\"\\n        EXT_PYDANTIC_V2[\\\"EXT_PYDANTIC_V2<br/>model_dump() -> model_validate_json()\\\"]\\n        EXT_DATACLASS[\\\"EXT_CONSTRUCTOR_KW_ARGS<br/>dataclasses fields\\\"]\\n        EXT_DATETIME[\\\"EXT_METHOD_SINGLE_ARG<br/>datetime.isoformat() -> fromisoformat()\\\"]\\n        EXT_COLLECTIONS[\\\"EXT_CONSTRUCTOR_SINGLE_ARG<br/>set, deque, frozenset\\\"]\\n        EXT_NUMPY[\\\"EXT_NUMPY_ARRAY<br/>buffer + metadata\\\"]\\n        EXT_ITEM[\\\"EXT_CONSTRUCTOR_KW_ARGS<br/>Item.__slots__\\\"]\\n    end\\n    \\n    MsgPackFlow --> EXT_PYDANTIC_V2\\n    MsgPackFlow --> EXT_DATACLASS\\n    MsgPackFlow --> EXT_DATETIME\\n    MsgPackFlow --> EXT_COLLECTIONS\\n    MsgPackFlow --> EXT_NUMPY\\n    MsgPackFlow --> EXT_ITEM\\n    \\n    subgraph \\\"Usage Context\\\"\\n        CheckpointUsage[\\\"Checkpoint Data<br/>Channel values, metadata\\\"]\\n        StoreUsage[\\\"Store Data<br/>Item values, search results\\\"]\\n    end\\n    \\n    Input --> CheckpointUsage\\n    Input --> StoreUsage\\n```\\n\\nThe serializer uses extension types with specific codes (`EXT_CONSTRUCTOR_SINGLE_ARG`, `EXT_PYDANTIC_V2`, `EXT_NUMPY_ARRAY`, etc.) for efficient type reconstruction during deserialization. Both checkpointing and store systems rely on this shared serialization layer for type preservation.\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:207-245](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:258-482](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:485-571](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:452-462]()\\n\\n## Implementation Variants\\n\\nThe persistence system provides multiple storage backend implementations for both checkpoint and store subsystems:\\n\\n### Checkpoint Implementations\\n\\n| Implementation | Use Case | Key Features | Connection Management |\\n|----------------|----------|--------------|----------------------|\\n| `InMemorySaver` | Development/Testing | Fast, ephemeral storage | `defaultdict[str, dict]` structures |\\n| `SqliteSaver` | Lightweight production | Single-file database | `sqlite3.Connection` |\\n| `AsyncSqliteSaver` | Async lightweight | Non-blocking I/O | `aiosqlite.Connection` |\\n| `PostgresSaver` | Production database | ACID compliance, scaling | `psycopg.Connection` |\\n| `AsyncPostgresSaver` | Async production | High concurrency | `AsyncConnection` |\\n\\n**Checkpoint Database Schema:**\\n\\nAll SQL-based checkpoint implementations use a consistent three-table schema:\\n- `checkpoints`: Main state snapshots with metadata\\n- `checkpoint_blobs`: Large binary data storage  \\n- `checkpoint_writes`: Intermediate write operations\\n\\n### Store Implementations\\n\\n| Implementation | Use Case | Key Features | Storage Backend |\\n|----------------|----------|--------------|-----------------|\\n| `InMemoryStore` | Development/Testing | Fast, ephemeral key-value | In-process dictionary |\\n| `SqliteStore` | Lightweight production | File-based persistence | SQLite with vector extension |\\n| `PostgresStore` | Production database | Vector search, indexing | PostgreSQL with pgvector |\\n| `AsyncPostgresStore` | Async production | High-concurrency access | Async PostgreSQL |\\n\\n**Store Database Schema:**\\n\\nStore implementations typically use a single table schema:\\n- `stores`: Key-value data with namespace, timestamps, and optional vector embeddings\\n\\n### Migration System\\n\\nBoth checkpoint and store SQL backends implement versioned migrations through the `MIGRATIONS` array pattern, enabling schema evolution while maintaining backward compatibility.\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:31-527](), [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py:37-72](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:32-473](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py:24-72](), [libs/langgraph/tests/conftest.py:74-117]()\\n\\n## Usage Patterns\\n\\nThe persistence system integrates transparently with LangGraph execution through configuration-based activation:\\n\\n**Basic Setup:**\\n```python\\n# Development\\nmemory = InMemorySaver()\\ngraph = builder.compile(checkpointer=memory)\\n\\n# Production PostgreSQL\\nwith PostgresSaver.from_conn_string(conn_string) as saver:\\n    graph = builder.compile(checkpointer=saver)\\n```\\n\\n**State Management:**\\n- Thread-based isolation via `thread_id` in configuration\\n- Namespace support for hierarchical state organization\\n- Automatic version tracking for optimistic concurrency control\\n- Metadata filtering for checkpoint queries\\n\\n**Integration Points:**\\n- Pregel engine calls checkpoint methods during step execution\\n- Channel system triggers state persistence on value updates\\n- Human-in-the-loop patterns leverage checkpoint history for rollback\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:47-63](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:54-76](), [libs/checkpoint/langgraph/checkpoint/base/__init__.py:190-242]()\", \"# Page: Checkpointing\\n\\n# Checkpointing\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py)\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/aio.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/aio.py)\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py)\\n- [libs/checkpoint-postgres/langgraph/checkpoint/postgres/shallow.py](libs/checkpoint-postgres/langgraph/checkpoint/postgres/shallow.py)\\n- [libs/checkpoint-postgres/tests/test_async.py](libs/checkpoint-postgres/tests/test_async.py)\\n- [libs/checkpoint-postgres/tests/test_sync.py](libs/checkpoint-postgres/tests/test_sync.py)\\n- [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py](libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py)\\n- [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/aio.py](libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/aio.py)\\n- [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/utils.py](libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/utils.py)\\n- [libs/checkpoint-sqlite/tests/test_aiosqlite.py](libs/checkpoint-sqlite/tests/test_aiosqlite.py)\\n- [libs/checkpoint-sqlite/tests/test_sqlite.py](libs/checkpoint-sqlite/tests/test_sqlite.py)\\n- [libs/checkpoint/langgraph/checkpoint/base/__init__.py](libs/checkpoint/langgraph/checkpoint/base/__init__.py)\\n- [libs/checkpoint/langgraph/checkpoint/memory/__init__.py](libs/checkpoint/langgraph/checkpoint/memory/__init__.py)\\n- [libs/checkpoint/tests/test_memory.py](libs/checkpoint/tests/test_memory.py)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's checkpoint system, which provides state persistence for graph execution across multiple interactions. Checkpointing enables features like conversation memory, graph resumption, and time travel debugging by saving snapshots of graph state at specific execution points.\\n\\nFor information about cross-thread persistent storage, see [Store System](#5.2). For serialization protocols used by checkpointers, see [Serialization](#5.3).\\n\\n## Core Data Structures\\n\\nThe checkpointing system is built around several key data structures that represent different aspects of persisted state:\\n\\n### Checkpoint Structure\\n\\nA `Checkpoint` represents a complete snapshot of graph state at a specific point in time. It contains channel values, version information, and execution metadata.\\n\\n```mermaid\\ngraph TD\\n    CP[\\\"Checkpoint<br/>TypedDict\\\"] --> ID[\\\"id: str<br/>Unique checkpoint identifier\\\"]\\n    CP --> TS[\\\"ts: str<br/>ISO 8601 timestamp\\\"]\\n    CP --> CV[\\\"channel_values: dict<br/>Serialized channel data\\\"]\\n    CP --> CVR[\\\"channel_versions: dict<br/>Version tracking per channel\\\"]\\n    CP --> VS[\\\"versions_seen: dict<br/>Node execution tracking\\\"]\\n    CP --> UC[\\\"updated_channels: list<br/>Modified channels\\\"]\\n```\\n\\n### CheckpointTuple Container\\n\\nThe `CheckpointTuple` packages a checkpoint with its associated configuration and metadata for retrieval operations:\\n\\n```mermaid\\ngraph TD\\n    CT[\\\"CheckpointTuple<br/>NamedTuple\\\"] --> CFG[\\\"config: RunnableConfig<br/>Thread and checkpoint IDs\\\"]\\n    CT --> CP[\\\"checkpoint: Checkpoint<br/>State snapshot\\\"]\\n    CT --> META[\\\"metadata: CheckpointMetadata<br/>Execution context\\\"]\\n    CT --> PARENT[\\\"parent_config: RunnableConfig<br/>Previous checkpoint reference\\\"]\\n    CT --> WRITES[\\\"pending_writes: list<br/>Uncommitted operations\\\"]\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:60-87](), [libs/checkpoint/langgraph/checkpoint/base/__init__.py:102-110]()\\n\\n## Architecture Overview\\n\\nThe checkpointing system follows a layered architecture with pluggable backends:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"LangGraph Core\\\"\\n        GRAPH[\\\"StateGraph/CompiledGraph\\\"]\\n        PREGEL[\\\"Pregel Runtime\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Interface Layer\\\"\\n        BASE[\\\"BaseCheckpointSaver<br/>Abstract base class\\\"]\\n        METHODS[\\\"get_tuple()<br/>put()<br/>list()<br/>put_writes()<br/>delete_thread()\\\"]\\n    end\\n    \\n    subgraph \\\"Backend Implementations\\\"\\n        MEM[\\\"InMemorySaver<br/>libs/checkpoint/memory\\\"]\\n        PG[\\\"PostgresSaver<br/>libs/checkpoint-postgres\\\"]\\n        SQLITE[\\\"SqliteSaver<br/>libs/checkpoint-sqlite\\\"]\\n    end\\n    \\n    subgraph \\\"Async Variants\\\"\\n        APG[\\\"AsyncPostgresSaver\\\"]\\n        ASQLITE[\\\"AsyncSqliteSaver\\\"]\\n    end\\n    \\n    subgraph \\\"Storage Layer\\\"\\n        MEMORY[\\\"In-Memory<br/>defaultdict storage\\\"]\\n        POSTGRES[\\\"PostgreSQL<br/>checkpoints + blobs tables\\\"]\\n        SQLITEDB[\\\"SQLite<br/>checkpoints + writes tables\\\"]\\n    end\\n    \\n    GRAPH --> BASE\\n    PREGEL --> BASE\\n    BASE --> MEM\\n    BASE --> PG\\n    BASE --> SQLITE\\n    PG --> APG\\n    SQLITE --> ASQLITE\\n    MEM --> MEMORY\\n    PG --> POSTGRES\\n    APG --> POSTGRES\\n    SQLITE --> SQLITEDB\\n    ASQLITE --> SQLITEDB\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:112-372](), [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:31-63](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:32-33](), [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py:37-72]()\\n\\n## Backend Implementations\\n\\n### InMemorySaver\\n\\nThe `InMemorySaver` provides a lightweight, thread-safe checkpoint implementation using in-memory data structures. It's designed for development, testing, and small-scale applications.\\n\\n| Feature | Implementation |\\n|---------|----------------|\\n| Storage | `defaultdict` with nested thread/namespace/checkpoint structure |\\n| Thread Safety | Context manager with `ExitStack` |\\n| Persistence | Memory-only, lost on process termination |\\n| Async Support | Sync methods wrapped for async compatibility |\\n\\nThe storage structure uses a hierarchical organization:\\n\\n```mermaid\\ngraph TD\\n    STORAGE[\\\"storage: defaultdict\\\"] --> THREAD[\\\"thread_id: str\\\"]\\n    THREAD --> NS[\\\"checkpoint_ns: str\\\"]\\n    NS --> CP_ID[\\\"checkpoint_id: str\\\"]\\n    CP_ID --> TUPLE[\\\"(checkpoint, metadata, parent_id)\\\"]\\n    \\n    WRITES[\\\"writes: defaultdict\\\"] --> W_KEY[\\\"(thread_id, ns, checkpoint_id)\\\"]\\n    W_KEY --> W_DICT[\\\"dict[(task_id, idx)] -> write_data\\\"]\\n    \\n    BLOBS[\\\"blobs: dict\\\"] --> B_KEY[\\\"(thread_id, ns, channel, version)\\\"]\\n    B_KEY --> B_DATA[\\\"(type, bytes)\\\"]\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:66-81](), [libs/checkpoint/langgraph/checkpoint/memory/__init__.py:88-96]()\\n\\n### PostgresSaver\\n\\nThe `PostgresSaver` provides production-grade persistence using PostgreSQL with optimized schema design and migration support.\\n\\n#### Database Schema\\n\\n| Table | Purpose | Key Columns |\\n|-------|---------|-------------|\\n| `checkpoints` | Main checkpoint data | `thread_id`, `checkpoint_ns`, `checkpoint_id`, `checkpoint` (JSONB) |\\n| `checkpoint_blobs` | Large channel values | `thread_id`, `checkpoint_ns`, `channel`, `version`, `blob` (BYTEA) |\\n| `checkpoint_writes` | Pending writes | `thread_id`, `checkpoint_ns`, `checkpoint_id`, `task_id`, `idx` |\\n| `checkpoint_migrations` | Schema versioning | `v` (version number) |\\n\\nThe PostgreSQL implementation includes several performance optimizations:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"PostgresSaver Optimizations\\\"\\n        PIPELINE[\\\"Pipeline Support<br/>Batch operations\\\"]\\n        POOL[\\\"Connection Pool<br/>Concurrent access\\\"]\\n        MIGRATION[\\\"Schema Migration<br/>Version management\\\"]\\n        BLOB[\\\"Blob Storage<br/>Large value optimization\\\"]\\n    end\\n    \\n    subgraph \\\"Query Patterns\\\"\\n        SELECT[\\\"Complex SELECT<br/>JOINs with blobs + writes\\\"]\\n        UPSERT[\\\"UPSERT Operations<br/>Conflict resolution\\\"]\\n        INDEX[\\\"Concurrent Indexes<br/>thread_id optimization\\\"]\\n    end\\n    \\n    PIPELINE --> SELECT\\n    POOL --> UPSERT\\n    MIGRATION --> INDEX\\n    BLOB --> SELECT\\n```\\n\\nSources: [libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py:24-72](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:37-52]()\\n\\n### SqliteSaver\\n\\nThe `SqliteSaver` provides a file-based persistence option suitable for development and single-user applications.\\n\\n| Capability | Sync Version | Async Version |\\n|------------|-------------|---------------|\\n| Class Name | `SqliteSaver` | `AsyncSqliteSaver` |\\n| Connection | `sqlite3.Connection` | `aiosqlite.Connection` |\\n| Threading | Lock-based safety | Async/await patterns |\\n| Transactions | Context manager | Async context manager |\\n\\nSources: [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/__init__.py:74-87](), [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/aio.py:106-120]()\\n\\n## Core Operations\\n\\n### Checkpoint Retrieval\\n\\nThe `get_tuple()` method retrieves checkpoints with optional version specification:\\n\\n```mermaid\\ngraph TD\\n    GET[\\\"get_tuple(config)\\\"] --> CHECK_ID{\\\"checkpoint_id<br/>in config?\\\"}\\n    CHECK_ID -->|Yes| SPECIFIC[\\\"Query specific<br/>checkpoint by ID\\\"]\\n    CHECK_ID -->|No| LATEST[\\\"Query latest<br/>checkpoint for thread\\\"]\\n    \\n    SPECIFIC --> LOAD[\\\"Load checkpoint data\\\"]\\n    LATEST --> LOAD\\n    LOAD --> WRITES[\\\"Fetch pending writes\\\"]\\n    WRITES --> TUPLE[\\\"Build CheckpointTuple\\\"]\\n    TUPLE --> RETURN[\\\"Return result\\\"]\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:156-168](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:181-250]()\\n\\n### Checkpoint Storage\\n\\nThe `put()` method persists checkpoints with blob optimization for large values:\\n\\n```mermaid\\ngraph TD\\n    PUT[\\\"put(config, checkpoint, metadata, versions)\\\"] --> SEPARATE[\\\"Separate inline vs blob values\\\"]\\n    SEPARATE --> INLINE[\\\"Store primitives<br/>in checkpoint JSONB\\\"]\\n    SEPARATE --> BLOBS[\\\"Store large objects<br/>in blobs table\\\"]\\n    BLOBS --> UPSERT_BLOBS[\\\"UPSERT checkpoint_blobs\\\"]\\n    INLINE --> UPSERT_CP[\\\"UPSERT checkpoints\\\"]\\n    UPSERT_BLOBS --> RETURN_CONFIG[\\\"Return updated config\\\"]\\n    UPSERT_CP --> RETURN_CONFIG\\n```\\n\\nSources: [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:252-331](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/base.py:185-208]()\\n\\n### Search and Filtering\\n\\nThe `list()` method supports metadata-based filtering and pagination:\\n\\n| Filter Type | Implementation | Example |\\n|-------------|----------------|---------|\\n| Thread ID | Direct column match | `{\\\"configurable\\\": {\\\"thread_id\\\": \\\"123\\\"}}` |\\n| Metadata | JSONB/JSON extraction | `{\\\"source\\\": \\\"input\\\", \\\"step\\\": 1}` |\\n| Before | Timestamp comparison | `before={\\\"configurable\\\": {\\\"checkpoint_id\\\": \\\"abc\\\"}}` |\\n| Limit | Query LIMIT clause | `limit=10` |\\n\\nSources: [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:103-179](), [libs/checkpoint-sqlite/langgraph/checkpoint/sqlite/utils.py:56-96]()\\n\\n## Thread and Namespace Organization\\n\\nCheckpoints are organized hierarchically by thread ID and optional namespace:\\n\\n```mermaid\\ngraph TD\\n    THREAD[\\\"Thread ID<br/>'conversation-123'\\\"] --> NS1[\\\"Default Namespace<br/>''\\\"]\\n    THREAD --> NS2[\\\"Custom Namespace<br/>'inner'\\\"]\\n    \\n    NS1 --> CP1[\\\"Checkpoint 1<br/>Main conversation flow\\\"]\\n    NS1 --> CP2[\\\"Checkpoint 2<br/>Main conversation flow\\\"]\\n    \\n    NS2 --> CP3[\\\"Checkpoint 3<br/>Sub-agent execution\\\"]\\n    NS2 --> CP4[\\\"Checkpoint 4<br/>Sub-agent execution\\\"]\\n```\\n\\nThe `delete_thread()` operation removes all checkpoints and writes across all namespaces for a given thread ID.\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:237-246](), [libs/checkpoint-postgres/langgraph/checkpoint/postgres/__init__.py:367-388]()\\n\\n## Version Management and Serialization\\n\\nThe checkpoint system tracks channel versions to detect updates and manage concurrent modifications. Each checkpoint stores both the current state and version information for all channels.\\n\\nThe `SerializerProtocol` interface handles serialization of complex objects, with the default `JsonPlusSerializer` supporting LangChain objects and custom types. Large objects are automatically separated into blob storage to optimize query performance.\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/base/__init__.py:57](), [libs/checkpoint/langgraph/checkpoint/base/__init__.py:126-133]()\", \"# Page: Store System\\n\\n# Store System\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/checkpoint-postgres/Makefile](libs/checkpoint-postgres/Makefile)\\n- [libs/checkpoint-postgres/langgraph/store/postgres/aio.py](libs/checkpoint-postgres/langgraph/store/postgres/aio.py)\\n- [libs/checkpoint-postgres/langgraph/store/postgres/base.py](libs/checkpoint-postgres/langgraph/store/postgres/base.py)\\n- [libs/checkpoint-postgres/tests/compose-postgres.yml](libs/checkpoint-postgres/tests/compose-postgres.yml)\\n- [libs/checkpoint-postgres/tests/conftest.py](libs/checkpoint-postgres/tests/conftest.py)\\n- [libs/checkpoint-postgres/tests/test_async_store.py](libs/checkpoint-postgres/tests/test_async_store.py)\\n- [libs/checkpoint-postgres/tests/test_store.py](libs/checkpoint-postgres/tests/test_store.py)\\n- [libs/checkpoint/Makefile](libs/checkpoint/Makefile)\\n- [libs/checkpoint/langgraph/store/base/__init__.py](libs/checkpoint/langgraph/store/base/__init__.py)\\n- [libs/checkpoint/langgraph/store/base/batch.py](libs/checkpoint/langgraph/store/base/batch.py)\\n- [libs/checkpoint/langgraph/store/base/embed.py](libs/checkpoint/langgraph/store/base/embed.py)\\n- [libs/checkpoint/langgraph/store/memory/__init__.py](libs/checkpoint/langgraph/store/memory/__init__.py)\\n- [libs/checkpoint/tests/test_store.py](libs/checkpoint/tests/test_store.py)\\n- [libs/langgraph/tests/memory_assert.py](libs/langgraph/tests/memory_assert.py)\\n\\n</details>\\n\\n\\n\\nThe Store System provides persistent key-value storage with hierarchical namespaces and optional vector search capabilities for LangGraph applications. This system enables long-term memory that persists across threads and conversations, supporting both simple key-value operations and semantic search through embeddings.\\n\\nFor information about checkpointing short-term state during graph execution, see [Checkpointing](#5.1). For serialization of store data, see [Serialization](#5.3).\\n\\n## Architecture Overview\\n\\nThe store system is built around a common interface with multiple backend implementations and optional features like vector search and TTL management.\\n\\n### Core Store Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Store Interface Layer\\\"\\n        BaseStore[\\\"BaseStore<br/>(Abstract Base Class)\\\"]\\n        AsyncBatchedBaseStore[\\\"AsyncBatchedBaseStore<br/>(Batching Wrapper)\\\"]\\n        \\n        BaseStore --> AsyncBatchedBaseStore\\n    end\\n    \\n    subgraph \\\"Data Types\\\"\\n        Item[\\\"Item<br/>(Stored Data + Metadata)\\\"]\\n        SearchItem[\\\"SearchItem<br/>(Item + Search Score)\\\"]\\n        Operations[\\\"Operations<br/>(GetOp, PutOp, SearchOp, ListNamespacesOp)\\\"]\\n        \\n        Item --> SearchItem\\n    end\\n    \\n    subgraph \\\"Store Implementations\\\"\\n        InMemoryStore[\\\"InMemoryStore<br/>(Dict-based)\\\"]\\n        PostgresStore[\\\"PostgresStore<br/>(SQL + pgvector)\\\"]\\n        AsyncPostgresStore[\\\"AsyncPostgresStore<br/>(Async SQL + pgvector)\\\"]\\n        \\n        BaseStore --> InMemoryStore\\n        AsyncBatchedBaseStore --> PostgresStore\\n        AsyncBatchedBaseStore --> AsyncPostgresStore\\n    end\\n    \\n    subgraph \\\"Optional Features\\\"\\n        VectorSearch[\\\"Vector Search<br/>(Embeddings + Similarity)\\\"]\\n        TTL[\\\"TTL Management<br/>(Expiration + Cleanup)\\\"]\\n        Indexing[\\\"Field Indexing<br/>(Search Configuration)\\\"]\\n        \\n        PostgresStore --> VectorSearch\\n        AsyncPostgresStore --> VectorSearch\\n        InMemoryStore --> VectorSearch\\n        PostgresStore --> TTL\\n        AsyncPostgresStore --> TTL\\n    end\\n    \\n    Operations --> BaseStore\\n    BaseStore --> Item\\n```\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:13-36](), [libs/checkpoint/langgraph/store/base/batch.py:58-81](), [libs/checkpoint-postgres/langgraph/store/postgres/base.py:230-236](), [libs/checkpoint/langgraph/store/memory/__init__.py:31-67]()\\n\\n### Namespace and Key Structure\\n\\nThe store system uses hierarchical namespaces to organize data, similar to a filesystem directory structure.\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Namespace Hierarchy\\\"\\n        Root[\\\"Root Namespace<br/>()\\\"]\\n        L1A[\\\"Level 1<br/>('users',)\\\"]\\n        L1B[\\\"Level 1<br/>('documents',)\\\"]\\n        L1C[\\\"Level 1<br/>('cache',)\\\"]\\n        \\n        L2A[\\\"Level 2<br/>('users', 'profiles')\\\"]\\n        L2B[\\\"Level 2<br/>('users', 'settings')\\\"]\\n        L2C[\\\"Level 2<br/>('documents', 'reports')\\\"]\\n        L2D[\\\"Level 2<br/>('cache', 'embeddings')\\\"]\\n        \\n        Keys1[\\\"Keys:<br/>user123, user456\\\"]\\n        Keys2[\\\"Keys:<br/>theme, lang\\\"]\\n        Keys3[\\\"Keys:<br/>q1_2024, q2_2024\\\"]\\n        Keys4[\\\"Keys:<br/>doc_abc, doc_xyz\\\"]\\n        \\n        Root --> L1A\\n        Root --> L1B  \\n        Root --> L1C\\n        L1A --> L2A\\n        L1A --> L2B\\n        L1B --> L2C\\n        L1C --> L2D\\n        \\n        L2A --> Keys1\\n        L2B --> Keys2\\n        L2C --> Keys3\\n        L2D --> Keys4\\n    end\\n```\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:67-91](), [libs/checkpoint/langgraph/store/base/__init__.py:158-201]()\\n\\n## Core Interface and Data Types\\n\\n### BaseStore Abstract Class\\n\\nThe `BaseStore` class defines the fundamental interface for all store implementations, providing both synchronous and asynchronous methods for data operations.\\n\\nKey methods include:\\n- `get`/`aget`: Retrieve single items by namespace and key\\n- `put`/`aput`: Store or update items  \\n- `delete`/`adelete`: Remove items\\n- `search`/`asearch`: Find items using filters and/or semantic search\\n- `list_namespaces`/`alist_namespaces`: Explore namespace hierarchy\\n- `batch`/`abatch`: Execute multiple operations efficiently\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:541-721]()\\n\\n### Item and SearchItem Classes\\n\\n```mermaid\\nclassDiagram\\n    class Item {\\n        +dict value\\n        +str key  \\n        +tuple namespace\\n        +datetime created_at\\n        +datetime updated_at\\n        +dict() dict\\n        +__eq__() bool\\n        +__hash__() int\\n    }\\n    \\n    class SearchItem {\\n        +float score\\n        +dict() dict\\n    }\\n    \\n    Item <|-- SearchItem\\n```\\n\\nThe `Item` class represents stored data with metadata, while `SearchItem` extends it with relevance scores for search results.\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:52-117](), [libs/checkpoint/langgraph/store/base/__init__.py:119-156]()\\n\\n### Operation Types\\n\\nThe store system uses operation objects to represent different types of requests:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Store Operations\\\"\\n        GetOp[\\\"GetOp<br/>namespace, key, refresh_ttl\\\"]\\n        PutOp[\\\"PutOp<br/>namespace, key, value, index, ttl\\\"]\\n        SearchOp[\\\"SearchOp<br/>namespace_prefix, filter, query, limit, offset\\\"]\\n        ListNamespacesOp[\\\"ListNamespacesOp<br/>match_conditions, max_depth, limit, offset\\\"]\\n    end\\n    \\n    subgraph \\\"Batch Processing\\\"\\n        BatchMethod[\\\"batch()/abatch()\\\"]\\n        Operations[\\\"List[Op]\\\"]\\n        Results[\\\"List[Result]\\\"]\\n        \\n        Operations --> BatchMethod\\n        BatchMethod --> Results\\n    end\\n    \\n    GetOp --> Operations\\n    PutOp --> Operations  \\n    SearchOp --> Operations\\n    ListNamespacesOp --> Operations\\n```\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:158-413](), [libs/checkpoint/langgraph/store/base/__init__.py:415-517]()\\n\\n## Store Implementations\\n\\n### InMemoryStore\\n\\nThe `InMemoryStore` provides a dictionary-based implementation suitable for development and testing.\\n\\nKey characteristics:\\n- Thread-safe operations using locks\\n- Optional vector search with configurable embeddings\\n- TTL support with background cleanup\\n- Hierarchical namespace storage using nested dictionaries\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"InMemoryStore Structure\\\"\\n        Store[\\\"InMemoryStore\\\"]\\n        Data[\\\"_data: dict<br/>(nested namespace dict)\\\"]\\n        Vectors[\\\"_vectors: dict<br/>(embeddings storage)\\\"] \\n        Lock[\\\"_lock: threading.Lock\\\"]\\n        TTL[\\\"_ttl_sweeper_task<br/>(background cleanup)\\\"]\\n        \\n        Store --> Data\\n        Store --> Vectors\\n        Store --> Lock\\n        Store --> TTL\\n    end\\n    \\n    subgraph \\\"Data Organization\\\"\\n        DataDict[\\\"{'users': {'profiles': {'user123': Item}}}\\\"]\\n        VectorDict[\\\"{'users.profiles.user123.field': embedding}\\\"]\\n        \\n        Data --> DataDict\\n        Vectors --> VectorDict\\n    end\\n```\\n\\nSources: [libs/checkpoint/langgraph/store/memory/__init__.py:67-147](), [libs/checkpoint/langgraph/store/memory/__init__.py:468-558]()\\n\\n### PostgresStore and AsyncPostgresStore\\n\\nThe PostgreSQL implementations provide production-ready persistence with advanced features through `PostgresStore` (synchronous) and `AsyncPostgresStore` (asynchronous) classes.\\n\\nDatabase schema:\\n- `store` table: Main key-value storage with TTL support\\n- `store_vectors` table: Vector embeddings for semantic search (optional)\\n- Migration tables: `store_migrations` and `vector_migrations` for schema versioning\\n\\n```mermaid\\nerDiagram\\n    store {\\n        text prefix PK\\n        text key PK\\n        jsonb value\\n        timestamp created_at\\n        timestamp updated_at\\n        timestamp expires_at\\n        float ttl_minutes\\n    }\\n    \\n    store_vectors {\\n        text prefix PK\\n        text key PK \\n        text field_name PK\\n        vector embedding\\n        timestamp created_at\\n        timestamp updated_at\\n    }\\n    \\n    store_migrations {\\n        int v PK\\n    }\\n    \\n    vector_migrations {\\n        int v PK\\n    }\\n    \\n    store ||--o{ store_vectors : \\\"has embeddings for\\\"\\n```\\n\\nConfiguration options include:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"PostgresStore Configuration\\\"\\n        PoolConfig[\\\"PoolConfig<br/>min_size, max_size, kwargs\\\"]\\n        PostgresIndexConfig[\\\"PostgresIndexConfig<br/>dims, embed, fields, distance_type\\\"]\\n        ANNIndexConfig[\\\"ANNIndexConfig<br/>kind, vector_type\\\"]\\n        TTLConfig[\\\"TTLConfig<br/>default_ttl, refresh_on_read, sweep_interval_minutes\\\"]\\n    end\\n    \\n    subgraph \\\"Vector Index Types\\\"\\n        HNSW[\\\"HNSWConfig<br/>m, ef_construction\\\"]\\n        IVFFlat[\\\"IVFFlatConfig<br/>nlist\\\"]\\n        Flat[\\\"Flat Index<br/>(no config)\\\"]\\n    end\\n    \\n    ANNIndexConfig --> HNSW\\n    ANNIndexConfig --> IVFFlat  \\n    ANNIndexConfig --> Flat\\n```\\n\\nKey PostgreSQL features:\\n- JSONB for efficient JSON operations and indexing\\n- pgvector extension for vector similarity search with multiple vector types (`vector`, `halfvec`, `bit`)\\n- Connection pooling via `PoolConfig` with configurable pool sizes\\n- Pipeline mode for reduced network roundtrips\\n- Configurable ANN index types: HNSW, IVFFlat, or flat indexes\\n- Distance metrics: L2, cosine similarity, inner product, or Hamming distance\\n- Automatic schema migrations through `MIGRATIONS` and `VECTOR_MIGRATIONS` sequences\\n\\nSources: [libs/checkpoint-postgres/langgraph/store/postgres/base.py:64-91](), [libs/checkpoint-postgres/langgraph/store/postgres/base.py:93-141](), [libs/checkpoint-postgres/langgraph/store/postgres/base.py:147-228](), [libs/checkpoint-postgres/langgraph/store/postgres/aio.py:42-131]()\\n\\n## Batching and Performance Optimization\\n\\n### AsyncBatchedBaseStore\\n\\nThe `AsyncBatchedBaseStore` wrapper provides automatic operation batching for improved performance through a background task queue system.\\n\\n```mermaid\\nsequenceDiagram\\n    participant Client1\\n    participant Client2\\n    participant AsyncBatchedBaseStore\\n    participant _aqueue\\n    participant _run_task\\n    participant ActualStore\\n    \\n    Client1->>+AsyncBatchedBaseStore: aget(ns1, key1)\\n    Client2->>+AsyncBatchedBaseStore: aget(ns2, key2)\\n    \\n    AsyncBatchedBaseStore->>_aqueue: put_nowait(Future1, GetOp1)\\n    AsyncBatchedBaseStore->>_aqueue: put_nowait(Future2, GetOp2)\\n    \\n    _run_task->>_aqueue: accumulate operations from same tick\\n    _run_task->>_run_task: _dedupe_ops([GetOp1, GetOp2])\\n    _run_task->>ActualStore: abatch([deduped_ops])\\n    ActualStore->>_run_task: [Result1, Result2]\\n    \\n    _run_task->>Client1: Future1.set_result(Result1)\\n    _run_task->>Client2: Future2.set_result(Result2)\\n    \\n    AsyncBatchedBaseStore-->>-Client1: return Result1\\n    AsyncBatchedBaseStore-->>-Client2: return Result2\\n```\\n\\nThe batching system provides several optimizations:\\n- **Operation accumulation**: Collects operations from the same event loop tick via `_aqueue`\\n- **Deduplication**: The `_dedupe_ops` function removes duplicate `GetOp`, `SearchOp`, and `ListNamespacesOp` operations\\n- **Put operation optimization**: Multiple `PutOp` operations to the same namespace/key are merged, keeping only the latest\\n- **Automatic task management**: The `_run` background task handles the batching lifecycle\\n- **Error handling**: Failed operations are propagated to all affected futures\\n\\nSources: [libs/checkpoint/langgraph/store/base/batch.py:58-81](), [libs/checkpoint/langgraph/store/base/batch.py:283-323](), [libs/checkpoint/langgraph/store/base/batch.py:326-366]()\\n\\n## Vector Search and Indexing\\n\\n### Index Configuration\\n\\nVector search requires configuring embeddings through the `IndexConfig` interface and its PostgreSQL-specific extension `PostgresIndexConfig`:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Base IndexConfig\\\"\\n        IndexConfig[\\\"IndexConfig<br/>(base configuration)\\\"]\\n        dims[\\\"dims: int<br/>(embedding dimensions)\\\"]\\n        embed[\\\"embed: Embeddings | EmbeddingsFunc | AEmbeddingsFunc | str\\\"]\\n        fields[\\\"fields: list[str] | None<br/>(JSON paths to index)\\\"]\\n        \\n        IndexConfig --> dims\\n        IndexConfig --> embed  \\n        IndexConfig --> fields\\n    end\\n    \\n    subgraph \\\"PostgresIndexConfig Extensions\\\"\\n        PostgresIndexConfig[\\\"PostgresIndexConfig<br/>(extends IndexConfig)\\\"]\\n        ann_index_config[\\\"ann_index_config: ANNIndexConfig\\\"]\\n        distance_type[\\\"distance_type: l2 | inner_product | cosine\\\"]\\n        \\n        PostgresIndexConfig --> ann_index_config\\n        PostgresIndexConfig --> distance_type\\n    end\\n    \\n    subgraph \\\"Embedding Function Types\\\"\\n        LangChainEmbeddings[\\\"Embeddings<br/>(LangChain interface)\\\"]\\n        EmbeddingsFunc[\\\"EmbeddingsFunc<br/>(sync function)\\\"]\\n        AEmbeddingsFunc[\\\"AEmbeddingsFunc<br/>(async function)\\\"]\\n        ProviderString[\\\"str<br/>('openai:text-embedding-3-small')\\\"]\\n        \\n        embed --> LangChainEmbeddings\\n        embed --> EmbeddingsFunc\\n        embed --> AEmbeddingsFunc\\n        embed --> ProviderString\\n    end\\n    \\n    subgraph \\\"Text Extraction Process\\\"\\n        Document[\\\"Document\\\"]\\n        tokenize_path[\\\"tokenize_path()<br/>(parse JSON paths)\\\"]\\n        get_text_at_path[\\\"get_text_at_path()<br/>(extract text values)\\\"]\\n        ensure_embeddings[\\\"ensure_embeddings()<br/>(normalize embedding function)\\\"]\\n        \\n        Document --> tokenize_path\\n        tokenize_path --> get_text_at_path\\n        get_text_at_path --> ensure_embeddings\\n    end\\n```\\n\\nThe `ensure_embeddings` function handles multiple embedding function types:\\n- **LangChain Embeddings**: Direct usage of the interface\\n- **Provider strings**: Automatic initialization via `init_embeddings` (requires `langchain>=0.3.9`)\\n- **Custom functions**: Wrapped in `EmbeddingsLambda` class for compatibility\\n- **Async functions**: Supported through `AEmbeddingsFunc` type\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:549-637](), [libs/checkpoint-postgres/langgraph/store/postgres/base.py:214-228](), [libs/checkpoint/langgraph/store/base/embed.py:34-102](), [libs/checkpoint/langgraph/store/base/embed.py:105-224]()\\n\\n### Field Path Extraction\\n\\nThe system supports sophisticated JSON path expressions through the `get_text_at_path` and `tokenize_path` functions for extracting text from nested document structures:\\n\\n| Path Type | Syntax | Example | Description |\\n|-----------|--------|---------|-------------|\\n| Simple fields | `\\\"field\\\"` | `\\\"title\\\"`, `\\\"content\\\"` | Top-level field access |\\n| Nested fields | `\\\"parent.child\\\"` | `\\\"metadata.author\\\"`, `\\\"content.sections\\\"` | Dot notation for nested objects |\\n| Array indexing | `\\\"field[index]\\\"` | `\\\"sections[0].text\\\"`, `\\\"tags[-1]\\\"` | Specific array element (supports negative indexing) |\\n| Array wildcards | `\\\"field[*]\\\"` | `\\\"tags[*]\\\"`, `\\\"sections[*].title\\\"` | All array elements (creates separate embeddings) |\\n| Object wildcards | `\\\"field.*\\\"` | `\\\"metadata.*\\\"` | All values in an object |\\n| Multi-field selection | `\\\"{field1,field2}\\\"` | `\\\"{title,content}\\\"`, `\\\"items[*].{name,description}\\\"` | Multiple fields in one path |\\n| Root document | `\\\"$\\\"` or `\\\"\\\"` | `\\\"$\\\"` | Entire document as JSON string |\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Path Processing Pipeline\\\"\\n        RawPath[\\\"Raw Path String<br/>e.g., 'items[*].{name,description}'\\\"]\\n        tokenize_path[\\\"tokenize_path()<br/>Split into tokens\\\"]\\n        TokenList[\\\"Token List<br/>['items', '[*]', '{name,description}']\\\"]\\n        get_text_at_path[\\\"get_text_at_path()<br/>Extract text values\\\"]\\n        TextList[\\\"Text Results<br/>['item1_name', 'item1_desc', 'item2_name', ...]\\\"]\\n        \\n        RawPath --> tokenize_path\\n        tokenize_path --> TokenList\\n        TokenList --> get_text_at_path\\n        get_text_at_path --> TextList\\n    end\\n```\\n\\nThe extraction process handles:\\n- **Type coercion**: Converts numbers, booleans to strings; serializes objects/arrays to JSON\\n- **Missing fields**: Silently skipped (no errors for non-existent paths)\\n- **Complex nesting**: Supports arbitrary depth combinations of arrays and objects\\n- **Multi-field syntax**: Comma-separated fields within `{}` braces\\n\\nSources: [libs/checkpoint/langgraph/store/base/embed.py:226-317](), [libs/checkpoint/langgraph/store/base/embed.py:322-386](), [libs/checkpoint/tests/test_store.py:73-139]()\\n\\n## TTL and Data Lifecycle\\n\\n### TTL Configuration and Management\\n\\nThe store system supports automatic expiration of stored items:\\n\\n```mermaid\\nstateDiagram-v2\\n    [*] --> Active: put(ttl=minutes)\\n    Active --> Expired: TTL timeout\\n    Active --> Active: refresh_ttl=True on read\\n    Active --> [*]: delete()\\n    Expired --> [*]: TTL sweeper cleanup\\n    \\n    note right of Active\\n        expires_at = now + ttl\\n        ttl_minutes stored\\n    end note\\n    \\n    note right of Expired  \\n        expires_at < now\\n        eligible for cleanup\\n    end note\\n```\\n\\nTTL features:\\n- Per-item TTL specification in minutes\\n- Automatic expiration timestamp calculation  \\n- Background sweeper task for cleanup\\n- Optional TTL refresh on read operations\\n- Configurable sweep intervals\\n\\nSources: [libs/checkpoint/langgraph/store/base/__init__.py:518-539](), [libs/checkpoint-postgres/langgraph/store/postgres/base.py:330-351]()\\n\\n## Usage Patterns and Examples\\n\\n### Basic Operations\\n\\n```python\\n# Hierarchical storage\\nstore.put((\\\"users\\\", \\\"123\\\"), \\\"profile\\\", {\\\"name\\\": \\\"Alice\\\", \\\"role\\\": \\\"admin\\\"})\\nstore.put((\\\"users\\\", \\\"123\\\"), \\\"settings\\\", {\\\"theme\\\": \\\"dark\\\", \\\"lang\\\": \\\"en\\\"})\\n\\n# Retrieval with namespace navigation\\nprofile = store.get((\\\"users\\\", \\\"123\\\"), \\\"profile\\\")\\nsettings = store.get((\\\"users\\\", \\\"123\\\"), \\\"settings\\\")\\n\\n# Search within namespace\\nuser_data = store.search((\\\"users\\\", \\\"123\\\"), limit=10)\\n```\\n\\n### Vector Search Usage\\n\\n```python\\n# Configure store with embeddings\\nstore = PostgresStore.from_conn_string(\\n    conn_string,\\n    index={\\n        \\\"dims\\\": 1536,\\n        \\\"embed\\\": OpenAIEmbeddings(),\\n        \\\"fields\\\": [\\\"content\\\", \\\"title\\\"]\\n    }\\n)\\n\\n# Store documents with automatic indexing\\nstore.put((\\\"docs\\\",), \\\"guide1\\\", {\\n    \\\"title\\\": \\\"Python Guide\\\", \\n    \\\"content\\\": \\\"Learn Python programming...\\\"\\n})\\n\\n# Semantic search\\nresults = store.search((\\\"docs\\\",), query=\\\"python tutorials\\\", limit=5)\\n```\\n\\n### Batch Operations\\n\\n```python\\n# Efficient batch operations\\nops = [\\n    PutOp((\\\"cache\\\",), f\\\"item_{i}\\\", {\\\"data\\\": f\\\"value_{i}\\\"})\\n    for i in range(100)\\n]\\nstore.batch(ops)\\n\\n# Mixed operation types\\nmixed_ops = [\\n    GetOp((\\\"users\\\",), \\\"profile\\\"),\\n    PutOp((\\\"cache\\\",), \\\"new_item\\\", {\\\"temp\\\": True}),\\n    SearchOp((\\\"docs\\\",), query=\\\"search term\\\")\\n]\\nresults = store.batch(mixed_ops)\\n```\\n\\nSources: [libs/checkpoint-postgres/langgraph/store/postgres/aio.py:45-84](), [libs/checkpoint/langgraph/store/memory/__init__.py:4-31](), [libs/checkpoint/tests/test_store.py:141-192]()\", \"# Page: Serialization\\n\\n# Serialization\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/checkpoint-postgres/pyproject.toml](libs/checkpoint-postgres/pyproject.toml)\\n- [libs/checkpoint-sqlite/pyproject.toml](libs/checkpoint-sqlite/pyproject.toml)\\n- [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py](libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py)\\n- [libs/checkpoint/langgraph/checkpoint/serde/types.py](libs/checkpoint/langgraph/checkpoint/serde/types.py)\\n- [libs/checkpoint/tests/test_jsonplus.py](libs/checkpoint/tests/test_jsonplus.py)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's serialization system, which handles the efficient conversion of Python objects to and from binary/JSON formats for checkpoint persistence. The serialization system is primarily implemented through the `JsonPlusSerializer` class, which provides robust support for complex Python types including Pydantic models, dataclasses, NumPy arrays, and LangChain objects.\\n\\nFor information about the broader persistence architecture, see [Persistence System](#5). For details about checkpoint storage backends, see [Checkpointing](#5.1).\\n\\n## Overview\\n\\nLangGraph's serialization system is designed to handle the complex object graphs that result from LLM application state, including:\\n- Python standard library types (datetime, UUID, pathlib, etc.)\\n- Pydantic models (both v1 and v2)\\n- Dataclasses and named tuples\\n- NumPy arrays and pandas DataFrames\\n- LangChain Serializable objects\\n- Custom LangGraph types like `Send` commands\\n\\nThe system uses a two-tier approach: efficient binary serialization via msgpack for performance, with JSON fallback for compatibility and debugging.\\n\\n## JsonPlusSerializer Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"JsonPlusSerializer\\\"\\n        API[\\\"Public API<br/>dumps_typed() / loads_typed()\\\"]\\n        ROUTER[\\\"Serialization Router<br/>_default() / _reviver()\\\"]\\n        MSGPACK[\\\"Msgpack Path<br/>ormsgpack + extensions\\\"]\\n        JSON[\\\"JSON Path<br/>json + custom encoders\\\"]\\n        FALLBACK[\\\"Pickle Fallback<br/>pickle.dumps/loads\\\"]\\n    end\\n    \\n    subgraph \\\"Extension System\\\"\\n        EXT_CONST[\\\"EXT_CONSTRUCTOR_*<br/>Object reconstruction\\\"]\\n        EXT_PYDANTIC[\\\"EXT_PYDANTIC_*<br/>Pydantic model handling\\\"]\\n        EXT_NUMPY[\\\"EXT_NUMPY_ARRAY<br/>NumPy array serialization\\\"]\\n        EXT_METHOD[\\\"EXT_METHOD_*<br/>Method-based reconstruction\\\"]\\n    end\\n    \\n    subgraph \\\"Type Detection\\\"\\n        SERIALIZABLE[\\\"LangChain Serializable<br/>to_json() method\\\"]\\n        PYDANTIC_V2[\\\"Pydantic v2<br/>model_dump() method\\\"]\\n        PYDANTIC_V1[\\\"Pydantic v1<br/>dict() method\\\"]\\n        DATACLASS[\\\"Dataclasses<br/>dataclasses.fields()\\\"]\\n        STDLIB[\\\"Standard Library<br/>datetime, UUID, pathlib, etc.\\\"]\\n    end\\n    \\n    API --> ROUTER\\n    ROUTER --> MSGPACK\\n    ROUTER --> JSON\\n    MSGPACK --> FALLBACK\\n    \\n    ROUTER --> SERIALIZABLE\\n    ROUTER --> PYDANTIC_V2\\n    ROUTER --> PYDANTIC_V1\\n    ROUTER --> DATACLASS\\n    ROUTER --> STDLIB\\n    \\n    MSGPACK --> EXT_CONST\\n    MSGPACK --> EXT_PYDANTIC\\n    MSGPACK --> EXT_NUMPY\\n    MSGPACK --> EXT_METHOD\\n```\\n\\n**Serialization Flow Diagram**\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:40-245]()\\n\\n## Core Interface\\n\\nThe `JsonPlusSerializer` implements the `SerializerProtocol` interface with two main methods:\\n\\n| Method | Purpose | Return Type |\\n|--------|---------|-------------|\\n| `dumps_typed(obj)` | Serialize object with type information | `tuple[str, bytes]` |\\n| `loads_typed(data)` | Deserialize object from typed data | `Any` |\\n| `dumps(obj)` | Serialize to JSON bytes | `bytes` |\\n| `loads(data)` | Deserialize from JSON bytes | `Any` |\\n\\nThe typed methods return a tuple of `(type_string, data_bytes)` where the type string indicates the serialization format used:\\n\\n- `\\\"msgpack\\\"` - Binary msgpack with extensions\\n- `\\\"json\\\"` - UTF-8 encoded JSON\\n- `\\\"pickle\\\"` - Pickle fallback (if enabled)\\n- `\\\"bytes\\\"` - Raw bytes passthrough\\n- `\\\"bytearray\\\"` - Bytearray passthrough\\n- `\\\"null\\\"` - Null value\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:202-244]()\\n\\n## Supported Data Types\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Python Standard Library\\\"\\n        DATETIME[\\\"datetime<br/>date, time, timezone\\\"]\\n        COLLECTIONS[\\\"Collections<br/>set, frozenset, deque\\\"]\\n        STDLIB_MISC[\\\"Misc Types<br/>UUID, Decimal, pathlib.Path\\\"]\\n        IP_ADDR[\\\"IP Addresses<br/>IPv4/IPv6 Address/Network\\\"]\\n        REGEX[\\\"Regular Expressions<br/>re.Pattern\\\"]\\n    end\\n    \\n    subgraph \\\"Object Models\\\"\\n        PYDANTIC[\\\"Pydantic Models<br/>v1 and v2 support\\\"]\\n        DATACLASS[\\\"Dataclasses<br/>Regular and slots\\\"]\\n        NAMEDTUPLE[\\\"Named Tuples<br/>_asdict() method\\\"]\\n        ENUM[\\\"Enumerations<br/>Enum classes\\\"]\\n    end\\n    \\n    subgraph \\\"Scientific Computing\\\"\\n        NUMPY[\\\"NumPy Arrays<br/>All dtypes and shapes\\\"]\\n        PANDAS[\\\"Pandas Objects<br/>DataFrame, Series\\\"]\\n    end\\n    \\n    subgraph \\\"LangGraph Types\\\"\\n        SEND[\\\"Send Commands<br/>SendProtocol\\\"]\\n        STORE_ITEM[\\\"Store Items<br/>Item class\\\"]\\n        LC_SERIALIZABLE[\\\"LangChain Serializable<br/>to_json() method\\\"]\\n    end\\n    \\n    DATETIME --> MSGPACK_EXT[\\\"Msgpack Extensions\\\"]\\n    COLLECTIONS --> MSGPACK_EXT\\n    STDLIB_MISC --> MSGPACK_EXT\\n    PYDANTIC --> MSGPACK_EXT\\n    DATACLASS --> MSGPACK_EXT\\n    NUMPY --> MSGPACK_EXT\\n    SEND --> MSGPACK_EXT\\n```\\n\\n**Supported Data Types Overview**\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:77-155](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:258-482]()\\n\\n## Extension Type System\\n\\nThe msgpack serialization uses a comprehensive extension type system to handle Python objects efficiently:\\n\\n| Extension Code | Purpose | Reconstruction Method |\\n|----------------|---------|----------------------|\\n| `EXT_CONSTRUCTOR_SINGLE_ARG` (0) | Single argument constructors | `Class(arg)` |\\n| `EXT_CONSTRUCTOR_POS_ARGS` (1) | Positional argument constructors | `Class(*args)` |\\n| `EXT_CONSTRUCTOR_KW_ARGS` (2) | Keyword argument constructors | `Class(**kwargs)` |\\n| `EXT_METHOD_SINGLE_ARG` (3) | Method-based reconstruction | `Class.method(arg)` |\\n| `EXT_PYDANTIC_V1` (4) | Pydantic v1 models | `Class.construct(**data)` |\\n| `EXT_PYDANTIC_V2` (5) | Pydantic v2 models | `Class.model_construct(**data)` |\\n| `EXT_NUMPY_ARRAY` (6) | NumPy arrays | Custom reconstruction |\\n\\nThe extension system encodes objects as tuples containing:\\n- Module name\\n- Class name  \\n- Constructor arguments or data\\n- Optional method name\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:249-256](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:485-571]()\\n\\n## Serialization Modes\\n\\n### Standard Mode\\n\\nIn standard mode, objects are fully reconstructed to their original types:\\n\\n```python\\nserde = JsonPlusSerializer()\\nobj = datetime(2024, 4, 19, 23, 4, 57)\\ntype_str, data = serde.dumps_typed(obj)\\nrestored = serde.loads_typed((type_str, data))\\n# restored is a datetime object\\n```\\n\\n### JSON-Compatible Mode\\n\\nJSON-compatible mode simplifies objects to basic JSON types for interoperability:\\n\\n```python\\nserde = JsonPlusSerializer(__unpack_ext_hook__=_msgpack_ext_hook_to_json)\\nobj = datetime(2024, 4, 19, 23, 4, 57)\\ntype_str, data = serde.dumps_typed(obj)\\nrestored = serde.loads_typed((type_str, data))\\n# restored is an ISO format string\\n```\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:573-664](), [libs/checkpoint/tests/test_jsonplus.py:174-279]()\\n\\n## Configuration Options\\n\\nThe `JsonPlusSerializer` supports several configuration options:\\n\\n| Parameter | Type | Purpose |\\n|-----------|------|---------|\\n| `pickle_fallback` | `bool` | Enable pickle for unsupported types |\\n| `__unpack_ext_hook__` | `Callable` | Custom extension unpacking hook |\\n\\nThe pickle fallback is particularly useful for complex objects like pandas DataFrames that don't have efficient msgpack representations.\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:43-54]()\\n\\n## Integration with Checkpoint System\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Checkpoint Backends\\\"\\n        POSTGRES[\\\"PostgresCheckpointSaver<br/>uses orjson\\\"]\\n        SQLITE[\\\"SqliteCheckpointSaver<br/>uses JsonPlusSerializer\\\"]\\n        MEMORY[\\\"MemoryCheckpointSaver<br/>uses JsonPlusSerializer\\\"]\\n    end\\n    \\n    subgraph \\\"Serialization Layer\\\"\\n        JSONPLUS[\\\"JsonPlusSerializer<br/>Main serializer\\\"]\\n        ORJSON[\\\"orjson<br/>Fast JSON library\\\"]\\n        PROTOCOL[\\\"SerializerProtocol<br/>Interface definition\\\"]\\n    end\\n    \\n    subgraph \\\"Data Flow\\\"\\n        GRAPH_STATE[\\\"Graph State<br/>Python objects\\\"]\\n        CHECKPOINT[\\\"Checkpoint<br/>Serialized state\\\"]\\n        STORAGE[\\\"Storage Backend<br/>Database/Memory\\\"]\\n    end\\n    \\n    POSTGRES --> ORJSON\\n    SQLITE --> JSONPLUS\\n    MEMORY --> JSONPLUS\\n    \\n    JSONPLUS --> PROTOCOL\\n    ORJSON --> PROTOCOL\\n    \\n    GRAPH_STATE --> CHECKPOINT\\n    CHECKPOINT --> STORAGE\\n    \\n    PROTOCOL --> CHECKPOINT\\n```\\n\\n**Checkpoint Integration Architecture**\\n\\nThe serialization system integrates with LangGraph's checkpoint backends through the `SerializerProtocol`. Different backends may use different serializers based on their requirements:\\n\\n- PostgreSQL backend uses `orjson` for performance\\n- SQLite and Memory backends use `JsonPlusSerializer` for full Python type support\\n\\nSources: [libs/checkpoint-postgres/pyproject.toml:16](), [libs/checkpoint-sqlite/pyproject.toml:15-16](), [libs/checkpoint/langgraph/checkpoint/serde/base.py]()\\n\\n## Error Handling and Fallbacks\\n\\nThe serialization system includes robust error handling:\\n\\n1. **UTF-8 Encoding Errors**: Falls back to JSON serialization\\n2. **Unknown Types**: Raises `TypeError` with descriptive message  \\n3. **Reconstruction Failures**: Returns `None` for graceful degradation\\n4. **Pickle Fallback**: Optional last resort for complex objects\\n\\nException types are specially handled - they are converted to string representations rather than being fully serialized to prevent security issues.\\n\\nSources: [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:217-222](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:479-481](), [libs/checkpoint/langgraph/checkpoint/serde/jsonplus.py:184-198]()\", \"# Page: Client-Server Architecture\\n\\n# Client-Server Architecture\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/reference/sdk/python_sdk_ref.md](docs/docs/cloud/reference/sdk/python_sdk_ref.md)\\n- [libs/checkpoint/tests/test_redis_cache.py](libs/checkpoint/tests/test_redis_cache.py)\\n- [libs/sdk-py/Makefile](libs/sdk-py/Makefile)\\n- [libs/sdk-py/langgraph_sdk/__init__.py](libs/sdk-py/langgraph_sdk/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/__init__.py](libs/sdk-py/langgraph_sdk/auth/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/exceptions.py](libs/sdk-py/langgraph_sdk/auth/exceptions.py)\\n- [libs/sdk-py/langgraph_sdk/auth/types.py](libs/sdk-py/langgraph_sdk/auth/types.py)\\n- [libs/sdk-py/langgraph_sdk/client.py](libs/sdk-py/langgraph_sdk/client.py)\\n- [libs/sdk-py/langgraph_sdk/schema.py](libs/sdk-py/langgraph_sdk/schema.py)\\n- [libs/sdk-py/langgraph_sdk/sse.py](libs/sdk-py/langgraph_sdk/sse.py)\\n- [libs/sdk-py/pyproject.toml](libs/sdk-py/pyproject.toml)\\n- [libs/sdk-py/tests/test_api_parity.py](libs/sdk-py/tests/test_api_parity.py)\\n- [libs/sdk-py/uv.lock](libs/sdk-py/uv.lock)\\n\\n</details>\\n\\n\\n\\nThis document provides an overview of LangGraph's distributed client-server architecture, focusing on how clients communicate with remote LangGraph servers through HTTP APIs. For specific client implementations, see [RemoteGraph Client](#6.1), [Python SDK](#6.2), [JavaScript SDK](#6.3), and [React UI Components](#6.4). For deployment options that enable this architecture, see [Deployment and Platform](#7).\\n\\n## Architecture Overview\\n\\nLangGraph implements a distributed architecture where graph execution can occur on remote servers while clients maintain a familiar local interface. This enables deployment of LangGraph applications as scalable services while preserving the developer experience of local execution.\\n\\n### High-Level Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Client Applications\\\"\\n        PY_CLIENT[\\\"Python Application<br/>LangGraphClient\\\"]\\n        JS_CLIENT[\\\"JavaScript Application<br/>LangGraphClient\\\"]\\n        REACT_APP[\\\"React Application<br/>useStream hook\\\"]\\n    end\\n    \\n    subgraph \\\"SDK Layer\\\"\\n        PY_SDK[\\\"langgraph_sdk.client<br/>HttpClient\\\"]\\n        JS_SDK[\\\"@langchain/langgraph-sdk<br/>Client\\\"]\\n        REACT_SDK[\\\"@langchain/langgraph-react<br/>LoadExternalComponent\\\"]\\n    end\\n    \\n    subgraph \\\"Transport Layer\\\"\\n        HTTP[\\\"HTTP/HTTPS<br/>HTTPX Transport\\\"]\\n        SSE[\\\"Server-Sent Events<br/>SSEDecoder\\\"]\\n        AUTH[\\\"Authentication<br/>API Key / JWT\\\"]\\n    end\\n    \\n    subgraph \\\"LangGraph Server\\\"\\n        API_SERVER[\\\"langgraph_api.server<br/>FastAPI Application\\\"]\\n        GRAPH_RUNTIME[\\\"Graph Execution Engine<br/>Pregel Runtime\\\"]\\n        PERSISTENCE[\\\"Checkpoint System<br/>PostgreSQL/SQLite\\\"]\\n    end\\n    \\n    PY_CLIENT --> PY_SDK\\n    JS_CLIENT --> JS_SDK\\n    REACT_APP --> REACT_SDK\\n    \\n    PY_SDK --> HTTP\\n    JS_SDK --> HTTP\\n    REACT_SDK --> SSE\\n    \\n    HTTP --> API_SERVER\\n    SSE --> API_SERVER\\n    AUTH --> API_SERVER\\n    \\n    API_SERVER --> GRAPH_RUNTIME\\n    API_SERVER --> PERSISTENCE\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:152-221](), [libs/sdk-py/langgraph_sdk/client.py:223-241](), [libs/sdk-py/langgraph_sdk/sse.py:1-152]()\\n\\n### Core Components\\n\\nThe client-server architecture consists of several key layers:\\n\\n| Component | Purpose | Key Classes |\\n|-----------|---------|-------------|\\n| **Client Applications** | Application code using LangGraph | User applications |\\n| **SDK Layer** | Language-specific client libraries | `LangGraphClient`, `HttpClient` |\\n| **Transport Layer** | Communication protocols | HTTPX, SSE, Authentication |\\n| **Server Layer** | LangGraph API server | FastAPI application |\\n| **Execution Layer** | Graph runtime and persistence | Pregel engine, checkpoint savers |\\n\\n## Communication Patterns\\n\\n### HTTP-Based Resource API\\n\\nThe LangGraph server exposes a REST-like API organized around core resources. Each resource type has dedicated client classes that handle CRUD operations and resource-specific functionality.\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Client SDK\\\"\\n        LC[\\\"LangGraphClient\\\"]\\n        AC[\\\"AssistantsClient\\\"]\\n        TC[\\\"ThreadsClient\\\"] \\n        RC[\\\"RunsClient\\\"]\\n        CC[\\\"CronClient\\\"]\\n        SC[\\\"StoreClient\\\"]\\n    end\\n    \\n    subgraph \\\"HTTP Layer\\\"\\n        HTTP_CLIENT[\\\"HttpClient<br/>HTTPX wrapper\\\"]\\n    end\\n    \\n    subgraph \\\"Server Endpoints\\\"\\n        ASST_EP[\\\"/assistants/*<br/>Assistant management\\\"]\\n        THREAD_EP[\\\"/threads/*<br/>Thread operations\\\"]\\n        RUN_EP[\\\"/runs/*<br/>Run execution\\\"]\\n        CRON_EP[\\\"/crons/*<br/>Scheduled tasks\\\"]\\n        STORE_EP[\\\"/store/*<br/>Document storage\\\"]\\n    end\\n    \\n    LC --> AC\\n    LC --> TC\\n    LC --> RC\\n    LC --> CC\\n    LC --> SC\\n    \\n    AC --> HTTP_CLIENT\\n    TC --> HTTP_CLIENT\\n    RC --> HTTP_CLIENT\\n    CC --> HTTP_CLIENT\\n    SC --> HTTP_CLIENT\\n    \\n    HTTP_CLIENT --> ASST_EP\\n    HTTP_CLIENT --> THREAD_EP\\n    HTTP_CLIENT --> RUN_EP\\n    HTTP_CLIENT --> CRON_EP\\n    HTTP_CLIENT --> STORE_EP\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:234-241](), [libs/sdk-py/langgraph_sdk/client.py:486-1023]()\\n\\n### Streaming Communication\\n\\nFor real-time graph execution, the architecture supports Server-Sent Events (SSE) streaming. This enables clients to receive incremental updates as graphs execute on the server.\\n\\n```mermaid\\ngraph TB\\n    CLIENT[\\\"Client Application\\\"]\\n    SDK[\\\"langgraph_sdk.client<br/>HttpClient.stream()\\\"]\\n    SSE_DECODER[\\\"SSEDecoder<br/>sse.py\\\"]\\n    TRANSPORT[\\\"HTTPX Streaming<br/>text/event-stream\\\"]\\n    SERVER[\\\"LangGraph Server<br/>Streaming Endpoints\\\"]\\n    \\n    CLIENT --> SDK\\n    SDK --> SSE_DECODER\\n    SSE_DECODER --> TRANSPORT\\n    TRANSPORT --> SERVER\\n    \\n    SERVER -.->|\\\"event: data\\\"| TRANSPORT\\n    TRANSPORT -.->|\\\"StreamPart\\\"| SSE_DECODER\\n    SSE_DECODER -.->|\\\"Parsed Events\\\"| SDK\\n    SDK -.->|\\\"AsyncIterator\\\"| CLIENT\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:413-459](), [libs/sdk-py/langgraph_sdk/sse.py:77-134]()\\n\\n## Resource Organization\\n\\nThe API organizes functionality into distinct resource types, each with its own client and set of operations:\\n\\n### Resource Types and Operations\\n\\n| Resource | Client Class | Primary Operations | Schema Types |\\n|----------|--------------|-------------------|--------------|\\n| **Assistants** | `AssistantsClient` | create, get, update, delete, search | `Assistant`, `AssistantVersion` |\\n| **Threads** | `ThreadsClient` | create, get, update, delete, search | `Thread`, `ThreadState` |\\n| **Runs** | `RunsClient` | create, get, stream, cancel, wait | `Run`, `RunCreate` |\\n| **Crons** | `CronClient` | create, get, update, delete, search | `Cron` |\\n| **Store** | `StoreClient` | put, get, delete, search | `Item`, `SearchItem` |\\n\\nSources: [libs/sdk-py/langgraph_sdk/schema.py:205-556](), [libs/sdk-py/langgraph_sdk/client.py:486-4318]()\\n\\n### Data Flow Patterns\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Client Side\\\"\\n        APP[\\\"Application Code\\\"]\\n        CLIENT[\\\"LangGraphClient\\\"]\\n        SCHEMA[\\\"Schema Validation<br/>Pydantic Models\\\"]\\n    end\\n    \\n    subgraph \\\"Transport\\\"\\n        SERIALIZER[\\\"orjson Serialization<br/>_orjson_default()\\\"]\\n        HTTP_REQ[\\\"HTTP Request<br/>POST/GET/PUT/DELETE\\\"]\\n        HTTP_RESP[\\\"HTTP Response<br/>JSON Payload\\\"]\\n    end\\n    \\n    subgraph \\\"Server Side\\\"\\n        API[\\\"FastAPI Endpoints\\\"]\\n        VALIDATION[\\\"Request Validation\\\"]\\n        BUSINESS[\\\"Business Logic\\\"]\\n        PERSISTENCE[\\\"Database/Storage\\\"]\\n    end\\n    \\n    APP --> CLIENT\\n    CLIENT --> SCHEMA\\n    SCHEMA --> SERIALIZER\\n    SERIALIZER --> HTTP_REQ\\n    HTTP_REQ --> API\\n    \\n    API --> VALIDATION\\n    VALIDATION --> BUSINESS\\n    BUSINESS --> PERSISTENCE\\n    \\n    PERSISTENCE -.-> BUSINESS\\n    BUSINESS -.-> HTTP_RESP\\n    HTTP_RESP -.-> CLIENT\\n    CLIENT -.-> APP\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:120-129](), [libs/sdk-py/langgraph_sdk/client.py:461-484]()\\n\\n## Authentication and Authorization\\n\\nThe client-server architecture includes a comprehensive authentication and authorization system that operates at the transport layer.\\n\\n### Authentication Flow\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Client Configuration\\\"\\n        API_KEY[\\\"API Key<br/>Environment Variables\\\"]\\n        HEADERS[\\\"Custom Headers<br/>User-Agent, etc.\\\"]\\n    end\\n    \\n    subgraph \\\"SDK Processing\\\"\\n        GET_HEADERS[\\\"_get_headers()<br/>Header Assembly\\\"]\\n        HTTP_CLIENT[\\\"HttpClient<br/>Request Processing\\\"]\\n    end\\n    \\n    subgraph \\\"Server Processing\\\"\\n        AUTH_MIDDLEWARE[\\\"Auth Middleware<br/>@auth.authenticate\\\"]\\n        USER_CONTEXT[\\\"User Context<br/>BaseUser Protocol\\\"]\\n        AUTHZ_HANDLERS[\\\"Authorization<br/>@auth.on handlers\\\"]\\n    end\\n    \\n    API_KEY --> GET_HEADERS\\n    HEADERS --> GET_HEADERS\\n    GET_HEADERS --> HTTP_CLIENT\\n    HTTP_CLIENT --> AUTH_MIDDLEWARE\\n    AUTH_MIDDLEWARE --> USER_CONTEXT\\n    USER_CONTEXT --> AUTHZ_HANDLERS\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:84-118](), [libs/sdk-py/langgraph_sdk/auth/__init__.py:181-254](), [libs/sdk-py/langgraph_sdk/auth/types.py:349-401]()\\n\\n### Authorization Model\\n\\nThe authorization system supports fine-grained access control through resource and action-specific handlers:\\n\\n| Authorization Level | Handler Pattern | Example |\\n|-------------------|-----------------|---------|\\n| **Global** | `@auth.on` | All requests |\\n| **Resource** | `@auth.on.threads` | All thread operations |\\n| **Action** | `@auth.on.threads.create` | Thread creation only |\\n| **Store** | `@auth.on.store` | Document storage operations |\\n\\nSources: [libs/sdk-py/langgraph_sdk/auth/__init__.py:110-174](), [libs/sdk-py/langgraph_sdk/auth/types.py:882-1013]()\\n\\n## Transport Mechanisms\\n\\n### HTTP Client Implementation\\n\\nThe SDK implements a robust HTTP client layer that handles connection management, error handling, and serialization:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Client Factory\\\"\\n        GET_CLIENT[\\\"get_client()<br/>Client Factory\\\"]\\n        TRANSPORT_DETECT[\\\"Transport Detection<br/>ASGI vs HTTP\\\"]\\n    end\\n    \\n    subgraph \\\"HTTP Client Stack\\\"\\n        HTTPX_CLIENT[\\\"httpx.AsyncClient<br/>Connection Pool\\\"]\\n        HTTP_TRANSPORT[\\\"AsyncHTTPTransport<br/>Retry Logic\\\"]\\n        ASGI_TRANSPORT[\\\"ASGITransport<br/>Local Testing\\\"]\\n    end\\n    \\n    subgraph \\\"Request Processing\\\"\\n        TIMEOUT[\\\"Timeout Configuration<br/>connect/read/write/pool\\\"]\\n        HEADERS[\\\"Header Management<br/>API Key, User-Agent\\\"]\\n        RETRIES[\\\"Retry Logic<br/>5 retries default\\\"]\\n    end\\n    \\n    GET_CLIENT --> TRANSPORT_DETECT\\n    TRANSPORT_DETECT --> HTTPX_CLIENT\\n    HTTPX_CLIENT --> HTTP_TRANSPORT\\n    HTTPX_CLIENT --> ASGI_TRANSPORT\\n    \\n    HTTPX_CLIENT --> TIMEOUT\\n    HTTPX_CLIENT --> HEADERS\\n    HTTP_TRANSPORT --> RETRIES\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:192-221](), [libs/sdk-py/langgraph_sdk/client.py:261-412]()\\n\\n### Error Handling Strategy\\n\\nThe HTTP client implements comprehensive error handling with enhanced debugging information:\\n\\n| Error Type | Handling Strategy | Implementation |\\n|------------|------------------|----------------|\\n| **HTTP Status Errors** | Enhanced error messages with response body | `HttpClient.get/post/put/patch` methods |\\n| **Connection Errors** | Retry logic with exponential backoff | `AsyncHTTPTransport(retries=5)` |\\n| **Serialization Errors** | Graceful fallback and type handling | `_orjson_default()` function |\\n| **Authentication Errors** | Clear 401/403 error reporting | `Auth.exceptions.HTTPException` |\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:287-295](), [libs/sdk-py/langgraph_sdk/client.py:319-328](), [libs/sdk-py/langgraph_sdk/auth/exceptions.py:9-58]()\\n\\n## SDK Structure and Patterns\\n\\n### Async/Sync API Parity\\n\\nThe SDK maintains strict parity between asynchronous and synchronous APIs, ensuring consistent interfaces across both programming models:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Async API\\\"\\n        ASYNC_CLIENT[\\\"LangGraphClient<br/>Async Methods\\\"]\\n        ASYNC_ASSISTANTS[\\\"AssistantsClient\\\"]\\n        ASYNC_THREADS[\\\"ThreadsClient\\\"]\\n        ASYNC_RUNS[\\\"RunsClient\\\"]\\n    end\\n    \\n    subgraph \\\"Sync API\\\"\\n        SYNC_CLIENT[\\\"SyncLangGraphClient<br/>Sync Methods\\\"]\\n        SYNC_ASSISTANTS[\\\"SyncAssistantsClient\\\"]\\n        SYNC_THREADS[\\\"SyncThreadsClient\\\"]\\n        SYNC_RUNS[\\\"SyncRunsClient\\\"]\\n    end\\n    \\n    subgraph \\\"Shared Components\\\"\\n        SCHEMA[\\\"Common Schema Types<br/>Assistant, Thread, Run\\\"]\\n        HTTP_LAYER[\\\"HTTP Transport Layer\\\"]\\n        AUTH_SYSTEM[\\\"Authentication System\\\"]\\n    end\\n    \\n    ASYNC_CLIENT --> ASYNC_ASSISTANTS\\n    ASYNC_CLIENT --> ASYNC_THREADS\\n    ASYNC_CLIENT --> ASYNC_RUNS\\n    \\n    SYNC_CLIENT --> SYNC_ASSISTANTS\\n    SYNC_CLIENT --> SYNC_THREADS\\n    SYNC_CLIENT --> SYNC_RUNS\\n    \\n    ASYNC_ASSISTANTS --> SCHEMA\\n    SYNC_ASSISTANTS --> SCHEMA\\n    \\n    ASYNC_CLIENT --> HTTP_LAYER\\n    SYNC_CLIENT --> HTTP_LAYER\\n    \\n    HTTP_LAYER --> AUTH_SYSTEM\\n```\\n\\nSources: [libs/sdk-py/tests/test_api_parity.py:51-121](), [libs/sdk-py/langgraph_sdk/client.py:223-241]()\\n\\n### Type System Integration\\n\\nThe SDK leverages Python's type system extensively to provide IDE support and runtime validation:\\n\\n| Component | Type Strategy | Key Types |\\n|-----------|---------------|-----------|\\n| **Schema Definitions** | TypedDict for API models | `Assistant`, `Thread`, `Run`, `Config` |\\n| **Client Methods** | Generic type parameters | `QueryParamTypes`, `Json` |\\n| **Authentication** | Protocol-based user types | `MinimalUser`, `BaseUser`, `AuthContext` |\\n| **Streaming** | AsyncIterator for real-time data | `StreamPart`, `AsyncIterator[StreamPart]` |\\n\\nSources: [libs/sdk-py/langgraph_sdk/schema.py:1-556](), [libs/sdk-py/langgraph_sdk/auth/types.py:140-346]()\", \"# Page: RemoteGraph Client\\n\\n# RemoteGraph Client\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/langgraph/langgraph/pregel/remote.py](libs/langgraph/langgraph/pregel/remote.py)\\n- [libs/langgraph/tests/test_remote_graph.py](libs/langgraph/tests/test_remote_graph.py)\\n\\n</details>\\n\\n\\n\\nThe `RemoteGraph` class provides a client implementation for executing LangGraph applications on remote servers while maintaining the same interface as local graphs. It enables seamless integration with LangGraph Platform deployments and other remote LangGraph Server implementations.\\n\\nFor information about the underlying SDK clients, see [Python SDK](#6.2). For deployment and server architecture, see [LangGraph Platform](#7.2).\\n\\n## Purpose and Scope\\n\\nThe `RemoteGraph` client serves as a transparent proxy that:\\n\\n- Executes graphs on remote LangGraph servers via HTTP API calls\\n- Implements the `PregelProtocol` interface for local/remote compatibility\\n- Supports all standard graph operations (streaming, state management, introspection)\\n- Enables composition as nodes within other graphs\\n- Handles configuration sanitization and distributed tracing\\n\\n## Architecture Overview\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Local Application\\\"\\n        RG[\\\"RemoteGraph\\\"]\\n        LocalGraph[\\\"StateGraph\\\"]\\n        Config[\\\"RunnableConfig\\\"]\\n    end\\n    \\n    subgraph \\\"SDK Layer\\\"\\n        AsyncClient[\\\"LangGraphClient\\\"]\\n        SyncClient[\\\"SyncLangGraphClient\\\"]\\n    end\\n    \\n    subgraph \\\"Remote Server\\\"\\n        API[\\\"LangGraph Server API\\\"]\\n        GraphEngine[\\\"Pregel Engine\\\"]\\n        Persistence[\\\"Checkpoint Store\\\"]\\n    end\\n    \\n    RG --> AsyncClient\\n    RG --> SyncClient\\n    LocalGraph --> RG\\n    Config --> RG\\n    \\n    AsyncClient --> API\\n    SyncClient --> API\\n    API --> GraphEngine\\n    GraphEngine --> Persistence\\n    \\n    RG -.->|\\\"implements\\\"| PregelProtocol[\\\"PregelProtocol\\\"]\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:104-113](), [libs/langgraph/langgraph/pregel/remote.py:22-27]()\\n\\n## Core Components\\n\\nThe `RemoteGraph` class integrates several key components to provide remote execution capabilities:\\n\\n| Component | Type | Purpose |\\n|-----------|------|---------|\\n| `assistant_id` | `str` | Identifies the remote graph/assistant to execute |\\n| `client` | `LangGraphClient` | Async HTTP client for API calls |\\n| `sync_client` | `SyncLangGraphClient` | Sync HTTP client for API calls |\\n| `config` | `RunnableConfig` | Default configuration for executions |\\n| `distributed_tracing` | `bool` | Enables LangSmith tracing headers |\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"RemoteGraph Initialization\\\"\\n        Init[\\\"__init__\\\"]\\n        ValidateClient[\\\"_validate_client\\\"]\\n        ValidateSyncClient[\\\"_validate_sync_client\\\"]\\n    end\\n    \\n    subgraph \\\"Client Management\\\"\\n        GetClient[\\\"get_client\\\"]\\n        GetSyncClient[\\\"get_sync_client\\\"]\\n        AsyncClient[\\\"LangGraphClient\\\"]\\n        SyncClient[\\\"SyncLangGraphClient\\\"]\\n    end\\n    \\n    Init --> GetClient\\n    Init --> GetSyncClient\\n    GetClient --> AsyncClient\\n    GetSyncClient --> SyncClient\\n    ValidateClient --> AsyncClient\\n    ValidateSyncClient --> SyncClient\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:118-166](), [libs/langgraph/langgraph/pregel/remote.py:167-179]()\\n\\n## Initialization and Configuration\\n\\n### Basic Initialization\\n\\nThe `RemoteGraph` constructor accepts multiple initialization patterns:\\n\\n```python\\n# URL-based initialization (creates default clients)\\nremote = RemoteGraph(\\\"assistant-id\\\", url=\\\"https://api.langgraph.com\\\", api_key=\\\"key\\\")\\n\\n# Custom client initialization  \\nremote = RemoteGraph(\\\"assistant-id\\\", client=custom_client)\\n\\n# Mixed initialization\\nremote = RemoteGraph(\\\"assistant-id\\\", url=\\\"...\\\", sync_client=custom_sync_client)\\n```\\n\\n### Configuration Management\\n\\nThe `with_config` method enables configuration chaining and merging:\\n\\n```mermaid\\ngraph TD\\n    OriginalConfig[\\\"Original Config\\\"]\\n    NewConfig[\\\"New Config\\\"] \\n    MergeConfigs[\\\"merge_configs\\\"]\\n    SanitizedConfig[\\\"Sanitized Config\\\"]\\n    \\n    OriginalConfig --> MergeConfigs\\n    NewConfig --> MergeConfigs\\n    MergeConfigs --> SanitizeConfig[\\\"_sanitize_config\\\"]\\n    SanitizeConfig --> SanitizedConfig\\n    \\n    subgraph \\\"Sanitization Process\\\"\\n        DropFields[\\\"Drop Internal Fields\\\"]\\n        ValidateTypes[\\\"Validate Primitive Types\\\"]\\n        RecursiveClean[\\\"Recursive Cleanup\\\"]\\n    end\\n    \\n    SanitizeConfig --> DropFields\\n    DropFields --> ValidateTypes\\n    ValidateTypes --> RecursiveClean\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:185-188](), [libs/langgraph/langgraph/pregel/remote.py:365-392](), [libs/langgraph/langgraph/pregel/remote.py:76-95]()\\n\\n## Graph Introspection\\n\\nThe `get_graph` and `aget_graph` methods retrieve remote graph structure:\\n\\n| Method | API Endpoint | Return Type | Purpose |\\n|--------|--------------|-------------|---------|\\n| `get_graph` | `GET /assistants/{assistant_id}/graph` | `DrawableGraph` | Sync graph retrieval |\\n| `aget_graph` | `GET /assistants/{assistant_id}/graph` | `DrawableGraph` | Async graph retrieval |\\n\\n```mermaid\\ngraph LR\\n    GetGraph[\\\"get_graph/aget_graph\\\"]\\n    APICall[\\\"GET /assistants/{id}/graph\\\"]\\n    ParseNodes[\\\"_get_drawable_nodes\\\"]\\n    DrawableGraph[\\\"DrawableGraph\\\"]\\n    \\n    GetGraph --> APICall\\n    APICall --> ParseNodes\\n    ParseNodes --> DrawableGraph\\n    \\n    subgraph \\\"Node Processing\\\"\\n        RawNodes[\\\"Raw Node Data\\\"]\\n        NodeMetadata[\\\"Extract Metadata\\\"]\\n        DrawableNode[\\\"DrawableNode\\\"]\\n    end\\n    \\n    ParseNodes --> RawNodes\\n    RawNodes --> NodeMetadata\\n    NodeMetadata --> DrawableNode\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:214-278](), [libs/langgraph/langgraph/pregel/remote.py:190-212]()\\n\\n## State Management\\n\\n### State Retrieval and Updates\\n\\nThe `RemoteGraph` provides comprehensive state management capabilities:\\n\\n| Operation | Sync Method | Async Method | API Endpoint |\\n|-----------|-------------|--------------|--------------|\\n| Get State | `get_state` | `aget_state` | `GET /threads/{thread_id}/state` |\\n| Update State | `update_state` | `aupdate_state` | `POST /threads/{thread_id}/state` |\\n| Get History | `get_state_history` | `aget_state_history` | `POST /threads/{thread_id}/history` |\\n\\n### State Snapshot Conversion\\n\\n```mermaid\\ngraph TD\\n    ThreadState[\\\"ThreadState (SDK)\\\"]\\n    CreateSnapshot[\\\"_create_state_snapshot\\\"]\\n    StateSnapshot[\\\"StateSnapshot (Core)\\\"]\\n    \\n    subgraph \\\"Conversion Process\\\"\\n        ProcessTasks[\\\"Process Tasks\\\"]\\n        ProcessCheckpoint[\\\"Process Checkpoint\\\"]\\n        ProcessMetadata[\\\"Process Metadata\\\"]\\n        ProcessInterrupts[\\\"Process Interrupts\\\"]\\n    end\\n    \\n    ThreadState --> CreateSnapshot\\n    CreateSnapshot --> ProcessTasks\\n    CreateSnapshot --> ProcessCheckpoint\\n    CreateSnapshot --> ProcessMetadata\\n    CreateSnapshot --> ProcessInterrupts\\n    \\n    ProcessTasks --> StateSnapshot\\n    ProcessCheckpoint --> StateSnapshot\\n    ProcessMetadata --> StateSnapshot\\n    ProcessInterrupts --> StateSnapshot\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:280-336](), [libs/langgraph/langgraph/pregel/remote.py:394-464](), [libs/langgraph/langgraph/pregel/remote.py:560-628]()\\n\\n## Stream Processing and Execution\\n\\n### Stream Mode Handling\\n\\nThe `_get_stream_modes` method normalizes and processes stream mode configurations:\\n\\n```mermaid\\ngraph TD\\n    InputModes[\\\"Input Stream Modes\\\"]\\n    ProcessModes[\\\"_get_stream_modes\\\"]\\n    \\n    subgraph \\\"Stream Mode Processing\\\"\\n        CoerceList[\\\"Coerce to List\\\"]\\n        AddParentModes[\\\"Add Parent Stream Modes\\\"]\\n        MapMessageModes[\\\"Map 'messages' to 'messages-tuple'\\\"]\\n        AddUpdates[\\\"Ensure 'updates' Mode\\\"]\\n        RemoveEvents[\\\"Remove 'events' Mode\\\"]\\n    end\\n    \\n    subgraph \\\"Output\\\"\\n        FinalModes[\\\"Final Stream Modes\\\"]\\n        RequestedModes[\\\"Requested Modes\\\"]\\n        SingleMode[\\\"Single Mode Flag\\\"]\\n        ParentStream[\\\"Parent Stream Protocol\\\"]\\n    end\\n    \\n    InputModes --> ProcessModes\\n    ProcessModes --> CoerceList\\n    CoerceList --> AddParentModes\\n    AddParentModes --> MapMessageModes\\n    MapMessageModes --> AddUpdates\\n    AddUpdates --> RemoveEvents\\n    \\n    RemoveEvents --> FinalModes\\n    RemoveEvents --> RequestedModes\\n    RemoveEvents --> SingleMode\\n    RemoveEvents --> ParentStream\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:630-679]()\\n\\n### Execution Flow\\n\\nThe streaming execution handles multiple concerns including error propagation and interrupt detection:\\n\\n```mermaid\\ngraph TD\\n    StreamCall[\\\"stream/astream\\\"]\\n    SDKStream[\\\"client.runs.stream\\\"]\\n    ProcessChunk[\\\"Process Chunk\\\"]\\n    \\n    subgraph \\\"Chunk Processing\\\"\\n        SplitEvent[\\\"Split Event and Namespace\\\"]\\n        HandleCommand[\\\"Handle Parent Commands\\\"]\\n        CheckInterrupt[\\\"Check for Interrupts\\\"]\\n        FilterModes[\\\"Filter Requested Modes\\\"]\\n        FormatOutput[\\\"Format Output\\\"]\\n    end\\n    \\n    subgraph \\\"Error Handling\\\"\\n        RemoteException[\\\"RemoteException\\\"]\\n        GraphInterrupt[\\\"GraphInterrupt\\\"]\\n        ParentCommand[\\\"ParentCommand\\\"]\\n    end\\n    \\n    StreamCall --> SDKStream\\n    SDKStream --> ProcessChunk\\n    ProcessChunk --> SplitEvent\\n    SplitEvent --> HandleCommand\\n    HandleCommand --> CheckInterrupt\\n    CheckInterrupt --> FilterModes\\n    FilterModes --> FormatOutput\\n    \\n    HandleCommand -.-> ParentCommand\\n    CheckInterrupt -.-> GraphInterrupt\\n    ProcessChunk -.-> RemoteException\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:681-897](), [libs/langgraph/langgraph/pregel/remote.py:742-788]()\\n\\n## Error Handling and Integration\\n\\n### Exception Types\\n\\nThe `RemoteGraph` defines specific exception handling:\\n\\n| Exception | Purpose | Source |\\n|-----------|---------|---------|\\n| `RemoteException` | Remote execution errors | [libs/langgraph/langgraph/pregel/remote.py:98-101]() |\\n| `GraphInterrupt` | Remote graph interrupts | [libs/langgraph/langgraph/errors.py]() |\\n| `ParentCommand` | Command propagation | [libs/langgraph/langgraph/errors.py]() |\\n\\n### Distributed Tracing Integration\\n\\nWhen `distributed_tracing=True`, the client automatically includes LangSmith tracing headers:\\n\\n```mermaid\\ngraph LR\\n    TracingEnabled[\\\"distributed_tracing=True\\\"]\\n    GetRunTree[\\\"ls.get_current_run_tree()\\\"]\\n    MergeHeaders[\\\"_merge_tracing_headers\\\"]\\n    HTTPRequest[\\\"HTTP Request with Trace Headers\\\"]\\n    \\n    TracingEnabled --> GetRunTree\\n    GetRunTree --> MergeHeaders\\n    MergeHeaders --> HTTPRequest\\n    \\n    subgraph \\\"Tracing Headers\\\"\\n        LangsmithTrace[\\\"langsmith-trace\\\"]\\n        Baggage[\\\"baggage\\\"]\\n    end\\n    \\n    MergeHeaders --> LangsmithTrace\\n    MergeHeaders --> Baggage\\n```\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:996-1007](), [libs/langgraph/langgraph/pregel/remote.py:737-738](), [libs/langgraph/langgraph/pregel/remote.py:845-846]()\\n\\n## Protocol Compliance\\n\\nThe `RemoteGraph` implements the `PregelProtocol` interface, ensuring API compatibility with local graphs. This enables transparent substitution in graph compositions and provides a consistent developer experience across local and remote execution environments.\\n\\nSources: [libs/langgraph/langgraph/pregel/remote.py:104](), [libs/langgraph/langgraph/pregel/protocol.py]()\", \"# Page: Python SDK\\n\\n# Python SDK\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/reference/sdk/python_sdk_ref.md](docs/docs/cloud/reference/sdk/python_sdk_ref.md)\\n- [libs/checkpoint/tests/test_redis_cache.py](libs/checkpoint/tests/test_redis_cache.py)\\n- [libs/sdk-py/Makefile](libs/sdk-py/Makefile)\\n- [libs/sdk-py/langgraph_sdk/__init__.py](libs/sdk-py/langgraph_sdk/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/__init__.py](libs/sdk-py/langgraph_sdk/auth/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/exceptions.py](libs/sdk-py/langgraph_sdk/auth/exceptions.py)\\n- [libs/sdk-py/langgraph_sdk/auth/types.py](libs/sdk-py/langgraph_sdk/auth/types.py)\\n- [libs/sdk-py/langgraph_sdk/client.py](libs/sdk-py/langgraph_sdk/client.py)\\n- [libs/sdk-py/langgraph_sdk/schema.py](libs/sdk-py/langgraph_sdk/schema.py)\\n- [libs/sdk-py/langgraph_sdk/sse.py](libs/sdk-py/langgraph_sdk/sse.py)\\n- [libs/sdk-py/pyproject.toml](libs/sdk-py/pyproject.toml)\\n- [libs/sdk-py/tests/test_api_parity.py](libs/sdk-py/tests/test_api_parity.py)\\n- [libs/sdk-py/uv.lock](libs/sdk-py/uv.lock)\\n\\n</details>\\n\\n\\n\\nThe LangGraph Python SDK provides a comprehensive client library for interacting with the LangGraph API from Python applications. It offers both asynchronous and synchronous interfaces for managing assistants, threads, runs, cron jobs, and persistent storage, along with built-in authentication and authorization capabilities.\\n\\nFor information about the RemoteGraph client for executing graphs on remote servers, see [RemoteGraph Client](#6.1). For deployment and platform management, see [LangGraph Platform](#7.2).\\n\\n## SDK Architecture Overview\\n\\nThe Python SDK follows a modular architecture with separate client classes for each major API resource:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Client Factory\\\"\\n        get_client[\\\"get_client()\\\"]\\n        get_sync_client[\\\"get_sync_client()\\\"]\\n    end\\n    \\n    subgraph \\\"Main Clients\\\"\\n        LangGraphClient[\\\"LangGraphClient\\\"]\\n        SyncLangGraphClient[\\\"SyncLangGraphClient\\\"]\\n    end\\n    \\n    subgraph \\\"Resource Clients\\\"\\n        AssistantsClient[\\\"AssistantsClient\\\"]\\n        ThreadsClient[\\\"ThreadsClient\\\"] \\n        RunsClient[\\\"RunsClient\\\"]\\n        CronClient[\\\"CronClient\\\"]\\n        StoreClient[\\\"StoreClient\\\"]\\n    end\\n    \\n    subgraph \\\"Sync Resource Clients\\\"\\n        SyncAssistantsClient[\\\"SyncAssistantsClient\\\"]\\n        SyncThreadsClient[\\\"SyncThreadsClient\\\"]\\n        SyncRunsClient[\\\"SyncRunsClient\\\"]\\n        SyncCronClient[\\\"SyncCronClient\\\"]\\n        SyncStoreClient[\\\"SyncStoreClient\\\"]\\n    end\\n    \\n    subgraph \\\"Core Infrastructure\\\"\\n        HttpClient[\\\"HttpClient\\\"]\\n        SSEDecoder[\\\"SSEDecoder\\\"]\\n        Auth[\\\"Auth\\\"]\\n    end\\n    \\n    get_client --> LangGraphClient\\n    get_sync_client --> SyncLangGraphClient\\n    \\n    LangGraphClient --> AssistantsClient\\n    LangGraphClient --> ThreadsClient\\n    LangGraphClient --> RunsClient\\n    LangGraphClient --> CronClient\\n    LangGraphClient --> StoreClient\\n    \\n    SyncLangGraphClient --> SyncAssistantsClient\\n    SyncLangGraphClient --> SyncThreadsClient\\n    SyncLangGraphClient --> SyncRunsClient\\n    SyncLangGraphClient --> SyncCronClient\\n    SyncLangGraphClient --> SyncStoreClient\\n    \\n    AssistantsClient --> HttpClient\\n    ThreadsClient --> HttpClient\\n    RunsClient --> HttpClient\\n    CronClient --> HttpClient\\n    StoreClient --> HttpClient\\n    \\n    HttpClient --> SSEDecoder\\n    LangGraphClient --> Auth\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:1-260](), [libs/sdk-py/langgraph_sdk/__init__.py:1-7]()\\n\\n## Core Client Classes\\n\\n### LangGraphClient\\n\\nThe `LangGraphClient` serves as the main entry point for all API interactions. It aggregates specialized client instances for different resource types:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"LangGraphClient Structure\\\"\\n        LGC[\\\"LangGraphClient\\\"]\\n        HTTP[\\\"HttpClient\\\"]\\n        \\n        subgraph \\\"Resource Clients\\\"\\n            AC[\\\"client.assistants<br/>AssistantsClient\\\"]\\n            TC[\\\"client.threads<br/>ThreadsClient\\\"] \\n            RC[\\\"client.runs<br/>RunsClient\\\"]\\n            CC[\\\"client.crons<br/>CronClient\\\"]\\n            SC[\\\"client.store<br/>StoreClient\\\"]\\n        end\\n    end\\n    \\n    LGC --> HTTP\\n    LGC --> AC\\n    LGC --> TC \\n    LGC --> RC\\n    LGC --> CC\\n    LGC --> SC\\n    \\n    AC --> HTTP\\n    TC --> HTTP\\n    RC --> HTTP\\n    CC --> HTTP\\n    SC --> HTTP\\n```\\n\\nThe client provides context manager support for proper resource cleanup:\\n\\n| Method | Purpose |\\n|--------|---------|\\n| `__aenter__()` | Enter async context |\\n| `__aexit__()` | Exit async context with cleanup |\\n| `aclose()` | Manually close HTTP connections |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:223-260]()\\n\\n### HttpClient Infrastructure\\n\\nThe `HttpClient` class handles all HTTP communication with the LangGraph API, providing unified error handling and content processing:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"HttpClient Methods\\\"\\n        GET[\\\"get(path, params, headers)\\\"]\\n        POST[\\\"post(path, json, params, headers)\\\"]\\n        PUT[\\\"put(path, json, params, headers)\\\"]\\n        PATCH[\\\"patch(path, json, params, headers)\\\"]\\n        DELETE[\\\"delete(path, json, params, headers)\\\"]\\n        STREAM[\\\"stream(path, method, json, params, headers)\\\"]\\n    end\\n    \\n    subgraph \\\"Processing Pipeline\\\"\\n        REQUEST[\\\"HTTP Request\\\"]\\n        RESPONSE[\\\"HTTP Response\\\"]\\n        DECODE[\\\"JSON Decode\\\"]\\n        ERROR[\\\"Error Handling\\\"]\\n        RESULT[\\\"Result\\\"]\\n    end\\n    \\n    GET --> REQUEST\\n    POST --> REQUEST\\n    PUT --> REQUEST\\n    PATCH --> REQUEST\\n    DELETE --> REQUEST\\n    STREAM --> REQUEST\\n    \\n    REQUEST --> RESPONSE\\n    RESPONSE --> ERROR\\n    ERROR --> DECODE\\n    DECODE --> RESULT\\n    \\n    STREAM --> SSE[\\\"SSE Processing\\\"]\\n    SSE --> StreamPart[\\\"StreamPart Events\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:261-459]()\\n\\n## Resource Management Clients\\n\\n### AssistantsClient Operations\\n\\nThe `AssistantsClient` manages versioned configurations for graphs:\\n\\n| Operation | Method | Purpose |\\n|-----------|--------|---------|\\n| Retrieve | `get(assistant_id)` | Get assistant by ID |\\n| Create | `create(graph_id, config, context, metadata)` | Create new assistant |\\n| Update | `update(assistant_id, graph_id, config, context)` | Update existing assistant |\\n| Delete | `delete(assistant_id)` | Remove assistant |\\n| Search | `search(metadata, graph_id, limit, offset)` | Find assistants by criteria |\\n| Count | `count(metadata, graph_id)` | Count matching assistants |\\n| Graph Info | `get_graph(assistant_id, xray)` | Get graph structure |\\n| Schema | `get_schemas(assistant_id)` | Get graph schemas |\\n| Versions | `get_versions(assistant_id)` | List assistant versions |\\n| Subgraphs | `get_subgraphs(assistant_id, namespace, recurse)` | Get subgraph information |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:486-1119]()\\n\\n### ThreadsClient Operations\\n\\nThe `ThreadsClient` handles conversational thread management:\\n\\n| Operation | Method | Purpose |\\n|-----------|--------|---------|\\n| Retrieve | `get(thread_id)` | Get thread by ID |\\n| Create | `create(metadata, thread_id, if_exists)` | Create new thread |\\n| Update | `update(thread_id, metadata)` | Update thread metadata |\\n| Delete | `delete(thread_id)` | Remove thread |\\n| Search | `search(metadata, values, status)` | Find threads by criteria |\\n| State Management | `get_state(thread_id, checkpoint)` | Get thread state |\\n| State Update | `update_state(thread_id, values, as_node)` | Update thread state |\\n| History | `get_history(thread_id, limit, before)` | Get state history |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:1120-1785]()\\n\\n### RunsClient Operations\\n\\nThe `RunsClient` controls individual graph executions:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Run Lifecycle Management\\\"\\n        CREATE[\\\"create()<br/>Start new run\\\"]\\n        STREAM[\\\"stream()<br/>Stream execution\\\"]\\n        WAIT[\\\"wait()<br/>Wait for completion\\\"]\\n        GET[\\\"get()<br/>Get run status\\\"]\\n        CANCEL[\\\"cancel()<br/>Cancel execution\\\"]\\n        DELETE[\\\"delete()<br/>Remove run\\\"]\\n    end\\n    \\n    subgraph \\\"Run Control\\\"\\n        INTERRUPT[\\\"join()<br/>Handle interrupts\\\"]\\n        RESUME[\\\"resume()<br/>Resume from interrupt\\\"]\\n    end\\n    \\n    subgraph \\\"Monitoring\\\"\\n        SEARCH[\\\"search()<br/>Find runs\\\"]\\n        COUNT[\\\"count()<br/>Count runs\\\"]\\n    end\\n    \\n    CREATE --> STREAM\\n    STREAM --> WAIT\\n    STREAM --> INTERRUPT\\n    INTERRUPT --> RESUME\\n    RESUME --> STREAM\\n    WAIT --> GET\\n    GET --> CANCEL\\n    CANCEL --> DELETE\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:1786-2728]()\\n\\n## Authentication System\\n\\nThe SDK includes a comprehensive authentication and authorization framework through the `Auth` class:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Auth Class Structure\\\"\\n        AUTH[\\\"Auth()\\\"]\\n        AUTHENTICATE[\\\"@auth.authenticate\\\"]\\n        ON[\\\"@auth.on\\\"]\\n    end\\n    \\n    subgraph \\\"Authentication Handler\\\"\\n        AUTH_FUNC[\\\"authenticate(authorization, headers, ...)\\\"]\\n        USER_RESULT[\\\"MinimalUserDict | str | BaseUser\\\"]\\n    end\\n    \\n    subgraph \\\"Authorization Handlers\\\"\\n        GLOBAL[\\\"@auth.on<br/>Global handler\\\"]\\n        RESOURCE[\\\"@auth.on.threads<br/>Resource handler\\\"]\\n        ACTION[\\\"@auth.on.threads.create<br/>Action handler\\\"]\\n        STORE[\\\"@auth.on.store<br/>Store handler\\\"]\\n    end\\n    \\n    subgraph \\\"Handler Resolution\\\"\\n        REQUEST[\\\"Incoming Request\\\"]\\n        AUTH_CHECK[\\\"Authentication\\\"]\\n        AUTHZ_CHECK[\\\"Authorization\\\"]\\n        RESPONSE[\\\"Allow/Deny/Filter\\\"]\\n    end\\n    \\n    AUTH --> AUTHENTICATE\\n    AUTH --> ON\\n    \\n    AUTHENTICATE --> AUTH_FUNC\\n    AUTH_FUNC --> USER_RESULT\\n    \\n    ON --> GLOBAL\\n    ON --> RESOURCE\\n    ON --> ACTION\\n    ON --> STORE\\n    \\n    REQUEST --> AUTH_CHECK\\n    AUTH_CHECK --> AUTHZ_CHECK\\n    AUTHZ_CHECK --> RESPONSE\\n```\\n\\n### Authentication Configuration\\n\\nThe authentication handler is registered using the `@auth.authenticate` decorator and can accept various request parameters:\\n\\n| Parameter | Type | Description |\\n|-----------|------|-------------|\\n| `request` | `Request` | Raw ASGI request object |\\n| `body` | `dict` | Parsed request body |\\n| `path` | `str` | Request path |\\n| `method` | `str` | HTTP method |\\n| `path_params` | `dict[str, str]` | URL path parameters |\\n| `query_params` | `dict[str, str]` | URL query parameters |\\n| `headers` | `dict[str, bytes]` | Request headers |\\n| `authorization` | `str \\\\| None` | Authorization header value |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:13-255](), [libs/sdk-py/langgraph_sdk/auth/types.py:1-1050]()\\n\\n### Authorization Handler Types\\n\\nThe authorization system supports fine-grained access control with different handler types:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Handler Hierarchy\\\"\\n        SPECIFIC[\\\"@auth.on.threads.create<br/>Most Specific\\\"]\\n        RESOURCE[\\\"@auth.on.threads<br/>Resource Level\\\"]\\n        GLOBAL[\\\"@auth.on<br/>Global Fallback\\\"]\\n    end\\n    \\n    REQUEST[\\\"Request\\\"] --> SPECIFIC\\n    SPECIFIC -->|\\\"Not Found\\\"| RESOURCE\\n    RESOURCE -->|\\\"Not Found\\\"| GLOBAL\\n    GLOBAL -->|\\\"Not Found\\\"| ACCEPT[\\\"Accept Request\\\"]\\n    \\n    SPECIFIC --> RESULT[\\\"Handler Result\\\"]\\n    RESOURCE --> RESULT\\n    GLOBAL --> RESULT\\n    \\n    RESULT --> ALLOW[\\\"None/True: Allow\\\"]\\n    RESULT --> DENY[\\\"False: 403 Error\\\"]\\n    RESULT --> FILTER[\\\"FilterType: Apply Filter\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:110-173](), [libs/sdk-py/langgraph_sdk/auth/types.py:60-125]()\\n\\n## Data Models and Schemas\\n\\nThe SDK defines comprehensive data models for all API resources using TypedDict classes:\\n\\n### Core Resource Models\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Assistant Models\\\"\\n        Assistant[\\\"Assistant<br/>graph_id, config, context, metadata\\\"]\\n        AssistantVersion[\\\"AssistantVersion<br/>versioned assistant data\\\"]\\n        GraphSchema[\\\"GraphSchema<br/>input/output/state schemas\\\"]\\n    end\\n    \\n    subgraph \\\"Thread Models\\\"\\n        Thread[\\\"Thread<br/>thread_id, status, values, interrupts\\\"]\\n        ThreadState[\\\"ThreadState<br/>values, next, checkpoint, tasks\\\"]\\n        ThreadTask[\\\"ThreadTask<br/>id, name, error, interrupts\\\"]\\n    end\\n    \\n    subgraph \\\"Run Models\\\"\\n        Run[\\\"Run<br/>run_id, thread_id, assistant_id, status\\\"]\\n        RunCreate[\\\"RunCreate<br/>creation parameters\\\"]\\n        RunStatus[\\\"RunStatus<br/>pending|running|error|success|timeout|interrupted\\\"]\\n    end\\n    \\n    subgraph \\\"Store Models\\\"\\n        Item[\\\"Item<br/>namespace, key, value, timestamps\\\"]\\n        SearchItem[\\\"SearchItem<br/>Item + optional score\\\"]\\n        SearchItemsResponse[\\\"SearchItemsResponse<br/>items list\\\"]\\n    end\\n    \\n    subgraph \\\"Control Models\\\"\\n        Command[\\\"Command<br/>goto, update, resume\\\"]\\n        Send[\\\"Send<br/>node, input\\\"]\\n        Checkpoint[\\\"Checkpoint<br/>thread_id, checkpoint_ns, checkpoint_id\\\"]\\n    end\\n```\\n\\n### Stream Processing Models\\n\\nFor real-time communication, the SDK uses Server-Sent Events (SSE):\\n\\n| Model | Purpose |\\n|-------|---------|\\n| `StreamPart` | Individual SSE event with event type and data |\\n| `StreamMode` | Streaming mode: values, messages, updates, events, etc. |\\n| `DisconnectMode` | Behavior on disconnection: cancel or continue |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/schema.py:1-556]()\\n\\n## Streaming Capabilities\\n\\nThe SDK provides sophisticated streaming support through the SSE (Server-Sent Events) infrastructure:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"SSE Processing Pipeline\\\"\\n        REQUEST[\\\"HTTP Stream Request\\\"]\\n        RESPONSE[\\\"HTTP Response Stream\\\"]\\n        DECODER[\\\"BytesLineDecoder\\\"]\\n        SSE[\\\"SSEDecoder\\\"]\\n        EVENTS[\\\"StreamPart Events\\\"]\\n    end\\n    \\n    subgraph \\\"Stream Modes\\\"\\n        VALUES[\\\"values: State values\\\"]\\n        MESSAGES[\\\"messages: Complete messages\\\"]\\n        UPDATES[\\\"updates: State updates\\\"]\\n        EVENTS_MODE[\\\"events: Execution events\\\"]\\n        TASKS[\\\"tasks: Task start/finish\\\"]\\n        CHECKPOINTS[\\\"checkpoints: State snapshots\\\"]\\n        DEBUG[\\\"debug: Debug information\\\"]\\n        CUSTOM[\\\"custom: Custom events\\\"]\\n    end\\n    \\n    REQUEST --> RESPONSE\\n    RESPONSE --> DECODER\\n    DECODER --> SSE\\n    SSE --> EVENTS\\n    \\n    EVENTS --> VALUES\\n    EVENTS --> MESSAGES\\n    EVENTS --> UPDATES\\n    EVENTS --> EVENTS_MODE\\n    EVENTS --> TASKS\\n    EVENTS --> CHECKPOINTS\\n    EVENTS --> DEBUG\\n    EVENTS --> CUSTOM\\n```\\n\\n### SSE Decoder Implementation\\n\\nThe `SSEDecoder` class handles the parsing of Server-Sent Events according to the HTML5 specification:\\n\\n| Field | Purpose |\\n|-------|---------|\\n| `event` | Event type identifier |\\n| `data` | JSON payload data |\\n| `id` | Event ID for resumption |\\n| `retry` | Reconnection timeout |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/sse.py:1-152]()\\n\\n## API Configuration and Transport\\n\\nThe SDK supports multiple transport configurations and connection methods:\\n\\n### Connection Factory\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Client Configuration\\\"\\n        get_client[\\\"get_client(url, api_key, headers, timeout)\\\"]\\n        CONFIG[\\\"Configuration Options\\\"]\\n    end\\n    \\n    subgraph \\\"Transport Selection\\\"\\n        URL_CHECK{\\\"URL provided?\\\"}\\n        LOOPBACK_CHECK{\\\"Defer loopback?\\\"}\\n        API_CHECK{\\\"LangGraph API available?\\\"}\\n    end\\n    \\n    subgraph \\\"Transport Types\\\"\\n        ASGI[\\\"ASGITransport<br/>Direct API integration\\\"]\\n        HTTP[\\\"AsyncHTTPTransport<br/>HTTP client with retries\\\"]\\n        CUSTOM[\\\"Custom Transport\\\"]\\n    end\\n    \\n    get_client --> CONFIG\\n    CONFIG --> URL_CHECK\\n    \\n    URL_CHECK -->|No| LOOPBACK_CHECK\\n    LOOPBACK_CHECK -->|Yes| ASGI\\n    LOOPBACK_CHECK -->|No| API_CHECK\\n    API_CHECK -->|Available| ASGI\\n    API_CHECK -->|Not Available| HTTP\\n    \\n    URL_CHECK -->|Yes| HTTP\\n    \\n    ASGI --> LangGraphClient\\n    HTTP --> LangGraphClient\\n    CUSTOM --> LangGraphClient\\n```\\n\\n### API Key Management\\n\\nThe SDK supports multiple API key sources with a defined precedence order:\\n\\n1. Explicit `api_key` parameter\\n2. `LANGGRAPH_API_KEY` environment variable\\n3. `LANGSMITH_API_KEY` environment variable  \\n4. `LANGCHAIN_API_KEY` environment variable\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:84-220]()\\n\\n## Error Handling and HTTP Status Management\\n\\nThe SDK implements comprehensive error handling throughout the HTTP client stack:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Error Processing\\\"\\n        REQUEST[\\\"HTTP Request\\\"]\\n        RESPONSE[\\\"HTTP Response\\\"]\\n        STATUS_CHECK[\\\"Status Check\\\"]\\n        ERROR_HANDLER[\\\"Error Handler\\\"]\\n    end\\n    \\n    subgraph \\\"Error Types\\\"\\n        HTTP_ERROR[\\\"HTTPStatusError\\\"]\\n        TRANSPORT_ERROR[\\\"TransportError\\\"]\\n        CONTENT_ERROR[\\\"Content Type Error\\\"]\\n    end\\n    \\n    subgraph \\\"Error Enhancement\\\"\\n        BODY_READ[\\\"Read Error Body\\\"]\\n        NOTE_ADD[\\\"Add Error Note (Python 3.11+)\\\"]\\n        LOG_ERROR[\\\"Log Error (Python < 3.11)\\\"]\\n    end\\n    \\n    REQUEST --> RESPONSE\\n    RESPONSE --> STATUS_CHECK\\n    STATUS_CHECK -->|Error| ERROR_HANDLER\\n    \\n    ERROR_HANDLER --> HTTP_ERROR\\n    ERROR_HANDLER --> TRANSPORT_ERROR\\n    ERROR_HANDLER --> CONTENT_ERROR\\n    \\n    HTTP_ERROR --> BODY_READ\\n    BODY_READ --> NOTE_ADD\\n    BODY_READ --> LOG_ERROR\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:274-458]()\\n\\n## Package Dependencies and Build System\\n\\nThe SDK maintains minimal external dependencies for broad compatibility:\\n\\n| Dependency | Version | Purpose |\\n|------------|---------|---------|\\n| `httpx` | >=0.25.2 | HTTP client with async support |\\n| `orjson` | >=3.10.1 | Fast JSON serialization |\\n\\n### Development Dependencies\\n\\n| Tool | Purpose |\\n|------|---------|\\n| `ruff` | Linting and formatting |\\n| `pytest` | Testing framework |\\n| `pytest-asyncio` | Async test support |\\n| `pytest-mock` | Test mocking |\\n| `mypy` | Type checking |\\n| `codespell` | Spell checking |\\n\\n**Sources:** [libs/sdk-py/pyproject.toml:1-56](), [libs/sdk-py/uv.lock:1-350]()\", \"# Page: JavaScript SDK\\n\\n# JavaScript SDK\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/reference/sdk/python_sdk_ref.md](docs/docs/cloud/reference/sdk/python_sdk_ref.md)\\n- [libs/checkpoint/tests/test_redis_cache.py](libs/checkpoint/tests/test_redis_cache.py)\\n- [libs/sdk-py/Makefile](libs/sdk-py/Makefile)\\n- [libs/sdk-py/langgraph_sdk/__init__.py](libs/sdk-py/langgraph_sdk/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/__init__.py](libs/sdk-py/langgraph_sdk/auth/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/exceptions.py](libs/sdk-py/langgraph_sdk/auth/exceptions.py)\\n- [libs/sdk-py/langgraph_sdk/auth/types.py](libs/sdk-py/langgraph_sdk/auth/types.py)\\n- [libs/sdk-py/langgraph_sdk/client.py](libs/sdk-py/langgraph_sdk/client.py)\\n- [libs/sdk-py/langgraph_sdk/schema.py](libs/sdk-py/langgraph_sdk/schema.py)\\n- [libs/sdk-py/langgraph_sdk/sse.py](libs/sdk-py/langgraph_sdk/sse.py)\\n- [libs/sdk-py/pyproject.toml](libs/sdk-py/pyproject.toml)\\n- [libs/sdk-py/tests/test_api_parity.py](libs/sdk-py/tests/test_api_parity.py)\\n- [libs/sdk-py/uv.lock](libs/sdk-py/uv.lock)\\n\\n</details>\\n\\n\\n\\nThis document covers the JavaScript/TypeScript SDK for interacting with the LangGraph API. The JavaScript SDK provides similar capabilities to the Python SDK, enabling client applications to manage assistants, threads, runs, cron jobs, and persistent storage through a comprehensive API client.\\n\\nFor information about the Python SDK implementation, see [Python SDK](#6.2). For remote graph execution patterns, see [RemoteGraph Client](#6.1). For browser-specific UI components, see [React UI Components](#6.4).\\n\\n## SDK Architecture\\n\\nThe JavaScript SDK follows a similar architectural pattern to the Python implementation, providing both promise-based and streaming APIs for interacting with LangGraph services. The SDK is built around a main client class that delegates to specialized sub-clients for different resource types.\\n\\n### Client Structure Overview\\n\\n```mermaid\\ngraph TD\\n    LangGraphClient[\\\"LangGraphClient\\\"]\\n    HttpClient[\\\"HttpClient\\\"]\\n    \\n    AssistantsClient[\\\"AssistantsClient\\\"]\\n    ThreadsClient[\\\"ThreadsClient\\\"]\\n    RunsClient[\\\"RunsClient\\\"]\\n    CronClient[\\\"CronClient\\\"]\\n    StoreClient[\\\"StoreClient\\\"]\\n    \\n    LangGraphClient --> HttpClient\\n    LangGraphClient --> AssistantsClient\\n    LangGraphClient --> ThreadsClient\\n    LangGraphClient --> RunsClient\\n    LangGraphClient --> CronClient\\n    LangGraphClient --> StoreClient\\n    \\n    AssistantsClient --> HttpClient\\n    ThreadsClient --> HttpClient\\n    RunsClient --> HttpClient\\n    CronClient --> HttpClient\\n    StoreClient --> HttpClient\\n```\\n\\n**Client Component Architecture**\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:223-240](), [libs/sdk-py/langgraph_sdk/client.py:261-272]()\\n\\n## Core Client Classes\\n\\n### LangGraphClient\\n\\nThe main entry point for the JavaScript SDK provides access to all LangGraph API resources through specialized sub-clients. Like the Python implementation, it manages HTTP connections and provides consistent error handling across all operations.\\n\\n**Key Features:**\\n- Centralized API authentication and configuration\\n- Automatic request/response serialization\\n- Error handling with detailed error messages\\n- Support for custom headers and timeout configuration\\n\\n### HttpClient\\n\\nHandles all HTTP communication with the LangGraph API, including:\\n- JSON serialization/deserialization\\n- Server-Sent Events (SSE) for streaming responses\\n- Request retries and error handling\\n- Custom header management\\n\\n### Resource-Specific Clients\\n\\n| Client | Purpose | Key Operations |\\n|--------|---------|----------------|\\n| `AssistantsClient` | Manage versioned graph configurations | `create`, `get`, `update`, `delete`, `search`, `getGraph`, `getSchemas` |\\n| `ThreadsClient` | Handle conversation threads and state | `create`, `get`, `update`, `delete`, `search`, `getState`, `updateState` |\\n| `RunsClient` | Control graph execution | `create`, `get`, `cancel`, `stream`, `wait` |\\n| `CronClient` | Manage scheduled operations | `create`, `get`, `update`, `delete`, `search` |\\n| `StoreClient` | Persistent cross-thread storage | `put`, `get`, `search`, `delete`, `listNamespaces` |\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:486-498](), [libs/sdk-py/langgraph_sdk/client.py:1160-1170](), [libs/sdk-py/langgraph_sdk/client.py:1570-1580]()\\n\\n## TypeScript Type Definitions\\n\\nThe JavaScript SDK provides comprehensive TypeScript definitions mirroring the schema definitions from the Python implementation. These types ensure type safety and provide excellent IDE support.\\n\\n### Core Data Models\\n\\n```mermaid\\ngraph LR\\n    Assistant[\\\"Assistant\\\"] \\n    Thread[\\\"Thread\\\"]\\n    Run[\\\"Run\\\"] \\n    Cron[\\\"Cron\\\"]\\n    Item[\\\"Item\\\"]\\n    \\n    Config[\\\"Config\\\"]\\n    Context[\\\"Context\\\"]\\n    Checkpoint[\\\"Checkpoint\\\"]\\n    ThreadState[\\\"ThreadState\\\"]\\n    \\n    Assistant --> Config\\n    Assistant --> Context\\n    Thread --> ThreadState\\n    ThreadState --> Checkpoint\\n    Run --> Assistant\\n    Run --> Thread\\n```\\n\\n**Core Entity Relationships**\\n\\n**Key Type Categories:**\\n\\n| Category | Types | Purpose |\\n|----------|-------|---------|\\n| **Resources** | `Assistant`, `Thread`, `Run`, `Cron`, `Item` | Main API entities |\\n| **State Management** | `ThreadState`, `Checkpoint`, `Config`, `Context` | Execution state and configuration |\\n| **Operations** | `RunCreate`, `ThreadsSearch`, `AssistantsUpdate` | Request/response payloads |\\n| **Enums** | `RunStatus`, `StreamMode`, `MultitaskStrategy` | Controlled vocabularies |\\n\\nSources: [libs/sdk-py/langgraph_sdk/schema.py:21-30](), [libs/sdk-py/langgraph_sdk/schema.py:144-165](), [libs/sdk-py/langgraph_sdk/schema.py:205-226]()\\n\\n## Authentication and Authorization\\n\\nThe JavaScript SDK includes a comprehensive authentication system that mirrors the Python implementation, supporting custom authentication handlers and fine-grained authorization controls.\\n\\n### Authentication Flow\\n\\n```mermaid\\nsequenceDiagram\\n    participant Client as \\\"JS Client\\\"\\n    participant Auth as \\\"Auth Handler\\\"\\n    participant API as \\\"LangGraph API\\\"\\n    \\n    Client->>Auth: authenticate(credentials)\\n    Auth->>Auth: verify credentials\\n    Auth-->>Client: return user + permissions\\n    Client->>API: request with auth headers\\n    API->>API: authorize(user, resource, action)\\n    API-->>Client: response or 403\\n```\\n\\n**Authentication Sequence**\\n\\n### Authentication Configuration\\n\\nThe SDK supports multiple authentication patterns:\\n\\n- **API Key Authentication**: Simple header-based authentication\\n- **JWT Token Authentication**: Bearer token validation\\n- **Custom Authentication**: User-defined authentication logic\\n- **Role-Based Authorization**: Permission-based access control\\n\\n**Authentication Headers:**\\n- Uses `x-api-key` header for API key authentication\\n- Supports custom header configurations\\n- Reads from environment variables (`LANGGRAPH_API_KEY`, `LANGSMITH_API_KEY`, `LANGCHAIN_API_KEY`)\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:84-97](), [libs/sdk-py/langgraph_sdk/client.py:100-117](), [libs/sdk-py/langgraph_sdk/auth/__init__.py:13-77]()\\n\\n## API Interaction Patterns\\n\\n### Request/Response Handling\\n\\nThe JavaScript SDK implements consistent patterns for API interactions, handling both synchronous operations and streaming responses.\\n\\n```mermaid\\ngraph TD\\n    Request[\\\"Client Request\\\"]\\n    Serialize[\\\"JSON Serialization\\\"]\\n    HttpCall[\\\"HTTP Request\\\"]\\n    Response[\\\"API Response\\\"]\\n    Deserialize[\\\"JSON Deserialization\\\"]\\n    Result[\\\"Typed Result\\\"]\\n    \\n    StreamRequest[\\\"Streaming Request\\\"]\\n    SSE[\\\"Server-Sent Events\\\"]\\n    StreamParser[\\\"Stream Parser\\\"]\\n    StreamResult[\\\"Stream Iterator\\\"]\\n    \\n    Request --> Serialize\\n    Serialize --> HttpCall\\n    HttpCall --> Response\\n    Response --> Deserialize\\n    Deserialize --> Result\\n    \\n    StreamRequest --> SSE\\n    SSE --> StreamParser\\n    StreamParser --> StreamResult\\n```\\n\\n**Request Processing Flow**\\n\\n### Error Handling\\n\\nThe SDK provides structured error handling with detailed error information:\\n\\n- **HTTP Status Errors**: Automatic handling of 4xx/5xx responses\\n- **Network Errors**: Connection timeout and retry logic\\n- **Validation Errors**: Type validation for requests and responses\\n- **API Errors**: Structured error messages from the LangGraph API\\n\\n### Streaming Support\\n\\nStreaming operations use Server-Sent Events (SSE) for real-time updates:\\n\\n- **Run Streaming**: Real-time execution updates during graph runs\\n- **Event Streaming**: Node execution events and state changes\\n- **Value Streaming**: Intermediate results and outputs\\n- **Debug Streaming**: Detailed execution information\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:413-458](), [libs/sdk-py/langgraph_sdk/sse.py:77-133]()\\n\\n## Configuration and Setup\\n\\n### Client Initialization\\n\\nThe JavaScript SDK provides factory functions for creating configured client instances:\\n\\n```typescript\\n// Basic client creation\\nconst client = createClient({\\n  url: \\\"http://localhost:8123\\\",\\n  apiKey: \\\"your-api-key\\\"\\n});\\n\\n// With custom configuration\\nconst client = createClient({\\n  url: \\\"https://api.langgraph.com\\\",\\n  apiKey: process.env.LANGGRAPH_API_KEY,\\n  timeout: 30000,\\n  headers: {\\n    \\\"User-Agent\\\": \\\"MyApp/1.0\\\"\\n  }\\n});\\n```\\n\\n### Environment Configuration\\n\\nThe SDK automatically reads configuration from environment variables:\\n- `LANGGRAPH_API_KEY`: Primary API key\\n- `LANGSMITH_API_KEY`: Alternative API key\\n- `LANGCHAIN_API_KEY`: Fallback API key\\n\\n### Timeout Configuration\\n\\nSupports granular timeout control:\\n- **Connect Timeout**: Connection establishment limit\\n- **Read Timeout**: Response reading limit\\n- **Write Timeout**: Request sending limit\\n- **Pool Timeout**: Connection pool wait limit\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:152-220](), [libs/sdk-py/pyproject.toml:14-17]()\\n\\n## Usage Patterns\\n\\n### Basic Operations\\n\\n**Assistant Management:**\\n```typescript\\n// Create assistant\\nconst assistant = await client.assistants.create({\\n  graphId: \\\"my-graph\\\",\\n  config: { model: \\\"gpt-4\\\" },\\n  name: \\\"My Assistant\\\"\\n});\\n\\n// Get assistant\\nconst assistant = await client.assistants.get(assistantId);\\n```\\n\\n**Thread Operations:**\\n```typescript\\n// Create thread\\nconst thread = await client.threads.create({\\n  metadata: { userId: \\\"user123\\\" }\\n});\\n\\n// Update thread state\\nawait client.threads.updateState(threadId, {\\n  values: { messages: [newMessage] }\\n});\\n```\\n\\n**Run Execution:**\\n```typescript\\n// Create and stream run\\nconst stream = client.runs.stream(threadId, assistantId, {\\n  input: { query: \\\"Hello\\\" },\\n  streamMode: \\\"values\\\"\\n});\\n\\nfor await (const chunk of stream) {\\n  console.log(chunk);\\n}\\n```\\n\\n### Advanced Usage\\n\\n**Store Operations:**\\n```typescript\\n// Put item in store\\nawait client.store.put([\\\"user\\\", userId], \\\"preferences\\\", {\\n  theme: \\\"dark\\\",\\n  language: \\\"en\\\"\\n});\\n\\n// Search items\\nconst results = await client.store.search([\\\"user\\\"], {\\n  query: \\\"preferences\\\"\\n});\\n```\\n\\n**Cron Scheduling:**\\n```typescript\\n// Schedule periodic run\\nconst cron = await client.crons.create({\\n  assistantId: assistantId,\\n  schedule: \\\"0 9 * * 1\\\",  // Every Monday at 9 AM\\n  payload: { input: { task: \\\"weekly-report\\\" } }\\n});\\n```\\n\\nSources: [libs/sdk-py/langgraph_sdk/client.py:765-834](), [libs/sdk-py/langgraph_sdk/client.py:1890-1950](), [libs/sdk-py/langgraph_sdk/client.py:2680-2720]()\", \"# Page: React UI Components\\n\\n# React UI Components\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/how-tos/generative_ui_react.md](docs/docs/cloud/how-tos/generative_ui_react.md)\\n- [docs/docs/cloud/how-tos/img/generative_ui_sample.jpg](docs/docs/cloud/how-tos/img/generative_ui_sample.jpg)\\n- [libs/langgraph/langgraph/graph/ui.py](libs/langgraph/langgraph/graph/ui.py)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's React UI components system for building Generative User Interfaces (Generative UI). This system enables LangGraph agents to generate and update rich, interactive React components dynamically during graph execution, going beyond text-only responses to create context-aware user interfaces.\\n\\nThe React UI system bridges server-side graph execution with client-side React applications through a message-based architecture. For information about the broader client-server architecture, see [Client-Server Architecture](#6). For details about the Python and JavaScript SDKs used for API interaction, see [Python SDK](#6.2) and [JavaScript SDK](#6.3).\\n\\n## Architecture Overview\\n\\nThe React UI system consists of three main layers: server-side UI message generation, client-side React components for rendering, and a configuration system that bundles UI components with graph definitions.\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Server-Side Graph Execution\\\"\\n        GraphNodes[\\\"Graph Nodes<br/>Python/JS Functions\\\"]\\n        UIMessages[\\\"UI Message System<br/>push_ui_message()<br/>delete_ui_message()\\\"]\\n        StateManagement[\\\"State Management<br/>ui_message_reducer<br/>AnyUIMessage[]\\\"]\\n    end\\n    \\n    subgraph \\\"Configuration & Bundling\\\"\\n        LangGraphJSON[\\\"langgraph.json<br/>ui section\\\"]\\n        ComponentFiles[\\\"UI Component Files<br/>*.tsx exports\\\"]\\n        BundleSystem[\\\"LangGraph Platform<br/>Component Bundling\\\"]\\n    end\\n    \\n    subgraph \\\"Client-Side React App\\\"\\n        useStreamHook[\\\"useStream Hook<br/>Thread Management\\\"]\\n        LoadExternalComponent[\\\"LoadExternalComponent<br/>Dynamic Component Loader\\\"]\\n        StreamContext[\\\"useStreamContext<br/>Thread Interaction\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Flow\\\"\\n        UIMessageStream[\\\"UI Message Stream<br/>Real-time Updates\\\"]\\n        ComponentBundle[\\\"Component Bundle<br/>JS/CSS Assets\\\"]\\n        ShadowDOM[\\\"Shadow DOM<br/>Style Isolation\\\"]\\n    end\\n    \\n    %% Server flow\\n    GraphNodes --> UIMessages\\n    UIMessages --> StateManagement\\n    UIMessages --> UIMessageStream\\n    \\n    %% Configuration flow\\n    LangGraphJSON --> ComponentFiles\\n    ComponentFiles --> BundleSystem\\n    BundleSystem --> ComponentBundle\\n    \\n    %% Client flow\\n    useStreamHook --> UIMessageStream\\n    UIMessageStream --> LoadExternalComponent\\n    LoadExternalComponent --> ComponentBundle\\n    LoadExternalComponent --> ShadowDOM\\n    LoadExternalComponent --> StreamContext\\n    \\n    style UIMessages fill:#e1f5fe\\n    style LoadExternalComponent fill:#f3e5f5\\n    style useStreamHook fill:#fff3e0\\n```\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:1-539](), [libs/langgraph/langgraph/graph/ui.py:1-230]()\\n\\n## Server-Side UI Message System\\n\\nThe server-side UI system provides APIs for graph nodes to emit UI messages that trigger client-side component rendering. The core message types and functions handle creating, updating, and removing UI components.\\n\\n### UI Message Types\\n\\nThe system defines two primary message types for UI operations:\\n\\n| Message Type | Purpose | Key Fields |\\n|--------------|---------|------------|\\n| `UIMessage` | Create/update UI component | `name`, `props`, `metadata` |\\n| `RemoveUIMessage` | Remove UI component | `id` |\\n\\nThe `UIMessage` type represents a UI component to render with its properties:\\n\\n```python\\n# UIMessage structure\\n{\\n    \\\"type\\\": \\\"ui\\\",\\n    \\\"id\\\": \\\"unique-identifier\\\", \\n    \\\"name\\\": \\\"component-name\\\",\\n    \\\"props\\\": {\\\"key\\\": \\\"value\\\"},\\n    \\\"metadata\\\": {\\\"message_id\\\": \\\"associated-message-id\\\"}\\n}\\n```\\n\\n**Sources:** [libs/langgraph/langgraph/graph/ui.py:22-58]()\\n\\n### UI Message Generation Functions\\n\\nGraph nodes use `push_ui_message()` to emit UI components and `delete_ui_message()` to remove them:\\n\\n```python\\n# Emit a UI component from a graph node\\npush_ui_message(\\n    name=\\\"weather-card\\\",\\n    props={\\\"city\\\": \\\"San Francisco\\\", \\\"temperature\\\": 72},\\n    message=ai_message,  # Associate with a chat message\\n    merge=True  # Merge props with existing message\\n)\\n\\n# Remove a UI component\\ndelete_ui_message(\\\"ui-message-id\\\")\\n```\\n\\nThe `push_ui_message()` function supports several key parameters:\\n- `name`: Component identifier matching the client-side component map\\n- `props`: Data passed to the React component\\n- `message`: Optional chat message to associate the UI with\\n- `merge`: Whether to merge props with existing UI message\\n- `id`: Optional explicit ID for updates to existing components\\n\\n**Sources:** [libs/langgraph/langgraph/graph/ui.py:61-131](), [libs/langgraph/langgraph/graph/ui.py:133-163]()\\n\\n### UI Message Reducer\\n\\nThe `ui_message_reducer()` function manages UI message state in graph state, handling message merging and removal:\\n\\n```python\\n# State definition with UI message reducer\\nclass AgentState(TypedDict):\\n    messages: Annotated[Sequence[BaseMessage], add_messages]\\n    ui: Annotated[Sequence[AnyUIMessage], ui_message_reducer]\\n```\\n\\nThe reducer processes UI messages by:\\n- Merging new UI messages with existing ones by ID\\n- Handling `remove-ui` messages to delete components\\n- Supporting prop merging when `merge=True` is specified\\n- Maintaining message order and preventing duplicate IDs\\n\\n**Sources:** [libs/langgraph/langgraph/graph/ui.py:166-230]()\\n\\n## Client-Side React Components\\n\\nThe client-side system provides React hooks and components for consuming UI messages and rendering dynamic components from the server.\\n\\n### useStream Hook Integration\\n\\nThe `useStream` hook from the React SDK manages thread state and includes UI messages in its return values:\\n\\n```typescript\\nconst { thread, values, submit } = useStream({\\n  apiUrl: \\\"http://localhost:2024\\\",\\n  assistantId: \\\"agent\\\",\\n  onCustomEvent: (event, options) => {\\n    // Handle streaming UI updates\\n    options.mutate((prev) => {\\n      const ui = uiMessageReducer(prev.ui ?? [], event);\\n      return { ...prev, ui };\\n    });\\n  }\\n});\\n\\n// Access UI messages from thread values\\nconst uiMessages = values.ui || [];\\n```\\n\\nThe hook provides:\\n- `thread`: Contains messages and thread metadata\\n- `values`: Current graph state including UI messages\\n- `submit`: Function to send new messages to the graph\\n- `onCustomEvent`: Callback for handling streaming UI updates\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:192-220](), [docs/docs/cloud/how-tos/generative_ui_react.md:336-352]()\\n\\n### LoadExternalComponent\\n\\nThe `LoadExternalComponent` renders UI components dynamically by fetching component bundles from LangGraph Platform:\\n\\n```typescript\\nimport { LoadExternalComponent } from \\\"@langchain/langgraph-sdk/react-ui\\\";\\n\\n// Render UI messages associated with a chat message\\n{thread.messages.map((message) => (\\n  <div key={message.id}>\\n    {message.content}\\n    {values.ui\\n      ?.filter((ui) => ui.metadata?.message_id === message.id)\\n      .map((ui) => (\\n        <LoadExternalComponent \\n          key={ui.id} \\n          stream={thread} \\n          message={ui}\\n          namespace=\\\"custom-namespace\\\"  // Optional custom namespace\\n          fallback={<div>Loading...</div>}  // Loading state\\n          components={clientComponents}  // Optional local components\\n          meta={{ userId: \\\"123\\\" }}  // Additional context\\n        />\\n      ))}\\n  </div>\\n))}\\n```\\n\\nKey `LoadExternalComponent` features:\\n- Fetches component bundles from LangGraph Platform automatically\\n- Renders components in Shadow DOM for style isolation\\n- Supports local component overrides via `components` prop\\n- Provides loading states with `fallback` prop\\n- Passes additional context via `meta` prop\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:192-220](), [docs/docs/cloud/how-tos/generative_ui_react.md:225-276]()\\n\\n### useStreamContext Hook\\n\\nUI components can access thread state and interact with the graph using `useStreamContext`:\\n\\n```typescript\\nimport { useStreamContext } from \\\"@langchain/langgraph-sdk/react-ui\\\";\\n\\nconst WeatherComponent = (props: { city: string }) => {\\n  const { thread, submit, meta } = useStreamContext();\\n  \\n  return (\\n    <div>\\n      <div>Weather for {props.city}</div>\\n      <button onClick={() => {\\n        submit({ \\n          messages: [{ \\n            type: \\\"human\\\", \\n            content: `Update weather for ${props.city}` \\n          }] \\n        });\\n      }}>\\n        Refresh\\n      </button>\\n    </div>\\n  );\\n};\\n```\\n\\nThe hook provides access to:\\n- `thread`: Current thread state and messages\\n- `submit`: Function to send new messages to the graph\\n- `meta`: Additional context passed from `LoadExternalComponent`\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:278-333]()\\n\\n## UI Message Flow Diagram\\n\\n```mermaid\\nsequenceDiagram\\n    participant GraphNode as \\\"Graph Node\\\"\\n    participant UISystem as \\\"UI Message System\\\"\\n    participant StreamWriter as \\\"Stream Writer\\\"\\n    participant ReactApp as \\\"React Application\\\"\\n    participant LoadComponent as \\\"LoadExternalComponent\\\"\\n    participant Platform as \\\"LangGraph Platform\\\"\\n    \\n    GraphNode->>UISystem: \\\"push_ui_message(name, props)\\\"\\n    UISystem->>StreamWriter: \\\"UIMessage event\\\"\\n    UISystem->>UISystem: \\\"ui_message_reducer(state, message)\\\"\\n    \\n    StreamWriter->>ReactApp: \\\"UI message stream\\\"\\n    ReactApp->>ReactApp: \\\"useStream onCustomEvent\\\"\\n    ReactApp->>LoadComponent: \\\"UI message props\\\"\\n    \\n    LoadComponent->>Platform: \\\"Fetch component bundle\\\"\\n    Platform-->>LoadComponent: \\\"JS/CSS assets\\\"\\n    LoadComponent->>LoadComponent: \\\"Render in Shadow DOM\\\"\\n    \\n    note over LoadComponent: \\\"Component rendered with props from server\\\"\\n    \\n    LoadComponent->>ReactApp: \\\"useStreamContext submit()\\\"\\n    ReactApp->>GraphNode: \\\"New user message\\\"\\n```\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:87-187](), [libs/langgraph/langgraph/graph/ui.py:61-131]()\\n\\n## Component Configuration and Bundling\\n\\nThe configuration system defines how UI components are bundled with graph definitions and served to client applications.\\n\\n### langgraph.json Configuration\\n\\nThe `langgraph.json` file defines UI component mappings in the `ui` section:\\n\\n```json\\n{\\n  \\\"node_version\\\": \\\"20\\\",\\n  \\\"graphs\\\": {\\n    \\\"agent\\\": \\\"./src/agent.py:graph\\\"\\n  },\\n  \\\"ui\\\": {\\n    \\\"agent\\\": \\\"./src/agent/ui.tsx\\\",\\n    \\\"custom-namespace\\\": \\\"./src/shared/ui.tsx\\\"\\n  }\\n}\\n```\\n\\nConfiguration features:\\n- Maps namespace identifiers to component files\\n- Supports multiple namespaces for component organization\\n- Automatically bundles CSS and Tailwind styles\\n- Excludes React/ReactDOM from bundles (provided by runtime)\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:31-61]()\\n\\n### Component File Structure\\n\\nUI component files export a default object mapping component names to React components:\\n\\n```typescript\\n// src/agent/ui.tsx\\nimport \\\"./styles.css\\\";\\n\\nconst WeatherComponent = (props: { city: string; temperature?: number }) => {\\n  return (\\n    <div className=\\\"bg-blue-100 p-4 rounded\\\">\\n      <h3>Weather for {props.city}</h3>\\n      {props.temperature && <p>Temperature: {props.temperature}°F</p>}\\n    </div>\\n  );\\n};\\n\\nconst DocumentComponent = (props: { title: string; content?: string }) => {\\n  return (\\n    <article>\\n      <h2>{props.title}</h2>\\n      <p style={{ whiteSpace: \\\"pre-wrap\\\" }}>{props.content}</p>\\n    </article>\\n  );\\n};\\n\\nexport default {\\n  weather: WeatherComponent,\\n  document: DocumentComponent,\\n};\\n```\\n\\nComponent requirements:\\n- Export default object with named component mappings\\n- Accept props matching server-side `push_ui_message()` calls\\n- Support optional props for progressive enhancement\\n- Can import CSS and use Tailwind classes\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:21-86](), [docs/docs/cloud/how-tos/generative_ui_react.md:494-508]()\\n\\n## Integration Patterns\\n\\n### Streaming UI Updates\\n\\nComponents can receive progressive updates during long-running operations by using the same UI message ID:\\n\\n```python\\n# Initial UI message\\nui_message = push_ui_message(\\\"document\\\", {\\\"title\\\": \\\"Draft\\\"}, message=message)\\n\\n# Stream updates with same ID\\nasync for chunk in content_stream:\\n    push_ui_message(\\n        \\\"document\\\", \\n        {\\\"content\\\": current_content}, \\n        id=ui_message[\\\"id\\\"],  # Same ID for updates\\n        merge=True  # Merge with existing props\\n    )\\n```\\n\\nThis pattern enables:\\n- Real-time component updates during AI generation\\n- Progressive enhancement of UI components\\n- Efficient partial updates without re-mounting components\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:335-492]()\\n\\n### TypeScript Integration\\n\\nThe JavaScript SDK provides type-safe UI message generation:\\n\\n```typescript\\nimport { typedUi } from \\\"@langchain/langgraph-sdk/react-ui/server\\\";\\nimport type ComponentMap from \\\"./ui.js\\\";\\n\\nasync function graphNode(state: AgentState, config: LangGraphRunnableConfig) {\\n  const ui = typedUi<typeof ComponentMap>(config);\\n  \\n  // Type-safe UI message creation\\n  ui.push(\\n    { name: \\\"weather\\\", props: { city: \\\"SF\\\" } },  // Props validated against ComponentMap\\n    { message: response }\\n  );\\n}\\n```\\n\\nThe typed UI system provides:\\n- Compile-time validation of component names and props\\n- IntelliSense support for UI message creation\\n- Type safety between server and client component interfaces\\n\\n**Sources:** [docs/docs/cloud/how-tos/generative_ui_react.md:134-187]()\", \"# Page: Deployment and Platform\\n\\n# Deployment and Platform\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [libs/cli/README.md](libs/cli/README.md)\\n- [libs/cli/generate_schema.py](libs/cli/generate_schema.py)\\n- [libs/cli/langgraph_cli/__init__.py](libs/cli/langgraph_cli/__init__.py)\\n- [libs/cli/langgraph_cli/cli.py](libs/cli/langgraph_cli/cli.py)\\n- [libs/cli/langgraph_cli/config.py](libs/cli/langgraph_cli/config.py)\\n- [libs/cli/pyproject.toml](libs/cli/pyproject.toml)\\n- [libs/cli/schemas/schema.json](libs/cli/schemas/schema.json)\\n- [libs/cli/schemas/schema.v0.json](libs/cli/schemas/schema.v0.json)\\n- [libs/cli/tests/unit_tests/cli/test_cli.py](libs/cli/tests/unit_tests/cli/test_cli.py)\\n- [libs/cli/tests/unit_tests/test_config.py](libs/cli/tests/unit_tests/test_config.py)\\n- [libs/cli/uv.lock](libs/cli/uv.lock)\\n\\n</details>\\n\\n\\n\\nLangGraph applications can be deployed across a spectrum of environments, from local development to production-ready cloud platforms. This overview introduces the main deployment approaches and their architectural patterns.\\n\\nFor details about the LangGraph execution model that powers these deployments, see [Core Architecture](#2). For client-side integration, see [Client-Server Architecture](#6).\\n\\n## Deployment Spectrum\\n\\nLangGraph supports three primary deployment approaches, each with different trade-offs between simplicity, control, and operational responsibility:\\n\\n**Deployment Architecture Overview**\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Development\\\"\\n        DEV[\\\"langgraph dev<br/>Local Development\\\"]\\n    end\\n    \\n    subgraph \\\"Cloud Platform\\\"\\n        CLOUD[\\\"LangGraph Platform<br/>Fully Managed SaaS\\\"]\\n        HYBRID[\\\"Self-Hosted Data Plane<br/>Hybrid Management\\\"]\\n    end\\n    \\n    subgraph \\\"Self-Hosted\\\"\\n        CONTAINER[\\\"Standalone Container<br/>Full Self-Management\\\"]\\n        K8S[\\\"Kubernetes<br/>Custom Orchestration\\\"]\\n    end\\n    \\n    DEV --> CLOUD\\n    DEV --> CONTAINER\\n    CLOUD --> HYBRID\\n    CONTAINER --> K8S\\n```\\n\\n**Management Responsibility Matrix**\\n\\n| Deployment Type | Infrastructure | Platform Services | Application Code | Data Control |\\n|----------------|---------------|-------------------|------------------|--------------|\\n| Local Development | Developer | Developer | Developer | Developer |\\n| LangGraph Platform | LangChain | LangChain | Developer | LangChain |\\n| Self-Hosted Data Plane | Developer | LangChain | Developer | Developer |\\n| Standalone Container | Developer | Developer | Developer | Developer |\\n\\nSources: [libs/cli/langgraph_cli/cli.py:651-738](), [libs/cli/langgraph_cli/config.py:359-484]()\\n\\n## Configuration Foundation\\n\\nAll deployment approaches use a common `langgraph.json` configuration file that defines application structure, dependencies, and runtime settings.\\n\\n**Configuration Structure**\\n\\n```mermaid\\ngraph TD\\n    CONFIG[\\\"langgraph.json\\\"] --> CORE[\\\"Core Settings\\\"]\\n    CONFIG --> PLATFORM[\\\"Platform Settings\\\"]\\n    CONFIG --> DEPLOYMENT[\\\"Deployment Settings\\\"]\\n    \\n    CORE --> GRAPHS[\\\"graphs: dict[str, str]\\\"]\\n    CORE --> DEPS[\\\"dependencies: list[str]\\\"]\\n    CORE --> ENV[\\\"env: dict | str\\\"]\\n    \\n    PLATFORM --> AUTH[\\\"auth: AuthConfig\\\"]\\n    PLATFORM --> STORE[\\\"store: StoreConfig\\\"]\\n    PLATFORM --> HTTP[\\\"http: HttpConfig\\\"]\\n    \\n    DEPLOYMENT --> PYTHON_VER[\\\"python_version: str\\\"]\\n    DEPLOYMENT --> NODE_VER[\\\"node_version: str\\\"]\\n    DEPLOYMENT --> DOCKER_LINES[\\\"dockerfile_lines: list[str]\\\"]\\n```\\n\\nThe configuration system validates and transforms these settings for different deployment targets through the `validate_config()` function in [libs/cli/langgraph_cli/config.py:575-718]().\\n\\nSources: [libs/cli/langgraph_cli/config.py:359-496](), [libs/cli/schemas/schema.json:1-200]()\\n\\n## CLI Tool Integration\\n\\nThe LangGraph CLI provides the primary interface for managing deployments across all environments. The CLI commands map to different deployment targets:\\n\\n**CLI Command Flow**\\n\\n```mermaid\\ngraph TD\\n    CLI[\\\"langgraph CLI\\\"] --> DEV_CMD[\\\"langgraph dev\\\"]\\n    CLI --> BUILD_CMD[\\\"langgraph build\\\"]\\n    CLI --> UP_CMD[\\\"langgraph up\\\"]\\n    CLI --> DOCKERFILE_CMD[\\\"langgraph dockerfile\\\"]\\n    \\n    DEV_CMD --> LOCAL_SERVER[\\\"Local Development Server\\\"]\\n    BUILD_CMD --> DOCKER_IMAGE[\\\"Docker Image\\\"]\\n    UP_CMD --> DOCKER_COMPOSE[\\\"Docker Compose Stack\\\"]\\n    DOCKERFILE_CMD --> DOCKERFILE_GEN[\\\"Generated Dockerfile\\\"]\\n    \\n    LOCAL_SERVER --> IN_MEMORY[\\\"langgraph-api.cli.run_server()\\\"]\\n    DOCKER_IMAGE --> REGISTRY[\\\"Container Registry\\\"]\\n    DOCKER_COMPOSE --> POSTGRES_SVC[\\\"langgraph-postgres\\\"]\\n    DOCKER_COMPOSE --> REDIS_SVC[\\\"langgraph-redis\\\"]\\n    DOCKER_COMPOSE --> API_SVC[\\\"langgraph-api\\\"]\\n    DOCKERFILE_GEN --> CUSTOM_BUILD[\\\"Custom Build Process\\\"]\\n```\\n\\nThe CLI bridges local development and production deployment by providing consistent tooling across environments. For detailed CLI functionality, see [CLI Tool](#7.1).\\n\\nSources: [libs/cli/langgraph_cli/cli.py:163-166](), [libs/cli/langgraph_cli/cli.py:651-738](), [libs/cli/langgraph_cli/cli.py:199-294]()\\n\\n## Deployment Options Overview\\n\\n### Local Development\\n\\nThe `langgraph dev` command provides hot-reload development with minimal setup. It runs an in-memory server that integrates with LangGraph Studio for visual debugging.\\n\\n**Key Features:**\\n- Hot reloading on code changes\\n- Built-in LangGraph Studio integration\\n- Optional remote debugging support\\n- Cloudflare tunnel for remote access\\n\\nFor complete development workflow details, see [CLI Tool](#7.1).\\n\\n### LangGraph Platform\\n\\nThe managed LangGraph Platform provides production-ready infrastructure with minimal operational overhead. It handles scaling, monitoring, and maintenance automatically.\\n\\n**Platform Architecture:**\\n- Managed control plane for deployment and monitoring\\n- Auto-scaling data plane for request processing\\n- Built-in observability and debugging tools\\n- Integration with LangSmith for tracing\\n\\nThe platform supports both fully managed and hybrid deployment models. For platform-specific features and deployment processes, see [LangGraph Platform](#7.2).\\n\\n### Self-Hosted Deployment\\n\\nSelf-hosted deployments provide maximum control and data sovereignty. They range from single-container deployments to full Kubernetes orchestration.\\n\\n**Self-Hosted Options:**\\n- Standalone Docker containers\\n- Docker Compose stacks\\n- Kubernetes deployments\\n- Custom orchestration platforms\\n\\nSelf-hosted deployments require managing PostgreSQL, Redis, and application scaling. For implementation details and infrastructure requirements, see [Self-Hosted Deployment](#7.3).\\n\\nSources: [libs/cli/langgraph_cli/cli.py:651-738](), [libs/cli/langgraph_cli/config.py:575-718]()\\n\\n## Docker Containerization\\n\\nThe CLI generates Docker images and compose configurations for deploying LangGraph applications. The containerization process handles dependency installation, environment setup, and service orchestration.\\n\\n### Docker Build Process\\n\\n```mermaid\\ngraph TD\\n    CONFIG[\\\"langgraph.json\\\"] --> VALIDATE[\\\"validate_config_file()\\\"]\\n    VALIDATE --> DOCKER_GEN[\\\"config_to_docker()\\\"]\\n    DOCKER_GEN --> DOCKERFILE[\\\"Generated Dockerfile\\\"]\\n    DOCKER_GEN --> CONTEXTS[\\\"Additional Build Contexts\\\"]\\n    \\n    DOCKERFILE --> BUILD[\\\"docker build\\\"]\\n    BUILD --> IMAGE[\\\"Docker Image\\\"]\\n    \\n    DEPS[\\\"Local Dependencies\\\"] --> ASSEMBLE[\\\"_assemble_local_deps()\\\"]\\n    ASSEMBLE --> REAL_PKG[\\\"Real Packages<br/>(pyproject.toml)\\\"]\\n    ASSEMBLE --> FAUX_PKG[\\\"Faux Packages<br/>(generated metadata)\\\"]\\n    \\n    REAL_PKG --> COPY_REAL[\\\"COPY package /deps/name\\\"]\\n    FAUX_PKG --> COPY_FAUX[\\\"ADD . /deps/__outer_name/src\\\"]\\n    FAUX_PKG --> GEN_META[\\\"RUN generate pyproject.toml\\\"]\\n```\\n\\n**Dockerfile Generation**\\n\\nThe Docker image generation is handled by [libs/cli/langgraph_cli/config.py:1211-1410]() with key functions:\\n\\n- `config_to_docker()` - Main generation function\\n- `_assemble_local_deps()` - Processes local Python packages [libs/cli/langgraph_cli/config.py:802-917]()\\n- `_get_pip_cleanup_lines()` - Removes build tools [libs/cli/langgraph_cli/config.py:488-532]()\\n\\n**Compose Service Generation**\\n\\nDocker Compose services are generated by the compose functions in `langgraph_cli.docker` module:\\n\\n```mermaid\\ngraph LR\\n    COMPOSE[\\\"compose()\\\"] --> SERVICES[\\\"Service Definitions\\\"]\\n    SERVICES --> POSTGRES[\\\"langgraph-postgres<br/>pgvector/pgvector:pg16\\\"]\\n    SERVICES --> REDIS[\\\"langgraph-redis<br/>redis:6\\\"] \\n    SERVICES --> API[\\\"langgraph-api<br/>Built Image\\\"]\\n    SERVICES --> DEBUGGER[\\\"langgraph-debugger<br/>langchain/langgraph-debugger\\\"]\\n    \\n    API --> HEALTHCHECK[\\\"healthcheck: python /api/healthcheck.py\\\"]\\n    API --> DEPENDS[\\\"depends_on: postgres, redis\\\"]\\n```\\n\\nSources: [libs/cli/langgraph_cli/config.py:1211-1410](), [libs/cli/tests/unit_tests/test_config.py:411-477]()\\n\\n## Development Workflow\\n\\nThe CLI provides different modes for development and production deployment, with `langgraph dev` offering hot-reload capabilities for rapid iteration.\\n\\n### Development Server Flow\\n\\n```mermaid\\ngraph TD\\n    DEV_CMD[\\\"langgraph dev\\\"] --> VALIDATE_CONFIG[\\\"validate_config_file()\\\"]\\n    VALIDATE_CONFIG --> CHECK_INMEM[\\\"Check langgraph-api import\\\"]\\n    CHECK_INMEM --> RUN_SERVER[\\\"langgraph_api.cli.run_server()\\\"]\\n    \\n    RUN_SERVER --> WATCH[\\\"File Watcher\\\"]\\n    RUN_SERVER --> STUDIO[\\\"LangGraph Studio Integration\\\"]\\n    RUN_SERVER --> TUNNEL[\\\"Optional Cloudflare Tunnel\\\"]\\n    \\n    WATCH --> RELOAD[\\\"Hot Reload on Changes\\\"]\\n    STUDIO --> BROWSER[\\\"Auto-open Browser\\\"]\\n```\\n\\n**Development Server Configuration**\\n\\nThe development server runs through [libs/cli/langgraph_cli/cli.py:651-738]() with these key features:\\n\\n- Hot reloading via `not no_reload` parameter\\n- Studio integration with `studio_url` parameter  \\n- Debug port support via `debug_port` parameter\\n- Tunnel support for remote access via `tunnel` parameter\\n\\n### Production Build Flow  \\n\\n```mermaid\\ngraph TD\\n    BUILD_CMD[\\\"langgraph build\\\"] --> PULL[\\\"docker pull base image\\\"]\\n    PULL --> GENERATE[\\\"config_to_docker()\\\"]\\n    GENERATE --> DOCKERFILE_STDIN[\\\"Dockerfile content\\\"] \\n    GENERATE --> BUILD_CONTEXTS[\\\"Additional contexts\\\"]\\n    \\n    DOCKERFILE_STDIN --> DOCKER_BUILD[\\\"docker build -f -\\\"]\\n    BUILD_CONTEXTS --> DOCKER_BUILD\\n    DOCKER_BUILD --> TAGGED_IMAGE[\\\"Tagged Docker Image\\\"]\\n    \\n    UP_CMD[\\\"langgraph up\\\"] --> COMPOSE_GEN[\\\"compose()\\\"]\\n    COMPOSE_GEN --> COMPOSE_YAML[\\\"Docker Compose YAML\\\"]\\n    COMPOSE_YAML --> DOCKER_COMPOSE[\\\"docker-compose up\\\"]\\n```\\n\\nSources: [libs/cli/langgraph_cli/cli.py:296-344](), [libs/cli/langgraph_cli/cli.py:754-849]()\\n\\n## Platform Infrastructure\\n\\nLangGraph Platform provides managed infrastructure components including databases, task queues, and monitoring systems that support production deployments.\\n\\n### Infrastructure Components\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Storage Layer\\\"\\n        POSTGRES[(\\\"PostgreSQL<br/>- Checkpoints<br/>- Threads<br/>- Runs<br/>- Store\\\")]\\n        REDIS[(\\\"Redis<br/>- Task Queue<br/>- Pub/Sub\\\")]\\n    end\\n    \\n    subgraph \\\"Compute Layer\\\"  \\n        SERVER[\\\"LangGraph Server<br/>FastAPI Application\\\"]\\n        WORKER[\\\"Task Workers<br/>Background Processing\\\"]\\n        SCHEDULER[\\\"Cron Scheduler\\\"]\\n    end\\n    \\n    subgraph \\\"Platform Services\\\"\\n        ASSISTANTS[\\\"Assistants API<br/>/assistants/*\\\"]\\n        THREADS[\\\"Threads API<br/>/threads/*\\\"] \\n        RUNS[\\\"Runs API<br/>/runs/*\\\"]\\n        STORE_API[\\\"Store API<br/>/store/*\\\"]\\n    end\\n    \\n    SERVER --> POSTGRES\\n    SERVER --> REDIS\\n    WORKER --> POSTGRES  \\n    WORKER --> REDIS\\n    SCHEDULER --> SERVER\\n    \\n    ASSISTANTS --> SERVER\\n    THREADS --> SERVER\\n    RUNS --> SERVER\\n    STORE_API --> SERVER\\n```\\n\\n**Service Configuration**\\n\\nPlatform services can be configured through the HTTP configuration in [libs/cli/langgraph_cli/config.py:306-356]():\\n\\n- `disable_assistants` - Remove `/assistants` routes [libs/cli/langgraph_cli/config.py:315-319]()\\n- `disable_threads` - Remove `/threads` routes [libs/cli/langgraph_cli/config.py:320-324]()  \\n- `disable_runs` - Remove `/runs` routes [libs/cli/langgraph_cli/config.py:325-329]()\\n- `disable_store` - Remove `/store` routes [libs/cli/langgraph_cli/config.py:330-334]()\\n\\n**Environment Variables for Platform**\\n\\nStandalone container deployments require these environment variables:\\n\\n- `REDIS_URI` - Redis connection for task queue\\n- `DATABASE_URI` - PostgreSQL connection for persistence  \\n- `LANGGRAPH_CLOUD_LICENSE_KEY` - Enterprise license key\\n- `LANGSMITH_ENDPOINT` - Self-hosted LangSmith endpoint\\n\\nSources: [docs/docs/cloud/deployment/standalone_container.md:9-26](), [docs/docs/concepts/langgraph_server.md:40-48]()\", \"# Page: CLI Tool\\n\\n# CLI Tool\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/deployment/cloud.md](docs/docs/cloud/deployment/cloud.md)\\n- [docs/docs/cloud/deployment/setup.md](docs/docs/cloud/deployment/setup.md)\\n- [docs/docs/cloud/deployment/setup_javascript.md](docs/docs/cloud/deployment/setup_javascript.md)\\n- [docs/docs/cloud/deployment/setup_pyproject.md](docs/docs/cloud/deployment/setup_pyproject.md)\\n- [docs/docs/cloud/reference/cli.md](docs/docs/cloud/reference/cli.md)\\n- [docs/docs/concepts/application_structure.md](docs/docs/concepts/application_structure.md)\\n- [docs/docs/concepts/langgraph_cli.md](docs/docs/concepts/langgraph_cli.md)\\n- [docs/docs/concepts/sdk.md](docs/docs/concepts/sdk.md)\\n- [libs/cli/README.md](libs/cli/README.md)\\n- [libs/cli/generate_schema.py](libs/cli/generate_schema.py)\\n- [libs/cli/langgraph_cli/__init__.py](libs/cli/langgraph_cli/__init__.py)\\n- [libs/cli/langgraph_cli/cli.py](libs/cli/langgraph_cli/cli.py)\\n- [libs/cli/langgraph_cli/config.py](libs/cli/langgraph_cli/config.py)\\n- [libs/cli/pyproject.toml](libs/cli/pyproject.toml)\\n- [libs/cli/schemas/schema.json](libs/cli/schemas/schema.json)\\n- [libs/cli/schemas/schema.v0.json](libs/cli/schemas/schema.v0.json)\\n- [libs/cli/tests/unit_tests/cli/test_cli.py](libs/cli/tests/unit_tests/cli/test_cli.py)\\n- [libs/cli/tests/unit_tests/test_config.py](libs/cli/tests/unit_tests/test_config.py)\\n- [libs/cli/uv.lock](libs/cli/uv.lock)\\n\\n</details>\\n\\n\\n\\nThe LangGraph CLI is a command-line interface for building, developing, and deploying LangGraph applications. It provides commands for local development with hot reloading, Docker-based deployment, and project scaffolding from templates. The CLI handles configuration parsing, Docker image generation, and orchestration of development and production services.\\n\\nFor information about the broader deployment architecture, see [LangGraph Platform](#7.2). For details on self-hosted deployment options, see [Self-Hosted Deployment](#7.3).\\n\\n## Core Architecture\\n\\nThe CLI tool serves as the primary interface between developers and the LangGraph deployment infrastructure. It operates on a configuration-driven model where `langgraph.json` defines application structure, dependencies, and deployment settings.\\n\\n### CLI Command Flow\\n\\n```mermaid\\ngraph TD\\n    User[\\\"User\\\"] --> CLI[\\\"cli()\\\"]\\n    CLI --> ConfigValidation[\\\"validate_config_file()\\\"]\\n    CLI --> DevCmd[\\\"dev()\\\"]\\n    CLI --> UpCmd[\\\"up()\\\"]\\n    CLI --> BuildCmd[\\\"build()\\\"]\\n    CLI --> NewCmd[\\\"new()\\\"]\\n    CLI --> DockerfileCmd[\\\"dockerfile()\\\"]\\n    \\n    ConfigValidation --> ValidatedConfig[\\\"Config TypedDict\\\"]\\n    ValidatedConfig --> LocalDepsAssembly[\\\"_assemble_local_deps()\\\"]\\n    \\n    DevCmd --> RunServer[\\\"langgraph_api.cli.run_server()\\\"]\\n    UpCmd --> PrepareArgs[\\\"prepare_args_and_stdin()\\\"]\\n    BuildCmd --> BuildFunc[\\\"_build()\\\"]\\n    NewCmd --> CreateNew[\\\"create_new()\\\"]\\n    DockerfileCmd --> ConfigToDocker[\\\"config_to_docker()\\\"]\\n    \\n    PrepareArgs --> DockerCompose[\\\"docker compose up\\\"]\\n    BuildFunc --> DockerBuild[\\\"docker build\\\"]\\n    ConfigToDocker --> GeneratedDockerfile[\\\"Dockerfile + contexts\\\"]\\n    \\n    DockerCompose --> Services[\\\"langgraph-redis + langgraph-postgres + langgraph-api\\\"]\\n    DockerBuild --> ApiImage[\\\"Tagged Docker Image\\\"]\\n```\\n\\nSources: [libs/cli/langgraph_cli/cli.py:163-166](), [libs/cli/langgraph_cli/cli.py:199-294](), [libs/cli/langgraph_cli/config.py:733-773]()\\n\\n### Configuration Processing Pipeline\\n\\n```mermaid\\ngraph LR\\n    LangGraphJson[\\\"langgraph.json\\\"] --> ValidateConfigFile[\\\"validate_config_file()\\\"]\\n    LocalPaths[\\\"Local Dependencies\\\"] --> AssembleLocalDeps[\\\"_assemble_local_deps()\\\"]\\n    EnvFile[\\\".env\\\"] --> EnvProcessing[\\\"Environment Variables\\\"]\\n    \\n    ValidateConfigFile --> ConfigDict[\\\"Config TypedDict\\\"]\\n    AssembleLocalDeps --> LocalDepsStruct[\\\"LocalDeps NamedTuple\\\"]\\n    EnvProcessing --> EnvVars[\\\"Environment Variables\\\"]\\n    \\n    ConfigDict --> DockerGeneration[\\\"config_to_docker()\\\"]\\n    LocalDepsStruct --> DockerGeneration\\n    ConfigDict --> ComposeGeneration[\\\"config_to_compose()\\\"]\\n    \\n    DockerGeneration --> DockerfileStdin[\\\"Dockerfile stdin + contexts\\\"]\\n    ComposeGeneration --> ComposeStdin[\\\"docker-compose.yml stdin\\\"]\\n    \\n    DockerfileStdin --> DockerBuildCommand[\\\"docker build\\\"]\\n    ComposeStdin --> DockerComposeCommand[\\\"docker compose up\\\"]\\n    \\n    DockerBuildCommand --> TaggedImage[\\\"langgraph-api:tag\\\"]\\n    DockerComposeCommand --> ServiceStack[\\\"langgraph-api + langgraph-postgres + langgraph-redis\\\"]\\n```\\n\\nSources: [libs/cli/langgraph_cli/config.py:587-730](), [libs/cli/langgraph_cli/config.py:834-917]()\\n\\n## Command Structure\\n\\nThe CLI provides five main commands, each serving different stages of the development and deployment lifecycle.\\n\\n### Development Commands\\n\\n#### `langgraph dev`\\nRuns a lightweight development server with hot reloading capabilities. This command starts an in-memory LangGraph API server without requiring Docker.\\n\\n```\\nKey Options:\\n--host: Network interface binding (default: 127.0.0.1)\\n--port: Port number (default: 2024)  \\n--no-reload: Disable automatic reloading\\n--config: Configuration file path (default: langgraph.json)\\n--debug-port: Enable remote debugging\\n--tunnel: Create public tunnel for remote access\\n--allow-blocking: Don't raise errors for synchronous I/O operations\\n--studio-url: URL of LangGraph Studio instance\\n```\\n\\nThe `dev` command requires the `inmem` extra (`pip install \\\"langgraph-cli[inmem]\\\"`) and imports `langgraph_api.cli.run_server` to run the API server directly in the Python process without Docker.\\n\\nSources: [libs/cli/langgraph_cli/cli.py:656-738]()\\n\\n#### `langgraph new`\\nCreates new LangGraph projects from templates. This command provides an interactive template selection process when no template is specified.\\n\\n```\\nUsage: langgraph new [PATH] --template TEMPLATE_NAME\\n```\\n\\nSources: [libs/cli/langgraph_cli/cli.py:741-751]()\\n\\n### Production Commands\\n\\n#### `langgraph up`\\nLaunches a full production-like environment using Docker Compose. This includes PostgreSQL with pgvector, Redis, and the LangGraph API server with proper service orchestration and health checks.\\n\\n```\\nKey Options:\\n--port: Exposed port (default: 8123)\\n--watch: Enable file watching for auto-rebuild\\n--recreate: Force container recreation\\n--pull: Pull latest base images (default: true)\\n--debugger-port: Launch LangGraph Studio debugger\\n--postgres-uri: Custom PostgreSQL connection string\\n--image: Use existing Docker image instead of building\\n```\\n\\nThe command uses `prepare_args_and_stdin()` to generate Docker Compose YAML with service definitions for `langgraph-api`, `langgraph-postgres`, `langgraph-redis`, and optionally `langgraph-debugger`.\\n\\nSources: [libs/cli/langgraph_cli/cli.py:201-294]()\\n\\n#### `langgraph build`\\nBuilds a Docker image that can be deployed independently. This creates a self-contained image with all dependencies and application code.\\n\\n```\\nUsage: langgraph build -t TAG [OPTIONS]\\n```\\n\\nThe build process uses `config_to_docker()` to generate a Dockerfile and handles multi-stage builds for local dependencies.\\n\\nSources: [libs/cli/langgraph_cli/cli.py:347-401]()\\n\\n#### `langgraph dockerfile`\\nGenerates a Dockerfile without building it, useful for custom deployment pipelines or CI/CD integration.\\n\\n```\\nUsage: langgraph dockerfile SAVE_PATH [OPTIONS]\\n```\\n\\nSources: [libs/cli/langgraph_cli/cli.py:458-578]()\\n\\n## Configuration System\\n\\nThe CLI operates on a JSON configuration file that defines application structure, dependencies, and deployment parameters.\\n\\n### Configuration Schema\\n\\nThe configuration system supports both Python and Node.js applications with validation handled by the `Config` TypedDict and related schemas.\\n\\n| Configuration Key | Purpose | Example |\\n|------------------|---------|---------|\\n| `dependencies` | Package dependencies and local paths | `[\\\".\\\"]`, `[\\\"langchain_openai\\\"]` |\\n| `graphs` | Mapping of graph IDs to Python/JS objects | `{\\\"agent\\\": \\\"./agent.py:graph\\\"}` |\\n| `env` | Environment variables (file path or object) | `\\\".env\\\"` or `{\\\"KEY\\\": \\\"value\\\"}` |\\n| `python_version` | Python runtime version | `\\\"3.11\\\"`, `\\\"3.12\\\"` |\\n| `pip_installer` | Package installer selection | `\\\"auto\\\"`, `\\\"pip\\\"`, `\\\"uv\\\"` |\\n| `dockerfile_lines` | Additional Docker instructions | `[\\\"RUN apt-get update\\\"]` |\\n\\nSources: [libs/cli/langgraph_cli/config.py:359-483]()\\n\\n### Local Dependencies Processing\\n\\nThe `_assemble_local_deps()` function processes local package references and classifies them into three categories:\\n\\n```mermaid\\ngraph TD\\n    LocalDep[\\\"Local Dependency\\\"] --> CheckFiles[\\\"Check pyproject.toml/setup.py\\\"]\\n    CheckFiles --> RealPkg[\\\"Real Package\\\"]\\n    CheckFiles --> FauxPkg[\\\"Faux Package\\\"]\\n    \\n    RealPkg --> PipInstall[\\\"pip install -e /deps/pkg\\\"]\\n    FauxPkg --> GenPyproject[\\\"Generate minimal pyproject.toml\\\"]\\n    GenPyproject --> PipInstall\\n    \\n    FauxPkg --> CheckInit[\\\"Has __init__.py?\\\"]\\n    CheckInit --> FlatLayout[\\\"Flat Layout\\\"]\\n    CheckInit --> SrcLayout[\\\"Src Layout\\\"]\\n    \\n    FlatLayout --> FlatPath[\\\"/deps/__outer_name/name\\\"]\\n    SrcLayout --> SrcPath[\\\"/deps/__outer_name/src\\\"]\\n```\\n\\nSources: [libs/cli/langgraph_cli/config.py:744-917]()\\n\\n### Configuration Validation\\n\\nThe `validate_config()` function enforces version constraints, dependency requirements, and path validation:\\n\\n- Python version must be >= 3.11\\n- Node.js version must be >= 20 (when specified)\\n- `dependencies` list is required for Python applications\\n- `graphs` dictionary must contain at least one entry\\n- Local dependency paths must exist and be directories\\n\\nSources: [libs/cli/langgraph_cli/config.py:574-698]()\\n\\n## Docker Integration\\n\\nThe CLI generates Docker configurations dynamically based on the parsed configuration. This includes multi-stage builds, build contexts, and service orchestration.\\n\\n### Dockerfile Generation\\n\\nThe `config_to_docker()` function produces Dockerfiles with the following structure:\\n\\n1. **Base Image Selection**: `langchain/langgraph-api:${python_version}` or `langchain/langgraphjs-api:${node_version}`\\n2. **Custom Dockerfile Lines**: User-specified Docker instructions\\n3. **Pip Configuration**: Custom pip.conf handling if specified\\n4. **PyPI Dependencies**: Installation of non-local packages\\n5. **Local Dependencies**: Copying and installing local packages\\n6. **Environment Variables**: Setting LANGSERVE_GRAPHS and other runtime config\\n7. **Cleanup**: Removal of packaging tools to reduce image size\\n8. **Working Directory**: Set to primary local dependency if \\\".\\\" is specified\\n\\nSources: [libs/cli/langgraph_cli/config.py:1004-1180]()\\n\\n### Docker Compose Generation\\n\\nFor the `up` command, the CLI generates a complete Docker Compose stack with:\\n\\n- **PostgreSQL**: pgvector-enabled database with persistent volumes\\n- **Redis**: Caching and job queue service  \\n- **LangGraph API**: Main application service with build configuration\\n- **LangGraph Debugger**: Optional Studio debugging interface\\n\\nEach service includes proper health checks, dependency ordering, and environment variable configuration.\\n\\nSources: [libs/cli/langgraph_cli/docker.py]()\\n\\n## Development Workflow\\n\\n### Hot Reloading Architecture\\n\\nThe `dev` command provides a streamlined development experience by running the API server directly in the Python process rather than in Docker.\\n\\n```mermaid\\ngraph TD\\n    DevCmd[\\\"langgraph dev\\\"] --> ParseConfig[\\\"Parse langgraph.json\\\"]\\n    ParseConfig --> SysPath[\\\"Add dependencies to sys.path\\\"]\\n    SysPath --> ImportGraphs[\\\"Import graph modules\\\"]\\n    ImportGraphs --> RunServer[\\\"langgraph_api.cli.run_server()\\\"]\\n    \\n    RunServer --> APIServer[\\\"FastAPI Server\\\"]\\n    RunServer --> FileWatcher[\\\"File Change Monitor\\\"]\\n    RunServer --> Browser[\\\"Auto-open browser\\\"]\\n    \\n    FileWatcher --> Reload[\\\"Hot Reload\\\"]\\n    Reload --> APIServer\\n    \\n    APIServer --> Studio[\\\"LangGraph Studio Integration\\\"]\\n    APIServer --> SDKClients[\\\"SDK Client Connections\\\"]\\n```\\n\\nSources: [libs/cli/langgraph_cli/cli.py:669-738]()\\n\\n### Watch Mode\\n\\nThe `up` command supports watch mode for Docker-based development, automatically rebuilding containers when source files change. This uses Docker Compose's `develop.watch` configuration to monitor specific paths.\\n\\nSources: [libs/cli/tests/unit_tests/cli/test_cli.py:158-165]()\\n\\n### Error Handling\\n\\n### Error Handling and Validation\\n\\nThe CLI provides comprehensive error handling and validation:\\n\\n- **Missing Dependencies**: ImportError handling when `langgraph-api` is not installed for `dev` command, with Python version-specific guidance\\n- **Python Version Constraints**: `validate_config()` enforces Python >= 3.11 and Node.js >= 20 requirements\\n- **Configuration Validation**: `click.UsageError` exceptions for malformed `langgraph.json` with specific field validation messages\\n- **Local Path Resolution**: `FileNotFoundError` and `NotADirectoryError` for invalid local dependency paths\\n- **Package Name Conflicts**: Reserved name checking in `_assemble_local_deps()` to prevent conflicts with system packages\\n\\nThe validation logic in `validate_config()` performs comprehensive checks including dependency requirements, graph definitions, version constraints, and path validation.\\n\\nSources: [libs/cli/langgraph_cli/cli.py:671-701](), [libs/cli/langgraph_cli/config.py:587-730]()\\n\\n## Package Structure\\n\\nThe CLI is distributed as the `langgraph-cli` package with optional dependencies for different use cases:\\n\\n### Core Installation\\n```bash\\npip install langgraph-cli\\n```\\nProvides all commands except `dev`, which requires additional runtime dependencies.\\n\\n### Development Installation  \\n```bash\\npip install \\\"langgraph-cli[inmem]\\\"\\n```\\nIncludes `langgraph-api` and `langgraph-runtime-inmem` for the in-memory development server.\\n\\nThe package entry point is defined as `langgraph = \\\"langgraph_cli.cli:cli\\\"` in the pyproject.toml.\\n\\nSources: [libs/cli/pyproject.toml:29-30](), [libs/cli/pyproject.toml:19-24]()\", \"# Page: LangGraph Platform\\n\\n# LangGraph Platform\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [.github/scripts/run_langgraph_cli_test.py](.github/scripts/run_langgraph_cli_test.py)\\n- [.github/workflows/_integration_test.yml](.github/workflows/_integration_test.yml)\\n- [.github/workflows/_lint.yml](.github/workflows/_lint.yml)\\n- [.github/workflows/_test.yml](.github/workflows/_test.yml)\\n- [.github/workflows/_test_langgraph.yml](.github/workflows/_test_langgraph.yml)\\n- [.github/workflows/_test_release.yml](.github/workflows/_test_release.yml)\\n- [.github/workflows/baseline.yml](.github/workflows/baseline.yml)\\n- [.github/workflows/bench.yml](.github/workflows/bench.yml)\\n- [.github/workflows/ci.yml](.github/workflows/ci.yml)\\n- [.github/workflows/link_check.yml](.github/workflows/link_check.yml)\\n- [.github/workflows/release.yml](.github/workflows/release.yml)\\n- [docs/docs/cloud/deployment/cloud.md](docs/docs/cloud/deployment/cloud.md)\\n- [docs/docs/cloud/deployment/setup.md](docs/docs/cloud/deployment/setup.md)\\n- [docs/docs/cloud/deployment/setup_javascript.md](docs/docs/cloud/deployment/setup_javascript.md)\\n- [docs/docs/cloud/deployment/setup_pyproject.md](docs/docs/cloud/deployment/setup_pyproject.md)\\n- [docs/docs/cloud/reference/api/openapi.json](docs/docs/cloud/reference/api/openapi.json)\\n- [docs/docs/cloud/reference/cli.md](docs/docs/cloud/reference/cli.md)\\n- [docs/docs/concepts/application_structure.md](docs/docs/concepts/application_structure.md)\\n- [docs/docs/concepts/langgraph_cli.md](docs/docs/concepts/langgraph_cli.md)\\n- [docs/docs/concepts/sdk.md](docs/docs/concepts/sdk.md)\\n- [libs/cli/Makefile](libs/cli/Makefile)\\n- [libs/cli/examples/.env.example](libs/cli/examples/.env.example)\\n- [libs/sdk-py/tests/test_select_fields_sync.py](libs/sdk-py/tests/test_select_fields_sync.py)\\n\\n</details>\\n\\n\\n\\nLangGraph Platform is a cloud Software-as-a-Service (SaaS) offering that provides managed hosting and deployment for LangGraph applications. It enables developers to deploy, manage, and scale their LangGraph graphs through a web-based interface and comprehensive REST API, with integrated support for persistence, authentication, monitoring, and CI/CD workflows.\\n\\nThis document covers the cloud platform's architecture, deployment model, API services, and management capabilities. For local development tooling, see [CLI Tool](#7.1). For self-hosted deployment options, see [Self-Hosted Deployment](#7.3).\\n\\n## Platform Architecture\\n\\nLangGraph Platform operates as a managed service that hosts LangGraph applications with full lifecycle management capabilities. The platform integrates with GitHub repositories for source code management and provides automated build and deployment pipelines.\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"GitHub Integration\\\"\\n        REPO[\\\"GitHub Repository\\\"]\\n        WEBHOOK[\\\"GitHub Webhooks\\\"]\\n    end\\n    \\n    subgraph \\\"LangGraph Platform\\\"\\n        LANGSMITH[\\\"LangSmith UI\\\"]\\n        DEPLOY_MGR[\\\"Deployment Manager\\\"]\\n        BUILD_SVC[\\\"Build Service\\\"]\\n        API_GW[\\\"API Gateway\\\"]\\n    end\\n    \\n    subgraph \\\"Application Runtime\\\"\\n        RUNTIME[\\\"LangGraph Server\\\"]\\n        POSTGRES[\\\"PostgreSQL Database\\\"]\\n        REDIS[\\\"Redis Cache\\\"]\\n        STORE[\\\"BaseStore with Vector Search\\\"]\\n    end\\n    \\n    subgraph \\\"Monitoring & Observability\\\"\\n        METRICS[\\\"Platform Metrics\\\"]\\n        LOGS[\\\"Application Logs\\\"]\\n        TRACES[\\\"LangSmith Traces\\\"]\\n    end\\n    \\n    REPO --> WEBHOOK\\n    WEBHOOK --> DEPLOY_MGR\\n    LANGSMITH --> DEPLOY_MGR\\n    DEPLOY_MGR --> BUILD_SVC\\n    BUILD_SVC --> RUNTIME\\n    API_GW --> RUNTIME\\n    RUNTIME --> POSTGRES\\n    RUNTIME --> REDIS\\n    RUNTIME --> STORE\\n    RUNTIME --> TRACES\\n    DEPLOY_MGR --> METRICS\\n    RUNTIME --> LOGS\\n```\\n\\n**LangGraph Platform Core Architecture**\\n\\nSources: [docs/docs/cloud/deployment/cloud.md:1-129](), [docs/docs/cloud/reference/api/openapi.json:1-50]()\\n\\n## Deployment Workflow\\n\\nThe platform uses a GitHub-based deployment model where applications are built from repository code and deployed through the LangSmith interface. Each deployment consists of multiple revisions that can be managed independently.\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Development\\\"\\n        DEV[\\\"Developer\\\"]\\n        LANGGRAPH_JSON[\\\"langgraph.json\\\"]\\n        PYPROJECT[\\\"pyproject.toml\\\"]\\n        REQUIREMENTS[\\\"requirements.txt\\\"]\\n        ENV_FILE[\\\".env\\\"]\\n    end\\n    \\n    subgraph \\\"Source Control\\\"\\n        GITHUB_REPO[\\\"GitHub Repository\\\"]\\n        GITHUB_APP[\\\"hosted-langserve GitHub App\\\"]\\n    end\\n    \\n    subgraph \\\"Platform Deployment\\\"\\n        LANGSMITH_UI[\\\"LangSmith UI\\\"]\\n        CREATE_DEPLOYMENT[\\\"Create New Deployment\\\"]\\n        BUILD_IMAGE[\\\"Docker Image Build\\\"]\\n        PROVISION_RUNTIME[\\\"Runtime Provisioning\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Environment\\\"\\n        DEPLOYED_APP[\\\"Deployed Application\\\"]\\n        ASSISTANT_API[\\\"/assistants API\\\"]\\n        THREAD_API[\\\"/threads API\\\"]\\n        RUN_API[\\\"/runs API\\\"]\\n    end\\n    \\n    DEV --> LANGGRAPH_JSON\\n    DEV --> PYPROJECT\\n    LANGGRAPH_JSON --> GITHUB_REPO\\n    PYPROJECT --> GITHUB_REPO\\n    GITHUB_REPO --> GITHUB_APP\\n    GITHUB_APP --> LANGSMITH_UI\\n    LANGSMITH_UI --> CREATE_DEPLOYMENT\\n    CREATE_DEPLOYMENT --> BUILD_IMAGE\\n    BUILD_IMAGE --> PROVISION_RUNTIME\\n    PROVISION_RUNTIME --> DEPLOYED_APP\\n    DEPLOYED_APP --> ASSISTANT_API\\n    DEPLOYED_APP --> THREAD_API\\n    DEPLOYED_APP --> RUN_API\\n```\\n\\n**GitHub to Production Deployment Flow**\\n\\nSources: [docs/docs/cloud/deployment/cloud.md:10-54](), [docs/docs/cloud/deployment/setup.md:148-184]()\\n\\n## API Services and Endpoints\\n\\nThe platform exposes a comprehensive REST API organized into several service categories. Each deployed application provides these standardized endpoints for graph execution and management.\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Core Graph Services\\\"\\n        ASSISTANTS[\\\"/assistants<br/>Assistant Management\\\"]\\n        THREADS[\\\"/threads<br/>Thread Management\\\"] \\n        RUNS[\\\"/runs<br/>Run Execution\\\"]\\n        STATELESS[\\\"/runs/stateless<br/>Stateless Execution\\\"]\\n    end\\n    \\n    subgraph \\\"Data Services\\\"\\n        STORE[\\\"/store<br/>Persistent Key-Value Store\\\"]\\n        STATE[\\\"/threads/{thread_id}/state<br/>Thread State Management\\\"]\\n        CHECKPOINTS[\\\"/threads/{thread_id}/state/{checkpoint_id}<br/>Checkpoint Access\\\"]\\n    end\\n    \\n    subgraph \\\"Advanced Services\\\"\\n        CRONS[\\\"/runs/crons<br/>Scheduled Runs\\\"]\\n        MCP[\\\"/mcp<br/>Model Context Protocol\\\"]\\n        WEBHOOKS[\\\"Webhook Integration\\\"]\\n    end\\n    \\n    subgraph \\\"System Services\\\"\\n        OK[\\\"/ok<br/>Health Check\\\"]\\n        INFO[\\\"/info<br/>Server Information\\\"]\\n        METRICS[\\\"/metrics<br/>Performance Metrics\\\"]\\n        DOCS[\\\"/docs<br/>API Documentation\\\"]\\n    end\\n    \\n    ASSISTANTS --> THREADS\\n    THREADS --> RUNS\\n    THREADS --> STATE\\n    STATE --> CHECKPOINTS\\n    STORE -.-> THREADS\\n    CRONS --> RUNS\\n```\\n\\n**LangGraph Platform API Service Architecture**\\n\\nSources: [docs/docs/cloud/reference/api/openapi.json:7-40](), [docs/docs/cloud/reference/api/openapi.json:41-103]()\\n\\n### Assistant Management\\n\\nAssistants represent configured instances of graphs that can be invoked multiple times. The platform provides full CRUD operations for assistant lifecycle management.\\n\\n| Endpoint | Method | Purpose |\\n|----------|--------|---------|\\n| `/assistants` | POST | Create new assistant |\\n| `/assistants/search` | POST | Search and list assistants |\\n| `/assistants/{assistant_id}` | GET, PATCH, DELETE | Manage individual assistants |\\n| `/assistants/{assistant_id}/graph` | GET | Retrieve graph structure |\\n| `/assistants/{assistant_id}/versions` | POST | Manage assistant versions |\\n\\nSources: [docs/docs/cloud/reference/api/openapi.json:42-770]()\\n\\n### Thread and Run Management\\n\\nThreads maintain conversation state across multiple runs, while runs represent individual graph executions. The platform supports both stateful and stateless execution modes.\\n\\n| Endpoint | Method | Purpose |\\n|----------|--------|---------|\\n| `/threads` | POST | Create new thread |\\n| `/threads/search` | POST | Search threads |\\n| `/threads/{thread_id}/runs` | POST, GET | Create and list runs |\\n| `/threads/{thread_id}/runs/{run_id}` | GET, PATCH | Manage individual runs |\\n| `/runs/stateless` | POST | Execute without state persistence |\\n\\nSources: [docs/docs/cloud/reference/api/openapi.json:771-1500]()\\n\\n## Configuration Management\\n\\nApplications are configured through the `langgraph.json` file which defines dependencies, graphs, environment variables, and platform-specific settings. The platform supports both Python and JavaScript applications with different configuration schemas.\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Configuration Structure\\\"\\n        LANGGRAPH_JSON[\\\"langgraph.json\\\"]\\n        DEPENDENCIES[\\\"dependencies[]\\\"]\\n        GRAPHS[\\\"graphs{}\\\"]\\n        ENV[\\\"env\\\"]\\n        AUTH[\\\"auth\\\"]\\n    end\\n    \\n    subgraph \\\"Python Configuration\\\"\\n        PY_DEPS[\\\"Python packages<br/>pyproject.toml<br/>requirements.txt\\\"]\\n        PY_VERSION[\\\"python_version: 3.11|3.12|3.13\\\"]\\n        PIP_CONFIG[\\\"pip_config_file\\\"]\\n        BASE_IMAGE[\\\"base_image\\\"]\\n    end\\n    \\n    subgraph \\\"JavaScript Configuration\\\" \\n        JS_DEPS[\\\"Node packages<br/>package.json\\\"]\\n        NODE_VERSION[\\\"node_version: 20\\\"]\\n        JS_BASE[\\\"langchain/langgraphjs-api\\\"]\\n    end\\n    \\n    subgraph \\\"Platform Features\\\"\\n        STORE_CONFIG[\\\"store.index<br/>store.ttl\\\"]\\n        CHECKPOINTER_CONFIG[\\\"checkpointer.ttl\\\"]\\n        HTTP_CONFIG[\\\"http.cors<br/>http.mount_prefix\\\"]\\n        DOCKERFILE_LINES[\\\"dockerfile_lines[]\\\"]\\n    end\\n    \\n    LANGGRAPH_JSON --> DEPENDENCIES\\n    LANGGRAPH_JSON --> GRAPHS\\n    LANGGRAPH_JSON --> ENV\\n    LANGGRAPH_JSON --> AUTH\\n    \\n    DEPENDENCIES --> PY_DEPS\\n    DEPENDENCIES --> JS_DEPS\\n    ENV --> PY_VERSION\\n    ENV --> NODE_VERSION\\n    \\n    LANGGRAPH_JSON --> STORE_CONFIG\\n    LANGGRAPH_JSON --> CHECKPOINTER_CONFIG\\n    LANGGRAPH_JSON --> HTTP_CONFIG\\n```\\n\\n**Configuration Schema and Platform Features**\\n\\nSources: [docs/docs/cloud/reference/cli.md:29-57](), [docs/docs/cloud/reference/cli.md:70-258]()\\n\\n### Key Configuration Properties\\n\\n| Property | Description | Example |\\n|----------|-------------|---------|\\n| `dependencies` | Package dependencies or local paths | `[\\\".\\\"]`, `[\\\"langchain_openai\\\"]` |\\n| `graphs` | Mapping of graph IDs to implementation paths | `{\\\"agent\\\": \\\"./agent.py:graph\\\"}` |\\n| `env` | Environment variables file or inline values | `\\\".env\\\"` or `{\\\"KEY\\\": \\\"value\\\"}` |\\n| `store.index` | Semantic search configuration | `{\\\"embed\\\": \\\"openai:text-embedding-3-small\\\"}` |\\n| `checkpointer.ttl` | Checkpoint time-to-live settings | `{\\\"default_ttl\\\": 43200}` |\\n| `http.cors` | CORS configuration for API access | `{\\\"allow_origins\\\": [\\\"*\\\"]}` |\\n\\nSources: [docs/docs/cloud/reference/cli.md:40-57]()\\n\\n## Platform Features\\n\\n### Persistence and Storage\\n\\nThe platform provides managed PostgreSQL databases for checkpointing and Redis for caching. The `BaseStore` supports vector search capabilities for semantic memory storage.\\n\\n| Feature | Configuration | Purpose |\\n|---------|---------------|---------|\\n| Checkpointing | `checkpointer.ttl` | Automatic state persistence with TTL |\\n| Vector Store | `store.index.embed` | Semantic search over stored documents |\\n| TTL Management | `store.ttl.default_ttl` | Automatic data expiration |\\n| Sweeping | `sweep_interval_minutes` | Background cleanup processes |\\n\\nSources: [docs/docs/cloud/reference/cli.md:192-246]()\\n\\n### Authentication and Security\\n\\nThe platform supports custom authentication handlers and configurable security policies through the `auth` configuration section.\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Authentication Flow\\\"\\n        CLIENT[\\\"Client Request\\\"]\\n        AUTH_HANDLER[\\\"Custom Auth Handler\\\"]\\n        PLATFORM_AUTH[\\\"Platform Authentication\\\"]\\n        GRAPH_ACCESS[\\\"Graph Access\\\"]\\n    end\\n    \\n    subgraph \\\"Configuration\\\"\\n        AUTH_CONFIG[\\\"auth.path\\\"]\\n        OPENAPI_CONFIG[\\\"auth.openapi\\\"]\\n        STUDIO_AUTH[\\\"auth.disable_studio_auth\\\"]\\n    end\\n    \\n    CLIENT --> AUTH_HANDLER\\n    AUTH_HANDLER --> PLATFORM_AUTH\\n    PLATFORM_AUTH --> GRAPH_ACCESS\\n    \\n    AUTH_CONFIG --> AUTH_HANDLER\\n    OPENAPI_CONFIG --> AUTH_HANDLER\\n    STUDIO_AUTH --> PLATFORM_AUTH\\n```\\n\\n**Authentication Architecture**\\n\\nSources: [docs/docs/cloud/reference/cli.md:165-190]()\\n\\n### Monitoring and Observability\\n\\nThe platform provides comprehensive monitoring through metrics endpoints, application logs, and LangSmith tracing integration.\\n\\n| Endpoint | Purpose |\\n|----------|---------|\\n| `/ok` | Health check status |\\n| `/info` | Server information and configuration |\\n| `/metrics` | Performance and usage metrics |\\n| `/docs` | Generated API documentation |\\n\\nSources: [docs/docs/cloud/reference/api/openapi.json:37-39]()\\n\\n## Deployment Types and Scaling\\n\\nThe platform offers two deployment tiers with different resource allocations and capabilities.\\n\\n| Deployment Type | Use Case | Resources | Features |\\n|----------------|----------|-----------|----------|\\n| Development | Non-production testing | Minimal resources | Basic functionality |\\n| Production | High-traffic applications | Up to 500 req/sec | High availability, automatic backups |\\n\\nSources: [docs/docs/cloud/deployment/cloud.md:23-26]()\\n\\n### IP Whitelisting\\n\\nProduction deployments route traffic through NAT gateways with static IP addresses for integration with external services requiring IP whitelisting.\\n\\n| Region | Static IP Addresses |\\n|--------|-------------------|\\n| US | `35.197.29.146`, `34.145.102.123`, `34.169.45.153`, `34.82.222.17` |\\n| EU | `34.90.213.236`, `34.13.244.114`, `34.32.180.189`, `34.34.69.108` |\\n\\nSources: [docs/docs/cloud/deployment/cloud.md:116-128]()\", \"# Page: Self-Hosted Deployment\\n\\n# Self-Hosted Deployment\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/deployment/cloud.md](docs/docs/cloud/deployment/cloud.md)\\n- [docs/docs/cloud/deployment/setup.md](docs/docs/cloud/deployment/setup.md)\\n- [docs/docs/cloud/deployment/setup_javascript.md](docs/docs/cloud/deployment/setup_javascript.md)\\n- [docs/docs/cloud/deployment/setup_pyproject.md](docs/docs/cloud/deployment/setup_pyproject.md)\\n- [docs/docs/cloud/reference/cli.md](docs/docs/cloud/reference/cli.md)\\n- [docs/docs/concepts/application_structure.md](docs/docs/concepts/application_structure.md)\\n- [docs/docs/concepts/langgraph_cli.md](docs/docs/concepts/langgraph_cli.md)\\n- [docs/docs/concepts/sdk.md](docs/docs/concepts/sdk.md)\\n\\n</details>\\n\\n\\n\\nThis document covers deploying LangGraph applications to your own infrastructure using Docker containers and custom deployment configurations. For information about the LangGraph CLI tools themselves, see [CLI Tool](#7.1). For deploying to the managed cloud service, see [LangGraph Platform](#7.2).\\n\\n## Overview\\n\\nSelf-hosted deployment allows you to run LangGraph applications on your own infrastructure while maintaining full control over the runtime environment, data persistence, and scaling. The deployment process centers around Docker containerization using the LangGraph CLI to build production-ready images.\\n\\nSelf-hosted deployment provides several key capabilities:\\n\\n- **Container-based deployment** using `langgraph build` to create Docker images\\n- **Local development environment** using `langgraph up` with Docker Compose orchestration  \\n- **Custom infrastructure integration** through generated Dockerfiles and configuration\\n- **Production-ready persistence** with PostgreSQL, SQLite, or Redis backends\\n- **Flexible base image selection** supporting both Debian and Wolfi Linux distributions\\n\\n## Docker-Based Deployment Workflow\\n\\n### Build and Deploy Process\\n\\n```mermaid\\nflowchart TD\\n    subgraph \\\"Development Environment\\\"\\n        config[\\\"langgraph.json<br/>Configuration File\\\"]\\n        deps[\\\"dependencies<br/>(requirements.txt, pyproject.toml)\\\"]\\n        graphs[\\\"Graph Definitions<br/>(./my_agent/agent.py:graph)\\\"]\\n        env[\\\".env<br/>Environment Variables\\\"]\\n    end\\n    \\n    subgraph \\\"CLI Build Commands\\\"\\n        build[\\\"langgraph build<br/>-t my-image\\\"]\\n        dockerfile_cmd[\\\"langgraph dockerfile<br/>Dockerfile\\\"]\\n        up[\\\"langgraph up<br/>--image my-image\\\"]\\n    end\\n    \\n    subgraph \\\"Docker Infrastructure\\\"\\n        base_image[\\\"langchain/langgraph-api:3.11<br/>Base Image\\\"]\\n        custom_image[\\\"my-image<br/>Custom Built Image\\\"]\\n        container[\\\"Running Container<br/>Port 8123\\\"]\\n    end\\n    \\n    subgraph \\\"Production Environment\\\"\\n        postgres[\\\"PostgreSQL<br/>Checkpointer Backend\\\"]\\n        redis[\\\"Redis<br/>Cache Layer\\\"]\\n        load_balancer[\\\"Load Balancer<br/>Multiple Instances\\\"]\\n    end\\n    \\n    config --> build\\n    deps --> build\\n    graphs --> build\\n    env --> build\\n    \\n    build --> custom_image\\n    dockerfile_cmd --> base_image\\n    up --> container\\n    \\n    custom_image --> container\\n    base_image --> custom_image\\n    \\n    container --> postgres\\n    container --> redis\\n    container --> load_balancer\\n```\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:352-476](), [docs/docs/cloud/deployment/setup.md:1-189]()*\\n\\n### Container Architecture Components\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"langgraph-api Container\\\"\\n        api_server[\\\"LangGraph API Server<br/>uvicorn + FastAPI\\\"]\\n        pregel_engine[\\\"Pregel Runtime Engine<br/>StateGraph Execution\\\"]\\n        checkpoint_saver[\\\"CheckpointSaver<br/>BaseCheckpointSaver Interface\\\"]\\n    end\\n    \\n    subgraph \\\"Configuration Mounting\\\"\\n        langgraph_json[\\\"langgraph.json<br/>Graph + Dependency Config\\\"]\\n        env_vars[\\\".env Variables<br/>OPENAI_API_KEY, etc.\\\"]\\n        graph_files[\\\"./my_agent/agent.py:graph<br/>Compiled StateGraph\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Backends\\\"\\n        postgres_uri[\\\"--postgres-uri<br/>External PostgreSQL\\\"]\\n        sqlite_local[\\\"Local SQLite<br/>In-Container Storage\\\"]\\n        redis_cache[\\\"Redis Backend<br/>BaseCache Implementation\\\"]\\n    end\\n    \\n    subgraph \\\"Network & Volumes\\\"\\n        port_mapping[\\\"Port 8123:8123<br/>API Endpoint Exposure\\\"]\\n        volume_mount[\\\"/app Volume<br/>Code + Dependencies\\\"]\\n        docker_compose[\\\"docker-compose.yml<br/>Additional Services\\\"]\\n    end\\n    \\n    langgraph_json --> api_server\\n    env_vars --> api_server\\n    graph_files --> pregel_engine\\n    \\n    checkpoint_saver --> postgres_uri\\n    checkpoint_saver --> sqlite_local\\n    api_server --> redis_cache\\n    \\n    api_server --> port_mapping\\n    graph_files --> volume_mount\\n    postgres_uri --> docker_compose\\n```\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:395-451](), [docs/docs/concepts/langgraph_cli.md:61-72]()*\\n\\n## Building Production Images\\n\\n### Using langgraph build Command\\n\\nThe `langgraph build` command creates production-ready Docker images from your LangGraph application:\\n\\n```bash\\nlanggraph build -t my-langgraph-app --platform linux/amd64,linux/arm64\\n```\\n\\nKey build options include:\\n\\n| Option | Description |\\n|--------|-------------|\\n| `-t, --tag` | Docker image tag (required) |\\n| `--platform` | Target platform(s) for multi-arch builds |\\n| `--pull / --no-pull` | Pull latest base images vs use local |\\n| `-c, --config` | Path to `langgraph.json` configuration |\\n\\nThe build process uses base images from the `langchain/langgraph-api` registry with support for Python versions 3.11, 3.12, and 3.13.\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:352-393]()*\\n\\n### Custom Dockerfile Generation\\n\\nFor advanced customization, generate a Dockerfile using:\\n\\n```bash\\nlanggraph dockerfile -c langgraph.json Dockerfile\\n```\\n\\nThis creates a customizable Dockerfile starting from:\\n\\n```dockerfile\\nFROM langchain/langgraph-api:3.11\\nADD ./pipconf.txt /pipconfig.txt\\nRUN PIP_CONFIG_FILE=/pipconfig.txt PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt langchain_community langchain_anthropic\\n```\\n\\nThe generated Dockerfile can be modified to add custom system dependencies, environment setup, or deployment-specific configurations.\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:452-493]()*\\n\\n### Base Image and Distribution Options\\n\\nConfigure base image settings in `langgraph.json`:\\n\\n```json\\n{\\n  \\\"base_image\\\": \\\"langchain/langgraph-server:0.2\\\",\\n  \\\"image_distro\\\": \\\"wolfi\\\",\\n  \\\"python_version\\\": \\\"3.12\\\",\\n  \\\"dockerfile_lines\\\": [\\n    \\\"RUN apt-get update && apt-get install -y curl\\\",\\n    \\\"COPY ./custom-config /app/config\\\"\\n  ]\\n}\\n```\\n\\nAvailable options:\\n- **Image distributions**: `\\\"debian\\\"` (default) or `\\\"wolfi\\\"` for smaller, more secure images\\n- **Python versions**: `\\\"3.11\\\"`, `\\\"3.12\\\"`, or `\\\"3.13\\\"`\\n- **Custom base images**: Pin to specific versions like `\\\"langchain/langgraph-server:0.2\\\"`\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:85-97](), [docs/docs/cloud/reference/cli.md:45-46]()*\\n\\n## Local Development and Testing\\n\\n### Running with langgraph up\\n\\nThe `langgraph up` command orchestrates a complete local deployment using Docker Compose:\\n\\n```bash\\nlanggraph up --port 8000 --postgres-uri postgresql://user:pass@localhost:5432/db\\n```\\n\\nThis command:\\n- Builds or pulls the specified Docker image\\n- Sets up PostgreSQL database for checkpointing\\n- Configures networking and volume mounts\\n- Exposes the API server on the specified port\\n\\nKey runtime options:\\n\\n| Option | Default | Description |\\n|--------|---------|-------------|\\n| `-p, --port` | `8123` | API server port |\\n| `--postgres-uri` | Local database | External PostgreSQL connection |\\n| `--watch` | - | Restart on file changes |\\n| `--image` | - | Use pre-built image instead of building |\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:395-451]()*\\n\\n### Docker Compose Integration\\n\\nUse `-d, --docker-compose FILE` to launch additional services alongside your LangGraph application:\\n\\n```yaml\\n# docker-compose.override.yml\\nservices:\\n  redis:\\n    image: redis:alpine\\n    ports:\\n      - \\\"6379:6379\\\"\\n  \\n  postgresql:\\n    image: postgres:15\\n    environment:\\n      POSTGRES_DB: langgraph\\n      POSTGRES_USER: postgres\\n      POSTGRES_PASSWORD: password\\n```\\n\\nThis allows integration with external databases, caching layers, and monitoring services.\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:420](), [docs/docs/cloud/reference/cli.md:446]()*\\n\\n## Production Deployment Configuration\\n\\n### Environment Variables and Secrets\\n\\nConfigure production environment variables through multiple mechanisms:\\n\\n1. **Direct configuration** in `langgraph.json`:\\n```json\\n{\\n  \\\"env\\\": {\\n    \\\"POSTGRES_URI\\\": \\\"postgresql://prod-host:5432/langgraph\\\",\\n    \\\"REDIS_URL\\\": \\\"redis://redis-cluster:6379\\\"\\n  }\\n}\\n```\\n\\n2. **External .env file**:\\n```json\\n{\\n  \\\"env\\\": \\\".env.production\\\"\\n}\\n```\\n\\n3. **Container environment** through Docker deployment orchestration\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:47](), [docs/docs/cloud/deployment/setup.md:76-95]()*\\n\\n### Persistence Backend Configuration\\n\\n#### PostgreSQL Checkpointer\\n\\nConfigure PostgreSQL for production checkpointing:\\n\\n```json\\n{\\n  \\\"dependencies\\\": [\\\"langgraph-checkpoint-postgres\\\"],\\n  \\\"env\\\": {\\n    \\\"POSTGRES_URI\\\": \\\"postgresql://user:password@postgres-host:5432/langgraph\\\"\\n  }\\n}\\n```\\n\\n#### Redis Cache Integration\\n\\nEnable Redis caching for improved performance:\\n\\n```json\\n{\\n  \\\"dependencies\\\": [\\\"redis\\\"],\\n  \\\"env\\\": {\\n    \\\"REDIS_URL\\\": \\\"redis://redis-cluster:6379/0\\\"\\n  }\\n}\\n```\\n\\n*Sources: [docs/docs/cloud/deployment/setup.md:32-55]()*\\n\\n### Time-to-Live (TTL) Configuration\\n\\nConfigure automatic data expiration for production deployments:\\n\\n```json\\n{\\n  \\\"store\\\": {\\n    \\\"ttl\\\": {\\n      \\\"refresh_on_read\\\": true,\\n      \\\"sweep_interval_minutes\\\": 60,\\n      \\\"default_ttl\\\": 10080\\n    }\\n  },\\n  \\\"checkpointer\\\": {\\n    \\\"ttl\\\": {\\n      \\\"strategy\\\": \\\"delete\\\",\\n      \\\"sweep_interval_minutes\\\": 10,\\n      \\\"default_ttl\\\": 43200\\n    }\\n  }\\n}\\n```\\n\\nTTL settings control:\\n- **Store TTL**: Memory/storage cleanup for `BaseStore` data\\n- **Checkpoint TTL**: Automatic checkpoint deletion after expiration\\n- **Sweep intervals**: Background cleanup frequency\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:192-247]()*\\n\\n## Container Orchestration and Scaling\\n\\n### Multi-Instance Deployment\\n\\nDeploy multiple container instances for high availability:\\n\\n```yaml\\n# kubernetes-deployment.yml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: langgraph-api\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: langgraph-api\\n  template:\\n    spec:\\n      containers:\\n      - name: langgraph-api\\n        image: my-langgraph-app:latest\\n        ports:\\n        - containerPort: 8123\\n        env:\\n        - name: POSTGRES_URI\\n          valueFrom:\\n            secretKeyRef:\\n              name: postgres-secret\\n              key: uri\\n```\\n\\n### Load Balancing and Service Discovery\\n\\nConfigure external load balancing for container orchestration:\\n\\n- **Port exposure**: Map container port 8123 to external load balancer\\n- **Health checks**: Use `/ok` endpoint for container health monitoring  \\n- **Service discovery**: Register container instances with orchestration platform\\n\\n*Sources: [docs/docs/cloud/reference/cli.md:421](), [docs/docs/concepts/langgraph_cli.md:8]()*\", \"# Page: Authentication and Authorization\\n\\n# Authentication and Authorization\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/reference/sdk/python_sdk_ref.md](docs/docs/cloud/reference/sdk/python_sdk_ref.md)\\n- [libs/checkpoint/tests/test_redis_cache.py](libs/checkpoint/tests/test_redis_cache.py)\\n- [libs/sdk-py/Makefile](libs/sdk-py/Makefile)\\n- [libs/sdk-py/langgraph_sdk/__init__.py](libs/sdk-py/langgraph_sdk/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/__init__.py](libs/sdk-py/langgraph_sdk/auth/__init__.py)\\n- [libs/sdk-py/langgraph_sdk/auth/exceptions.py](libs/sdk-py/langgraph_sdk/auth/exceptions.py)\\n- [libs/sdk-py/langgraph_sdk/auth/types.py](libs/sdk-py/langgraph_sdk/auth/types.py)\\n- [libs/sdk-py/langgraph_sdk/client.py](libs/sdk-py/langgraph_sdk/client.py)\\n- [libs/sdk-py/langgraph_sdk/schema.py](libs/sdk-py/langgraph_sdk/schema.py)\\n- [libs/sdk-py/langgraph_sdk/sse.py](libs/sdk-py/langgraph_sdk/sse.py)\\n- [libs/sdk-py/pyproject.toml](libs/sdk-py/pyproject.toml)\\n- [libs/sdk-py/tests/test_api_parity.py](libs/sdk-py/tests/test_api_parity.py)\\n- [libs/sdk-py/uv.lock](libs/sdk-py/uv.lock)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's authentication and authorization system, which provides secure access control for LangGraph applications and APIs. The system supports both client-side API key authentication and server-side authorization with fine-grained resource access control.\\n\\nFor information about deployment configuration including auth settings, see [Deployment and Platform](#7). For details on the Python SDK client implementation, see [Python SDK](#6.2).\\n\\n## Authentication Mechanisms\\n\\nLangGraph provides multiple authentication mechanisms for different deployment scenarios:\\n\\n### API Key Authentication\\n\\nThe primary authentication method uses API keys passed via HTTP headers. The client SDK automatically handles API key resolution from multiple environment variable sources.\\n\\n**API Key Resolution Flow**\\n```mermaid\\nflowchart TD\\n    A[\\\"Client Request\\\"] --> B[\\\"_get_api_key()\\\"]\\n    B --> C{\\\"Explicit api_key?\\\"}\\n    C -->|Yes| D[\\\"Use Provided Key\\\"]\\n    C -->|No| E[\\\"Check LANGGRAPH_API_KEY\\\"]\\n    E --> F{\\\"Found?\\\"}\\n    F -->|Yes| G[\\\"Use LANGGRAPH_API_KEY\\\"]\\n    F -->|No| H[\\\"Check LANGSMITH_API_KEY\\\"]\\n    H --> I{\\\"Found?\\\"}\\n    I -->|Yes| J[\\\"Use LANGSMITH_API_KEY\\\"]\\n    I -->|No| K[\\\"Check LANGCHAIN_API_KEY\\\"]\\n    K --> L{\\\"Found?\\\"}\\n    L -->|Yes| M[\\\"Use LANGCHAIN_API_KEY\\\"]\\n    L -->|No| N[\\\"No API Key\\\"]\\n    D --> O[\\\"Add x-api-key Header\\\"]\\n    G --> O\\n    J --> O\\n    M --> O\\n    O --> P[\\\"HTTP Request\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:84-117]()\\n\\n### Custom Authentication Handlers\\n\\nServer-side applications can implement custom authentication using the `@auth.authenticate` decorator, which supports various request parameters and user representations.\\n\\n```mermaid\\nflowchart LR\\n    A[\\\"HTTP Request\\\"] --> B[\\\"@auth.authenticate Handler\\\"]\\n    B --> C[\\\"Extract Credentials\\\"]\\n    C --> D[\\\"Verify User\\\"]\\n    D --> E[\\\"Return User Object\\\"]\\n    E --> F[\\\"AuthContext Creation\\\"]\\n    F --> G[\\\"Authorization Check\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:181-254](), [libs/sdk-py/langgraph_sdk/auth/types.py:259-346]()\\n\\n## Authorization Framework\\n\\nThe authorization system provides fine-grained access control through the `Auth` class, supporting resource-specific and action-specific authorization handlers.\\n\\n### Core Authorization Architecture\\n\\n```mermaid\\nflowchart TD\\n    A[\\\"Request\\\"] --> B[\\\"Authentication\\\"]\\n    B --> C[\\\"AuthContext Creation\\\"]\\n    C --> D[\\\"Handler Resolution\\\"]\\n    D --> E{\\\"Specific Handler?\\\"}\\n    E -->|Yes| F[\\\"Resource.Action Handler\\\"]\\n    E -->|No| G{\\\"Resource Handler?\\\"}\\n    G -->|Yes| H[\\\"Resource Handler\\\"]\\n    G -->|No| I{\\\"Global Handler?\\\"}\\n    I -->|Yes| J[\\\"Global Handler\\\"]\\n    I -->|No| K[\\\"Accept Request\\\"]\\n    F --> L[\\\"Handler Result\\\"]\\n    H --> L\\n    J --> L\\n    L --> M{\\\"Result Type?\\\"}\\n    M -->|\\\"None/True\\\"| N[\\\"Accept\\\"]\\n    M -->|\\\"False\\\"| O[\\\"403 Forbidden\\\"]\\n    M -->|\\\"FilterType\\\"| P[\\\"Apply Filter\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:110-180](), [libs/sdk-py/langgraph_sdk/auth/types.py:364-401]()\\n\\n### Resource and Action Mapping\\n\\nThe authorization system covers five main resources with specific actions:\\n\\n| Resource | Actions | Type Definition |\\n|----------|---------|----------------|\\n| `assistants` | create, read, update, delete, search | `AssistantsCreate`, `AssistantsRead`, etc. |\\n| `threads` | create, read, update, delete, search, create_run | `ThreadsCreate`, `ThreadsRead`, etc. |\\n| `crons` | create, read, update, delete, search | `CronsCreate`, `CronsRead`, etc. |\\n| `runs` | create_run (via threads) | `RunsCreate` |\\n| `store` | put, get, search, list_namespaces, delete | `StorePut`, `StoreGet`, etc. |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/types.py:403-880]()\\n\\n## Client Authentication Implementation\\n\\n### HTTP Client Integration\\n\\nThe `LangGraphClient` automatically handles authentication through the `HttpClient` wrapper, which adds authentication headers to all requests.\\n\\n```mermaid\\nflowchart LR\\n    A[\\\"get_client()\\\"] --> B[\\\"_get_headers()\\\"]\\n    B --> C[\\\"API Key Resolution\\\"]\\n    C --> D[\\\"User-Agent Header\\\"]\\n    D --> E[\\\"Custom Headers\\\"]\\n    E --> F[\\\"httpx.AsyncClient\\\"]\\n    F --> G[\\\"LangGraphClient\\\"]\\n    G --> H[\\\"HttpClient Wrapper\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:152-221](), [libs/sdk-py/langgraph_sdk/client.py:261-273]()\\n\\n### Header Management\\n\\nThe client enforces header security by preventing override of reserved headers and automatically adding authentication information.\\n\\n| Header | Purpose | Source |\\n|--------|---------|--------|\\n| `x-api-key` | API authentication | Environment variables or explicit parameter |\\n| `User-Agent` | Client identification | `langgraph-sdk-py/{version}` |\\n| Custom headers | User-defined | Passed through with validation |\\n\\n**Reserved Headers:** `x-api-key` cannot be overridden in custom headers.\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/client.py:81-117]()\\n\\n## Server-Side Authorization\\n\\n### Auth Class Usage\\n\\nThe `Auth` class provides a declarative way to define authentication and authorization rules for LangGraph applications.\\n\\n**Basic Auth Setup**\\n```mermaid\\nflowchart TD\\n    A[\\\"langgraph.json\\\"] --> B[\\\"auth.path Configuration\\\"]\\n    B --> C[\\\"Auth Instance Creation\\\"]\\n    C --> D[\\\"@auth.authenticate Registration\\\"]\\n    C --> E[\\\"@auth.on Handler Registration\\\"]\\n    D --> F[\\\"Authentication Logic\\\"]\\n    E --> G[\\\"Authorization Logic\\\"]\\n    F --> H[\\\"Server Integration\\\"]\\n    G --> H\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:26-88]()\\n\\n### Handler Registration Patterns\\n\\nThe authorization system supports multiple handler registration patterns for different granularities of control:\\n\\n```mermaid\\nflowchart TD\\n    A[\\\"@auth.on\\\"] --> B[\\\"Global Handler\\\"]\\n    A --> C[\\\"@auth.on.threads\\\"]\\n    A --> D[\\\"@auth.on.assistants\\\"]\\n    A --> E[\\\"@auth.on.store\\\"]\\n    C --> F[\\\"@auth.on.threads.create\\\"]\\n    C --> G[\\\"@auth.on.threads.read\\\"]\\n    D --> H[\\\"@auth.on.assistants.update\\\"]\\n    D --> I[\\\"@auth.on.assistants.delete\\\"]\\n    E --> J[\\\"Store Operations\\\"]\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:545-671]()\\n\\n### Authorization Context and Results\\n\\nAuthorization handlers receive an `AuthContext` containing user information and request details, and return results that control access:\\n\\n| Result Type | Behavior | Use Case |\\n|-------------|----------|----------|\\n| `None` or `True` | Accept request | Allow access |\\n| `False` | Return 403 Forbidden | Deny access |\\n| `FilterType` | Apply filtering | Selective access with data filtering |\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/types.py:119-124](), [libs/sdk-py/langgraph_sdk/auth/types.py:364-401]()\\n\\n## User Types and Authentication Context\\n\\n### User Protocols and Types\\n\\nThe authentication system supports multiple user representations through protocols and typed dictionaries:\\n\\n```mermaid\\nclassDiagram\\n    class MinimalUser {\\n        <<Protocol>>\\n        +identity: str\\n    }\\n    class BaseUser {\\n        <<Protocol>>\\n        +identity: str\\n        +display_name: str\\n        +is_authenticated: bool\\n        +permissions: Sequence[str]\\n    }\\n    class StudioUser {\\n        +username: str\\n        +is_authenticated: bool\\n        +permissions: Sequence[str]\\n    }\\n    class MinimalUserDict {\\n        <<TypedDict>>\\n        +identity: str\\n        +display_name: str\\n        +is_authenticated: bool\\n        +permissions: Sequence[str]\\n    }\\n    \\n    MinimalUser --|> BaseUser\\n    BaseUser <|.. StudioUser\\n    BaseUser <|.. MinimalUserDict\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/types.py:140-207](), [libs/sdk-py/langgraph_sdk/auth/types.py:208-257]()\\n\\n### Authentication Context Flow\\n\\n```mermaid\\nsequenceDiagram\\n    participant R as Request\\n    participant A as Authenticator\\n    participant AC as AuthContext\\n    participant H as Handler\\n    \\n    R->>A: HTTP Request\\n    A->>A: Extract Credentials\\n    A->>A: Verify User\\n    A->>AC: Create AuthContext\\n    AC->>H: Pass to Handler\\n    H->>H: Authorization Logic\\n    H->>AC: Return Result\\n    AC->>R: Response\\n```\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/types.py:349-401]()\\n\\n## Configuration Integration\\n\\n### LangGraph Configuration\\n\\nAuthentication is configured in `langgraph.json` through the `auth` section:\\n\\n```json\\n{\\n  \\\"auth\\\": {\\n    \\\"path\\\": \\\"./auth.py:my_auth\\\",\\n    \\\"disable_studio_auth\\\": false\\n  }\\n}\\n```\\n\\nThe auth path points to a Python module containing an `Auth` instance with registered handlers.\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/__init__.py:26-38]()\\n\\n### Studio Authentication\\n\\nLangGraph Studio provides built-in authentication that can be controlled through configuration. The `StudioUser` class represents authenticated Studio users with specific permissions.\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/types.py:208-257]()\\n\\n## Error Handling and Exceptions\\n\\nThe authentication system uses `HTTPException` for standardized error responses:\\n\\n```mermaid\\nflowchart TD\\n    A[\\\"Authentication Error\\\"] --> B[\\\"HTTPException\\\"]\\n    B --> C[\\\"Status Code (default: 401)\\\"]\\n    B --> D[\\\"Detail Message\\\"]\\n    B --> E[\\\"Optional Headers\\\"]\\n    C --> F[\\\"HTTP Response\\\"]\\n    D --> F\\n    E --> F\\n```\\n\\n**Default Behavior:** Returns 401 Unauthorized with standard HTTP status messages.\\n\\n**Sources:** [libs/sdk-py/langgraph_sdk/auth/exceptions.py:9-57]()\", \"# Page: Prebuilt Components\\n\\n# Prebuilt Components\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/agents/agents.md](docs/docs/agents/agents.md)\\n- [docs/docs/agents/context.md](docs/docs/agents/context.md)\\n- [docs/docs/agents/run_agents.md](docs/docs/agents/run_agents.md)\\n- [docs/docs/cloud/deployment/graph_rebuild.md](docs/docs/cloud/deployment/graph_rebuild.md)\\n- [libs/langgraph/langgraph/graph/__init__.py](libs/langgraph/langgraph/graph/__init__.py)\\n- [libs/langgraph/langgraph/graph/message.py](libs/langgraph/langgraph/graph/message.py)\\n- [libs/langgraph/langgraph/pregel/_messages.py](libs/langgraph/langgraph/pregel/_messages.py)\\n- [libs/langgraph/langgraph/pregel/main.py](libs/langgraph/langgraph/pregel/main.py)\\n- [libs/langgraph/tests/test_deprecation.py](libs/langgraph/tests/test_deprecation.py)\\n- [libs/langgraph/tests/test_messages_state.py](libs/langgraph/tests/test_messages_state.py)\\n- [libs/langgraph/tests/test_runtime.py](libs/langgraph/tests/test_runtime.py)\\n- [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py](libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py)\\n- [libs/prebuilt/langgraph/prebuilt/tool_node.py](libs/prebuilt/langgraph/prebuilt/tool_node.py)\\n- [libs/prebuilt/langgraph/prebuilt/tool_validator.py](libs/prebuilt/langgraph/prebuilt/tool_validator.py)\\n- [libs/prebuilt/tests/test_deprecation.py](libs/prebuilt/tests/test_deprecation.py)\\n- [libs/prebuilt/tests/test_react_agent.py](libs/prebuilt/tests/test_react_agent.py)\\n- [libs/prebuilt/tests/test_tool_node.py](libs/prebuilt/tests/test_tool_node.py)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's prebuilt components - high-level abstractions that simplify common agent and workflow patterns. These components provide ready-to-use implementations built on top of the core StateGraph and Pregel execution engine, allowing developers to quickly create functional agents without manually constructing graphs.\\n\\nFor information about the underlying graph construction and execution, see [Core Architecture](#2). For deployment and platform integration, see [Deployment and Platform](#7).\\n\\n## Overview\\n\\nPrebuilt components serve as the high-level API layer in LangGraph's architecture, abstracting away the complexity of manual graph construction for common patterns. The primary component is `create_react_agent`, which implements the ReAct (Reasoning and Acting) pattern for agents that can iteratively reason and execute tools.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"User Interface\\\"\\n        USER[\\\"Developer Code\\\"]\\n        SIMPLE[\\\"Simple Agent Creation\\\"]\\n    end\\n    \\n    subgraph \\\"Prebuilt Layer\\\"\\n        CRA[\\\"create_react_agent()\\\"]\\n        TN[\\\"ToolNode\\\"]\\n        TC[\\\"tools_condition()\\\"]\\n        UTILS[\\\"Helper Functions\\\"]\\n    end\\n    \\n    subgraph \\\"Core LangGraph\\\"\\n        SG[\\\"StateGraph\\\"]\\n        NODES[\\\"Graph Nodes\\\"]\\n        EDGES[\\\"Graph Edges\\\"]\\n        PREGEL[\\\"Pregel Runtime\\\"]\\n    end\\n    \\n    subgraph \\\"LangChain Integration\\\"\\n        TOOLS[\\\"BaseTool\\\"]\\n        MODELS[\\\"BaseChatModel\\\"]\\n        MESSAGES[\\\"BaseMessage Types\\\"]\\n    end\\n    \\n    USER --> SIMPLE\\n    SIMPLE --> CRA\\n    CRA --> TN\\n    CRA --> SG\\n    TN --> TOOLS\\n    TN --> NODES\\n    SG --> PREGEL\\n    CRA --> MODELS\\n    TN --> MESSAGES\\n    \\n    TC --> EDGES\\n    UTILS --> CRA\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:248-280](), [libs/prebuilt/langgraph/prebuilt/tool_node.py:239-315](), [README.md:26-47]()\\n\\n## create_react_agent Function\\n\\nThe `create_react_agent` function is the primary prebuilt component, implementing a complete ReAct agent pattern. It creates a compiled StateGraph that alternates between calling a language model and executing tools until a stopping condition is met.\\n\\n### Core Architecture\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Agent State Management\\\"\\n        AS[\\\"AgentState TypedDict\\\"]\\n        ASP[\\\"AgentStatePydantic BaseModel\\\"]\\n        MSGS[\\\"messages: Sequence[BaseMessage]\\\"]\\n        STEPS[\\\"remaining_steps: RemainingSteps\\\"]\\n    end\\n    \\n    subgraph \\\"create_react_agent Components\\\"\\n        CRA[\\\"create_react_agent()\\\"]\\n        PROMPT[\\\"_get_prompt_runnable()\\\"]\\n        MODEL[\\\"_get_model()\\\"]\\n        VALIDATE[\\\"_should_bind_tools()\\\"]\\n    end\\n    \\n    subgraph \\\"Generated Graph Structure\\\"\\n        AGENT_NODE[\\\"agent Node\\\"]\\n        TOOL_NODE[\\\"tools Node (ToolNode)\\\"]\\n        CONDITION[\\\"should_continue()\\\"]\\n        PRE_HOOK[\\\"pre_model_hook Node\\\"]\\n        POST_HOOK[\\\"post_model_hook Node\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Flow\\\"\\n        INPUT[\\\"User Input\\\"]\\n        CALL_MODEL[\\\"call_model() / acall_model()\\\"]\\n        TOOL_EXEC[\\\"Tool Execution\\\"]\\n        OUTPUT[\\\"Final Response\\\"]\\n    end\\n    \\n    CRA --> AS\\n    CRA --> AGENT_NODE\\n    CRA --> TOOL_NODE\\n    AGENT_NODE --> CALL_MODEL\\n    TOOL_NODE --> TOOL_EXEC\\n    CONDITION --> AGENT_NODE\\n    CONDITION --> TOOL_NODE\\n    \\n    INPUT --> AGENT_NODE\\n    CALL_MODEL --> CONDITION\\n    TOOL_EXEC --> CONDITION\\n    CONDITION --> OUTPUT\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:58-91](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:248-280](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:609-669]()\\n\\n### State Schema Types\\n\\nThe agent supports flexible state schemas through `AgentState` and `AgentStatePydantic` base classes:\\n\\n| Component | Type | Purpose |\\n|-----------|------|---------|\\n| `AgentState` | TypedDict | Basic dictionary-based state schema |\\n| `AgentStatePydantic` | BaseModel | Pydantic-based state with validation |\\n| `messages` | Annotated[Sequence[BaseMessage], add_messages] | Message history with reducer |\\n| `remaining_steps` | RemainingSteps | Step limit management |\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:62-76]()\\n\\n### Model Integration\\n\\nThe function supports both static and dynamic model selection:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Model Types\\\"\\n        STATIC[\\\"Static Model\\\"]\\n        DYNAMIC[\\\"Dynamic Model Callable\\\"]\\n        STRING[\\\"String Identifier\\\"]\\n    end\\n    \\n    subgraph \\\"Model Processing\\\"\\n        GET_MODEL[\\\"_get_model()\\\"]\\n        SHOULD_BIND[\\\"_should_bind_tools()\\\"]\\n        BIND_TOOLS[\\\"model.bind_tools()\\\"]\\n        RESOLVE[\\\"_resolve_model() / _aresolve_model()\\\"]\\n    end\\n    \\n    subgraph \\\"Runtime Resolution\\\"\\n        SYNC_CALL[\\\"call_model()\\\"]\\n        ASYNC_CALL[\\\"acall_model()\\\"]\\n        MODEL_INPUT[\\\"_get_model_input_state()\\\"]\\n    end\\n    \\n    STATIC --> GET_MODEL\\n    DYNAMIC --> RESOLVE\\n    STRING --> GET_MODEL\\n    \\n    GET_MODEL --> SHOULD_BIND\\n    SHOULD_BIND --> BIND_TOOLS\\n    \\n    RESOLVE --> SYNC_CALL\\n    RESOLVE --> ASYNC_CALL\\n    MODEL_INPUT --> SYNC_CALL\\n    MODEL_INPUT --> ASYNC_CALL\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:194-214](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:547-567](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:609-669]()\\n\\n### Graph Construction Logic\\n\\nThe agent constructs different graph topologies based on configuration:\\n\\n**Dynamic Graph Topology Construction**\\n```mermaid\\ngraph TD\\n    subgraph \\\"Configuration Inputs\\\"\\n        TOOLS_ENABLED[\\\"tool_calling_enabled\\\"]\\n        PRE_HOOK[\\\"pre_model_hook\\\"]\\n        POST_HOOK[\\\"post_model_hook\\\"]\\n        RESPONSE_FORMAT[\\\"response_format\\\"]\\n        VERSION[\\\"version: v1 | v2\\\"]\\n        CONTEXT_SCHEMA[\\\"context_schema\\\"]\\n    end\\n    \\n    subgraph \\\"No Tools Graph Structure\\\"\\n        NT_WORKFLOW[\\\"StateGraph(state_schema, context_schema)\\\"]\\n        NT_AGENT[\\\"agent Node (RunnableCallable)\\\"]\\n        NT_PRE[\\\"pre_model_hook Node\\\"]\\n        NT_POST[\\\"post_model_hook Node\\\"]\\n        NT_STRUCT[\\\"generate_structured_response Node\\\"]\\n        NT_ENTRY[\\\"Entry point routing\\\"]\\n    end\\n    \\n    subgraph \\\"Tools Graph Structure\\\"\\n        T_WORKFLOW[\\\"StateGraph(state_schema, context_schema)\\\"]\\n        T_AGENT[\\\"agent Node (call_model/acall_model)\\\"]\\n        T_TOOLS[\\\"tools Node (ToolNode)\\\"]\\n        T_CONDITION[\\\"should_continue() conditional\\\"]\\n        T_PRE[\\\"pre_model_hook Node\\\"]\\n        T_POST[\\\"post_model_hook Node\\\"]\\n        T_ROUTER[\\\"post_model_hook_router()\\\"]\\n    end\\n    \\n    TOOLS_ENABLED -->|False| NT_WORKFLOW\\n    TOOLS_ENABLED -->|True| T_WORKFLOW\\n    CONTEXT_SCHEMA --> NT_WORKFLOW\\n    CONTEXT_SCHEMA --> T_WORKFLOW\\n    \\n    PRE_HOOK --> NT_PRE\\n    PRE_HOOK --> T_PRE\\n    POST_HOOK --> NT_POST\\n    POST_HOOK --> T_POST\\n    POST_HOOK --> T_ROUTER\\n    RESPONSE_FORMAT --> NT_STRUCT\\n    \\n    T_CONDITION --> T_AGENT\\n    T_CONDITION --> T_TOOLS\\n    VERSION --> T_CONDITION\\n    NT_ENTRY --> NT_AGENT\\n    NT_ENTRY --> NT_PRE\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:735-776](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:803-937]()\\n\\n## ToolNode Component\\n\\n`ToolNode` handles the execution of tool calls from AI messages, supporting parallel execution, error handling, and advanced features like state injection.\\n\\n### Core Functionality\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"ToolNode Initialization\\\"\\n        INIT[\\\"__init__()\\\"]\\n        TOOLS[\\\"tools: Sequence[Union[BaseTool, Callable]]\\\"]\\n        CONFIG[\\\"handle_tool_errors\\\"]\\n        MAPPING[\\\"tools_by_name: dict[str, BaseTool]\\\"]\\n    end\\n    \\n    subgraph \\\"Input Processing\\\"\\n        PARSE[\\\"_parse_input()\\\"]\\n        EXTRACT[\\\"Extract tool_calls from AIMessage\\\"]\\n        INJECT[\\\"inject_tool_args()\\\"]\\n        STATE_ARGS[\\\"tool_to_state_args\\\"]\\n        STORE_ARGS[\\\"tool_to_store_arg\\\"]\\n    end\\n    \\n    subgraph \\\"Execution\\\"\\n        SYNC_EXEC[\\\"_func() with executor.map()\\\"]\\n        ASYNC_EXEC[\\\"_afunc() with asyncio.gather()\\\"]\\n        RUN_ONE[\\\"_run_one() / _arun_one()\\\"]\\n        VALIDATE[\\\"_validate_tool_call()\\\"]\\n    end\\n    \\n    subgraph \\\"Output Generation\\\"\\n        TOOL_MSG[\\\"ToolMessage\\\"]\\n        COMMAND[\\\"Command (advanced)\\\"]\\n        COMBINE[\\\"_combine_tool_outputs()\\\"]\\n    end\\n    \\n    INIT --> MAPPING\\n    TOOLS --> MAPPING\\n    CONFIG --> RUN_ONE\\n    \\n    PARSE --> EXTRACT\\n    EXTRACT --> INJECT\\n    INJECT --> STATE_ARGS\\n    INJECT --> STORE_ARGS\\n    \\n    SYNC_EXEC --> RUN_ONE\\n    ASYNC_EXEC --> RUN_ONE\\n    RUN_ONE --> VALIDATE\\n    \\n    RUN_ONE --> TOOL_MSG\\n    RUN_ONE --> COMMAND\\n    TOOL_MSG --> COMBINE\\n    COMMAND --> COMBINE\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/tool_node.py:319-350](), [libs/prebuilt/langgraph/prebuilt/tool_node.py:352-389](), [libs/prebuilt/langgraph/prebuilt/tool_node.py:549-586]()\\n\\n### Error Handling Strategies\\n\\n`ToolNode` provides sophisticated error handling through the `handle_tool_errors` parameter:\\n\\n| Strategy | Type | Behavior |\\n|----------|------|----------|\\n| `True` | bool | Catch all errors, return default error template |\\n| Custom string | str | Catch all errors, return custom message |\\n| Exception tuple | tuple[type[Exception], ...] | Catch only specified exception types |\\n| Custom handler | Callable[..., str] | Call function with exception, return result |\\n| `False` | bool | Disable error handling, propagate exceptions |\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/tool_node.py:125-171](), [libs/prebuilt/langgraph/prebuilt/tool_node.py:435-490]()\\n\\n### State and Store Injection\\n\\nTools can access graph state and persistent storage through special annotations:\\n\\n**State and Store Injection Mechanisms**\\n```mermaid\\ngraph TD\\n    subgraph \\\"Injection Annotations\\\"\\n        IS[\\\"InjectedState\\\"]\\n        IS_FIELD[\\\"InjectedState('field_name')\\\"]\\n        ISTORE[\\\"InjectedStore\\\"]\\n        ITCID[\\\"InjectedToolCallId\\\"]\\n    end\\n    \\n    subgraph \\\"Processing Functions\\\"\\n        STATE_ARGS[\\\"_get_state_args()\\\"]\\n        STORE_ARG[\\\"_get_store_arg()\\\"]\\n        INJECT_STATE[\\\"_inject_state()\\\"]\\n        INJECT_STORE[\\\"_inject_store()\\\"]\\n    end\\n    \\n    subgraph \\\"Tool Execution Context\\\"\\n        TOOL_CALL[\\\"ToolCall with injected args\\\"]\\n        TOOL_INVOKE[\\\"tool.invoke(call_args, config)\\\"]\\n        STATE_ACCESS[\\\"Access to graph state\\\"]\\n        STORE_ACCESS[\\\"Access to BaseStore\\\"]\\n    end\\n    \\n    IS --> STATE_ARGS\\n    IS_FIELD --> STATE_ARGS\\n    ISTORE --> STORE_ARG\\n    \\n    STATE_ARGS --> INJECT_STATE\\n    STORE_ARG --> INJECT_STORE\\n    \\n    INJECT_STATE --> TOOL_CALL\\n    INJECT_STORE --> TOOL_CALL\\n    \\n    TOOL_CALL --> TOOL_INVOKE\\n    TOOL_INVOKE --> STATE_ACCESS\\n    TOOL_INVOKE --> STORE_ACCESS\\n```\\n\\n**Injection Pattern Support**\\n\\n| Annotation | Purpose | Usage Pattern |\\n|------------|---------|---------------|\\n| `InjectedState` | Full state object injection | `state: Annotated[StateType, InjectedState]` |\\n| `InjectedState(\\\"field\\\")` | Specific field injection | `field: Annotated[FieldType, InjectedState(\\\"field_name\\\")]` |\\n| `InjectedStore` | Store object injection | `store: Annotated[BaseStore, InjectedStore]` |\\n| `InjectedToolCallId` | Tool call ID injection | `call_id: Annotated[str, InjectedToolCallId]` |\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/tool_node.py:600-687](), [libs/prebuilt/langgraph/prebuilt/tool_node.py:689-717](), [libs/prebuilt/tests/test_react_agent.py:681-758]()\\n\\n## Context Management Patterns\\n\\nPrebuilt components provide sophisticated context management through three distinct patterns that enable agents to access different types of information during execution.\\n\\n### Context Types and Access Patterns\\n\\n**Context Management in create_react_agent**\\n```mermaid\\ngraph TD\\n    subgraph \\\"Static Runtime Context\\\"\\n        SRC[\\\"context parameter\\\"]\\n        CS[\\\"context_schema\\\"]\\n        RT[\\\"Runtime[ContextT]\\\"]\\n        GR[\\\"get_runtime(ContextSchema)\\\"]\\n    end\\n    \\n    subgraph \\\"Dynamic Runtime Context\\\"\\n        STATE[\\\"AgentState/Custom State\\\"]\\n        MSGS[\\\"messages: Annotated[list, add_messages]\\\"]\\n        CUSTOM[\\\"Custom state fields\\\"]\\n    end\\n    \\n    subgraph \\\"Cross-Conversation Context\\\"\\n        STORE[\\\"BaseStore\\\"]\\n        INJECT_STORE[\\\"InjectedStore in tools\\\"]\\n        PERSIST[\\\"Persistent memory\\\"]\\n    end\\n    \\n    subgraph \\\"Agent Execution\\\"\\n        CRA[\\\"create_react_agent()\\\"]\\n        PROMPT[\\\"Dynamic prompt function\\\"]\\n        TOOLS[\\\"Tools with injections\\\"]\\n        MODEL[\\\"Model callable\\\"]\\n    end\\n    \\n    SRC --> RT\\n    CS --> RT\\n    RT --> GR\\n    GR --> PROMPT\\n    GR --> TOOLS\\n    \\n    STATE --> MSGS\\n    STATE --> CUSTOM\\n    CUSTOM --> PROMPT\\n    CUSTOM --> TOOLS\\n    \\n    STORE --> INJECT_STORE\\n    INJECT_STORE --> TOOLS\\n    PERSIST --> STORE\\n    \\n    CRA --> PROMPT\\n    CRA --> TOOLS\\n    CRA --> MODEL\\n```\\n\\nSources: [docs/docs/agents/context.md:22-31](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:248-279]()\\n\\n### Static Runtime Context Integration\\n\\nThe `create_react_agent` function supports static runtime context through the `context_schema` parameter and runtime injection:\\n\\n| Component | Purpose | Access Method |\\n|-----------|---------|---------------|\\n| `context_schema` | Type definition for runtime context | Parameter to `create_react_agent` |\\n| `Runtime[ContextT]` | Injected runtime object | Function parameter in nodes/tools |\\n| `get_runtime(ContextSchema)` | Context accessor function | Called within nodes/tools |\\n| `context` parameter | Runtime context data | Passed to `invoke`/`stream` calls |\\n\\nSources: [docs/docs/agents/context.md:32-121](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:271-279]()\\n\\n### Dynamic State Context\\n\\nAgent state provides mutable context that evolves during execution:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"State Schema Options\\\"\\n        AS[\\\"AgentState (TypedDict)\\\"]\\n        ASP[\\\"AgentStatePydantic (BaseModel)\\\"]\\n        CUSTOM[\\\"Custom State Schema\\\"]\\n    end\\n    \\n    subgraph \\\"State Components\\\"\\n        MSGS[\\\"messages field\\\"]\\n        STEPS[\\\"remaining_steps field\\\"]\\n        FIELDS[\\\"Custom fields\\\"]\\n    end\\n    \\n    subgraph \\\"Access Patterns\\\"\\n        PROMPT_ACCESS[\\\"Prompt function access\\\"]\\n        TOOL_ACCESS[\\\"Tool state injection\\\"]\\n        NODE_ACCESS[\\\"Node function access\\\"]\\n    end\\n    \\n    AS --> MSGS\\n    ASP --> MSGS\\n    CUSTOM --> MSGS\\n    \\n    AS --> STEPS\\n    ASP --> STEPS\\n    CUSTOM --> STEPS\\n    \\n    CUSTOM --> FIELDS\\n    \\n    MSGS --> PROMPT_ACCESS\\n    FIELDS --> PROMPT_ACCESS\\n    FIELDS --> TOOL_ACCESS\\n    MSGS --> NODE_ACCESS\\n    FIELDS --> NODE_ACCESS\\n```\\n\\nSources: [docs/docs/agents/context.md:149-267](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:62-91]()\\n\\n### Store-Based Persistent Context\\n\\nTools and agents can access persistent storage across conversations:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Store Integration\\\"\\n        BS[\\\"BaseStore interface\\\"]\\n        IMS[\\\"InMemoryStore\\\"]\\n        CUSTOM_STORE[\\\"Custom store implementations\\\"]\\n    end\\n    \\n    subgraph \\\"Tool Injection\\\"\\n        IS[\\\"InjectedStore annotation\\\"]\\n        STORE_ARG[\\\"_get_store_arg()\\\"]\\n        INJECT_FUNC[\\\"_inject_store()\\\"]\\n    end\\n    \\n    subgraph \\\"Agent Configuration\\\"\\n        CRA_STORE[\\\"create_react_agent(store=...)\\\"]\\n        TN_STORE[\\\"ToolNode with store access\\\"]\\n        RUNTIME_STORE[\\\"runtime.store\\\"]\\n    end\\n    \\n    BS --> IMS\\n    BS --> CUSTOM_STORE\\n    \\n    IS --> STORE_ARG\\n    STORE_ARG --> INJECT_FUNC\\n    INJECT_FUNC --> TN_STORE\\n    \\n    CRA_STORE --> TN_STORE\\n    CRA_STORE --> RUNTIME_STORE\\n    IMS --> CRA_STORE\\n    CUSTOM_STORE --> CRA_STORE\\n```\\n\\nSources: [docs/docs/agents/context.md:304-309](), [libs/prebuilt/langgraph/prebuilt/tool_node.py:689-717]()\\n\\n## Supporting Components\\n\\n### tools_condition Function\\n\\nThe `tools_condition` utility function provides routing logic for conditional tool execution:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"tools_condition Logic\\\"\\n        INPUT[\\\"State with messages\\\"]\\n        CHECK[\\\"Check last AIMessage\\\"]\\n        TOOL_CALLS[\\\"Has tool_calls?\\\"]\\n        ROUTE_TOOLS[\\\"Route to tools\\\"]\\n        ROUTE_END[\\\"Route to END\\\"]\\n    end\\n    \\n    INPUT --> CHECK\\n    CHECK --> TOOL_CALLS\\n    TOOL_CALLS -->|Yes| ROUTE_TOOLS\\n    TOOL_CALLS -->|No| ROUTE_END\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/tool_node.py:1125-1150]()\\n\\n### Message Content Handling\\n\\nThe `msg_content_output` function standardizes tool output formatting for LangChain message compatibility:\\n\\n| Input Type | Output Format | Purpose |\\n|------------|---------------|---------|\\n| str | str | Direct string content |\\n| list[dict] with type | list[dict] | Structured content blocks |\\n| Other types | JSON string | Serialized representation |\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/tool_node.py:86-123]()\\n\\n## Integration Patterns\\n\\n### React Agent Execution Flow\\n\\nThe complete ReAct agent execution follows this pattern:\\n\\n```mermaid\\nsequenceDiagram\\n    participant User\\n    participant Agent as \\\"agent Node\\\"\\n    participant Tools as \\\"tools Node\\\"\\n    participant Model as \\\"Language Model\\\"\\n    \\n    User->>Agent: {\\\"messages\\\": [HumanMessage]}\\n    \\n    loop Until no tool calls\\n        Agent->>Model: Process messages with prompt\\n        Model->>Agent: AIMessage with/without tool_calls\\n        \\n        alt Has tool calls\\n            Agent->>Tools: Execute tool_calls\\n            Tools->>Tools: Parallel tool execution\\n            Tools->>Agent: ToolMessage results\\n        else No tool calls\\n            Agent->>User: Return final response\\n        end\\n    end\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:433-445]()\\n\\n### Version Differences\\n\\nThe `create_react_agent` function supports two execution versions with different tool call distribution strategies:\\n\\n**Version Comparison**\\n```mermaid\\ngraph TD\\n    subgraph \\\"Version v1 Flow\\\"\\n        V1_AI[\\\"AIMessage with tool_calls\\\"]\\n        V1_TOOLS[\\\"tools Node\\\"]\\n        V1_PARALLEL[\\\"Parallel execution within node\\\"]\\n        V1_RESULTS[\\\"All ToolMessages returned\\\"]\\n    end\\n    \\n    subgraph \\\"Version v2 Flow\\\"\\n        V2_AI[\\\"AIMessage with tool_calls\\\"]\\n        V2_SEND[\\\"Send API distribution\\\"]\\n        V2_INSTANCES[\\\"Multiple ToolNode instances\\\"]\\n        V2_INDIVIDUAL[\\\"Individual tool call per instance\\\"]\\n    end\\n    \\n    subgraph \\\"should_continue() Routing\\\"\\n        SC_CHECK[\\\"Check tool_calls in last AIMessage\\\"]\\n        SC_V1[\\\"return 'tools'\\\"]\\n        SC_V2[\\\"return [Send('tools', [tool_call])]\\\"]\\n        SC_POST[\\\"return 'post_model_hook'\\\"]\\n        SC_END[\\\"return END\\\"]\\n    end\\n    \\n    V1_AI --> V1_TOOLS\\n    V1_TOOLS --> V1_PARALLEL\\n    V1_PARALLEL --> V1_RESULTS\\n    \\n    V2_AI --> V2_SEND\\n    V2_SEND --> V2_INSTANCES\\n    V2_INSTANCES --> V2_INDIVIDUAL\\n    \\n    SC_CHECK --> SC_V1\\n    SC_CHECK --> SC_V2\\n    SC_CHECK --> SC_POST\\n    SC_CHECK --> SC_END\\n```\\n\\n| Version | Tool Processing | Execution Model | Send Usage |\\n|---------|----------------|-----------------|------------|\\n| v1 | Single message with all tool calls | Parallel execution within ToolNode | No |\\n| v2 | Individual tool calls as separate messages | Send API distribution across ToolNode instances | Yes |\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:405-414](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:792-801]()\\n\\n## Advanced Features\\n\\n### Structured Response Generation\\n\\nAgents can generate structured outputs using the `response_format` parameter:\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Response Format Options\\\"\\n        SCHEMA[\\\"Pydantic Schema\\\"]\\n        JSON_SCHEMA[\\\"JSON Schema\\\"]\\n        TYPED_DICT[\\\"TypedDict\\\"]\\n        TUPLE_FORMAT[\\\"(prompt, schema) Tuple\\\"]\\n    end\\n    \\n    subgraph \\\"Generation Process\\\"\\n        FINAL_STATE[\\\"Agent completion state\\\"]\\n        STRUCT_NODE[\\\"generate_structured_response Node\\\"]\\n        WITH_STRUCT[\\\"model.with_structured_output()\\\"]\\n        STRUCT_RESP[\\\"structured_response key\\\"]\\n    end\\n    \\n    SCHEMA --> WITH_STRUCT\\n    JSON_SCHEMA --> WITH_STRUCT\\n    TYPED_DICT --> WITH_STRUCT\\n    TUPLE_FORMAT --> WITH_STRUCT\\n    \\n    FINAL_STATE --> STRUCT_NODE\\n    STRUCT_NODE --> WITH_STRUCT\\n    WITH_STRUCT --> STRUCT_RESP\\n```\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:692-733]()\\n\\n### Pre/Post Model Hooks\\n\\nThe agent supports custom processing before and after model calls:\\n\\n| Hook Type | Purpose | Input/Output |\\n|-----------|---------|--------------|\\n| pre_model_hook | Message processing, trimming, summarization | State → State update with messages/llm_input_messages |\\n| post_model_hook | Human-in-the-loop, validation, guardrails | State → State update |\\n\\nSources: [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:355-387](), [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py:818-834]()\", \"# Page: Development and Testing\\n\\n# Development and Testing\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [.github/workflows/codespell-ignore-words.txt](.github/workflows/codespell-ignore-words.txt)\\n- [.github/workflows/codespell.yml](.github/workflows/codespell.yml)\\n- [.github/workflows/deploy_docs.yml](.github/workflows/deploy_docs.yml)\\n- [.github/workflows/extract_ignored_words_list.py](.github/workflows/extract_ignored_words_list.py)\\n- [.github/workflows/pr_lint.yml](.github/workflows/pr_lint.yml)\\n- [.gitignore](.gitignore)\\n- [docs/.gitignore](docs/.gitignore)\\n- [docs/Makefile](docs/Makefile)\\n- [docs/_scripts/_patch.py](docs/_scripts/_patch.py)\\n- [docs/_scripts/prepare_notebooks_for_ci.py](docs/_scripts/prepare_notebooks_for_ci.py)\\n- [docs/docs/examples/index.md](docs/docs/examples/index.md)\\n- [docs/overrides/main.html](docs/overrides/main.html)\\n- [docs/package.json](docs/package.json)\\n- [docs/pyproject.toml](docs/pyproject.toml)\\n\\n</details>\\n\\n\\n\\nThis document provides an overview of LangGraph's development workflows, testing strategies, and documentation generation processes. It covers the essential tools, configurations, and practices used across the multi-library monorepo to maintain code quality, generate comprehensive documentation, and ensure reliable releases.\\n\\nThe LangGraph development ecosystem encompasses three main areas: the core framework libraries, the documentation system, and the deployment toolchain. Each uses specialized tools and workflows optimized for their specific requirements.\\n\\nFor detailed implementation of the CI/CD pipeline, see [CI/CD Pipeline](page_10.2). For comprehensive testing framework documentation, see [Testing Framework](page_10.3). For documentation system specifics, see [Documentation System](page_10.1).\\n\\n## Development Environment Setup\\n\\nLangGraph uses a modern Python development stack centered around the UV package manager for fast dependency resolution and environment management. The project is organized as a monorepo with multiple libraries under the `libs/` directory, each with its own `pyproject.toml` and `uv.lock` files.\\n\\n### UV Package Manager and Dependency Groups\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Package Configuration\\\"\\n        PyProjectToml[\\\"pyproject.toml\\\"]\\n        UVLock[\\\"uv.lock\\\"]\\n        UVSync[\\\"uv sync --frozen\\\"]\\n        DefaultGroups[\\\"[tool.uv] default-groups\\\"]\\n    end\\n    \\n    subgraph \\\"Core Libraries\\\"\\n        LibsLangGraph[\\\"libs/langgraph/\\\"]\\n        LibsCheckpoint[\\\"libs/checkpoint/\\\"]\\n        LibsCLI[\\\"libs/cli/\\\"]\\n        LibsSDKPy[\\\"libs/sdk-py/\\\"]\\n        LibsPrebuilt[\\\"libs/prebuilt/\\\"]\\n    end\\n    \\n    subgraph \\\"Dev Dependency Group\\\"\\n        Pytest[\\\"pytest\\\"]\\n        PytestCov[\\\"pytest-cov\\\"]\\n        PytestMock[\\\"pytest-mock\\\"]\\n        PytestWatcher[\\\"pytest-watcher\\\"]\\n        Ruff[\\\"ruff\\\"]\\n        MyPy[\\\"mypy\\\"]\\n        Syrupy[\\\"syrupy\\\"]\\n        HttpX[\\\"httpx\\\"]\\n        Jupyter[\\\"jupyter\\\"]\\n    end\\n    \\n    UVSync --> PyProjectToml\\n    PyProjectToml --> DefaultGroups\\n    DefaultGroups --> Pytest\\n    DefaultGroups --> Ruff\\n    DefaultGroups --> MyPy\\n    \\n    LibsLangGraph --> PyProjectToml\\n    LibsCheckpoint --> PyProjectToml\\n    LibsCLI --> PyProjectToml\\n```\\n\\nThe development environment uses UV package manager with `pyproject.toml` files in each library. The main LangGraph library defines a comprehensive `dev` dependency group that includes testing, linting, and development tools. The `[tool.uv] default-groups = ['dev']` configuration automatically installs development dependencies.\\n\\n**Sources:** [libs/langgraph/pyproject.toml:26-52](), [libs/langgraph/pyproject.toml:54-55](), [libs/langgraph/uv.lock:1-10]()\\n\\n### Makefile Build System Integration\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Core Make Targets\\\"\\n        MakeInstall[\\\"make install\\\"]\\n        MakeTest[\\\"make test\\\"]\\n        MakeLint[\\\"make lint\\\"]\\n        MakeFormat[\\\"make format\\\"]\\n        MakeCoverage[\\\"make coverage\\\"]\\n    end\\n    \\n    subgraph \\\"UV Commands\\\"\\n        UVSyncFrozen[\\\"uv sync --frozen --all-extras --all-packages --group dev\\\"]\\n        UVRunPytest[\\\"uv run pytest\\\"]\\n        UVRunRuff[\\\"uv run ruff\\\"]\\n        UVRunMypy[\\\"uv run mypy\\\"]\\n    end\\n    \\n    subgraph \\\"Infrastructure Commands\\\"\\n        StartPostgres[\\\"make start-postgres\\\"]\\n        StopPostgres[\\\"make stop-postgres\\\"]\\n        StartDevServer[\\\"make start-dev-server\\\"]\\n        StopDevServer[\\\"make stop-dev-server\\\"]\\n    end\\n    \\n    subgraph \\\"Docker Compose Services\\\"\\n        PostgresCompose[\\\"tests/compose-postgres.yml\\\"]\\n        LangGraphDev[\\\"langgraph dev --config tests/example_app/langgraph.json\\\"]\\n    end\\n    \\n    MakeInstall --> UVSyncFrozen\\n    MakeTest --> UVRunPytest\\n    MakeLint --> UVRunRuff\\n    MakeFormat --> UVRunRuff\\n    \\n    MakeTest --> StartPostgres\\n    MakeTest --> StartDevServer\\n    StartPostgres --> PostgresCompose\\n    StartDevServer --> LangGraphDev\\n```\\n\\nThe build system uses Make targets that wrap UV commands for common development tasks. Key targets include `make install` for dependency installation, `make test` for running the full test suite with Docker services, and various linting/formatting targets.\\n\\n**Sources:** [libs/langgraph/Makefile:14-15](), [libs/langgraph/Makefile:61-74](), [libs/langgraph/Makefile:40-56]()\\n\\n## Testing Infrastructure\\n\\nLangGraph employs a comprehensive testing strategy with multiple backend support, parallel execution, and extensive fixture-based configuration to ensure reliability across different deployment scenarios.\\n\\n### Parameterized Test Fixtures and Backend Support\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Conftest Module Structure\\\"\\n        TestsConftestPy[\\\"tests/conftest.py\\\"]\\n        ConftestCheckpointer[\\\"tests/conftest_checkpointer.py\\\"]\\n        ConftestStore[\\\"tests/conftest_store.py\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpointer Fixture Functions\\\"\\n        CheckpointerMemory[\\\"_checkpointer_memory()\\\"]\\n        CheckpointerSqlite[\\\"_checkpointer_sqlite()\\\"]\\n        CheckpointerSqliteAes[\\\"_checkpointer_sqlite_aes()\\\"]\\n        CheckpointerPostgres[\\\"_checkpointer_postgres()\\\"]\\n        CheckpointerPostgresPipe[\\\"_checkpointer_postgres_pipe()\\\"]\\n        CheckpointerPostgresPool[\\\"_checkpointer_postgres_pool()\\\"]\\n    end\\n    \\n    subgraph \\\"Parameterized Fixtures\\\"\\n        SyncCheckpointer[\\\"@pytest.fixture sync_checkpointer\\\"]\\n        AsyncCheckpointer[\\\"@pytest.fixture async_checkpointer\\\"]\\n        SyncStore[\\\"@pytest.fixture sync_store\\\"]\\n        AsyncStore[\\\"@pytest.fixture async_store\\\"]\\n        DeterministicUUIDs[\\\"@pytest.fixture deterministic_uuids\\\"]\\n    end\\n    \\n    subgraph \\\"Backend Implementations\\\"\\n        BaseCheckpointSaver[\\\"BaseCheckpointSaver\\\"]\\n        InMemorySaver[\\\"InMemorySaver\\\"]\\n        SqliteSaver[\\\"SqliteSaver\\\"]\\n        PostgresSaver[\\\"PostgresSaver\\\"]\\n        BaseStore[\\\"BaseStore\\\"]\\n    end\\n    \\n    TestsConftestPy --> SyncCheckpointer\\n    TestsConftestPy --> AsyncCheckpointer\\n    TestsConftestPy --> DeterministicUUIDs\\n    \\n    SyncCheckpointer --> CheckpointerMemory\\n    SyncCheckpointer --> CheckpointerSqlite\\n    SyncCheckpointer --> CheckpointerPostgres\\n    \\n    CheckpointerMemory --> InMemorySaver\\n    CheckpointerSqlite --> SqliteSaver\\n    CheckpointerPostgres --> PostgresSaver\\n```\\n\\nThe testing infrastructure uses parameterized fixtures defined in `tests/conftest.py` that automatically test code against multiple backend implementations. The `sync_checkpointer` and `async_checkpointer` fixtures cycle through different persistence backends, while `deterministic_uuids` provides reproducible UUID generation for tests.\\n\\n**Sources:** [libs/langgraph/tests/conftest.py:120-164](), [libs/langgraph/tests/conftest.py:167-202](), [libs/langgraph/tests/conftest.py:45-50]()\\n\\n### Test Execution Matrix and NO_DOCKER Support\\n\\n| Test Type | Command | Backend Support | Docker Required |\\n|-----------|---------|----------------|-----------------|\\n| Standard Tests | `make test` | Memory, SQLite, PostgreSQL | Conditional |\\n| Parallel Tests | `make test_parallel` | All backends | Yes |\\n| Watch Mode | `make test_watch` | All backends | Yes |\\n| Coverage | `make coverage` | Memory, SQLite | No |\\n| Integration | `make integration_tests` | All backends | Yes |\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Test Execution Flow\\\"\\n        MakeTest[\\\"make test\\\"]\\n        NODockerCheck[\\\"NO_DOCKER environment check\\\"]\\n        StartPostgres[\\\"make start-postgres\\\"]\\n        StartDevServer[\\\"make start-dev-server\\\"]\\n        UVRunPytest[\\\"uv run pytest $(TEST)\\\"]\\n        Cleanup[\\\"make stop-postgres; make stop-dev-server\\\"]\\n    end\\n    \\n    subgraph \\\"Docker Services\\\"\\n        DockerCompose[\\\"docker compose -f tests/compose-postgres.yml\\\"]\\n        PostgresContainer[\\\"PostgreSQL container\\\"]\\n        DevServerProcess[\\\"langgraph dev process\\\"]\\n    end\\n    \\n    MakeTest --> NODockerCheck\\n    NODockerCheck --> StartPostgres\\n    StartPostgres --> DockerCompose\\n    DockerCompose --> PostgresContainer\\n    StartPostgres --> StartDevServer\\n    StartDevServer --> DevServerProcess\\n    StartDevServer --> UVRunPytest\\n    UVRunPytest --> Cleanup\\n```\\n\\nTest execution supports conditional Docker usage through the `NO_DOCKER` environment variable. When Docker is available, tests run against all backends including PostgreSQL. The `make test` target orchestrates service startup, test execution, and cleanup.\\n\\n**Sources:** [libs/langgraph/Makefile:58-74](), [libs/langgraph/Makefile:40-44](), [libs/langgraph/tests/conftest.py:37]()\\n\\n### Snapshot Testing with Syrupy and Deterministic Fixtures\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Syrupy Snapshot Testing\\\"\\n        SnapshotAssertion[\\\"SnapshotAssertion\\\"]\\n        AMBRFiles[\\\".ambr snapshot files\\\"]\\n        SnapshotWarnUnused[\\\"--snapshot-warn-unused\\\"]\\n    end\\n    \\n    subgraph \\\"Deterministic UUID Fixture\\\"\\n        DeterministicUUIDs[\\\"deterministic_uuids fixture\\\"]\\n        MockerFixture[\\\"MockerFixture\\\"]\\n        UUIDSideEffect[\\\"UUID side_effect generator\\\"]\\n    end\\n    \\n    subgraph \\\"Test Configuration\\\"\\n        PytestIniOptions[\\\"[tool.pytest.ini_options]\\\"]\\n        FullTrace[\\\"--full-trace\\\"]\\n        StrictMarkers[\\\"--strict-markers\\\"]\\n        Durations[\\\"--durations=5\\\"]\\n    end\\n    \\n    subgraph \\\"Example Usage\\\"\\n        TestReactAgent[\\\"test_react_agent_graph.py\\\"]\\n        GraphDrawMermaid[\\\"agent.get_graph().draw_mermaid()\\\"]\\n        SnapshotComparison[\\\"== snapshot\\\"]\\n    end\\n    \\n    DeterministicUUIDs --> MockerFixture\\n    MockerFixture --> UUIDSideEffect\\n    \\n    TestReactAgent --> SnapshotAssertion\\n    GraphDrawMermaid --> SnapshotComparison\\n    SnapshotComparison --> AMBRFiles\\n```\\n\\nThe testing framework uses `syrupy` for snapshot testing with `.ambr` files that capture expected outputs. The `deterministic_uuids` fixture uses `MockerFixture` to generate predictable UUIDs with `UUID(f\\\"00000000-0000-4000-8000-{i:012}\\\", version=4)` pattern for reproducible test runs.\\n\\n**Sources:** [libs/langgraph/tests/conftest.py:45-50](), [libs/langgraph/pyproject.toml:108-109](), [libs/prebuilt/tests/test_react_agent_graph.py:38-52](), [libs/prebuilt/tests/__snapshots__/test_react_agent_graph.ambr:1-10]()\\n\\n## Code Quality and Linting\\n\\nLangGraph maintains code quality through automated linting, formatting, and type checking integrated into both local development and CI workflows.\\n\\n### Ruff and MyPy Code Quality Pipeline\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Makefile Lint Targets\\\"\\n        MakeLint[\\\"make lint\\\"]\\n        MakeLintDiff[\\\"make lint_diff\\\"]\\n        MakeLintPackage[\\\"make lint_package\\\"]\\n        MakeLintTests[\\\"make lint_tests\\\"]\\n        MakeFormat[\\\"make format\\\"]\\n    end\\n    \\n    subgraph \\\"Ruff Commands\\\"\\n        RuffCheck[\\\"uv run ruff check .\\\"]\\n        RuffFormat[\\\"uv run ruff format $(PYTHON_FILES)\\\"]\\n        RuffImports[\\\"uv run ruff check --select I $(PYTHON_FILES)\\\"]\\n        RuffFormatDiff[\\\"uv run ruff format $(PYTHON_FILES) --diff\\\"]\\n    end\\n    \\n    subgraph \\\"MyPy Type Checking\\\"\\n        MyPyLangGraph[\\\"uv run mypy langgraph --cache-dir $(MYPY_CACHE)\\\"]\\n        MyPyCacheMain[\\\".mypy_cache\\\"]\\n        MyPyCacheTest[\\\".mypy_cache_test\\\"]\\n    end\\n    \\n    subgraph \\\"Codespell Integration\\\"\\n        CodespellCheck[\\\"make spell_check\\\"]\\n        CodespellFix[\\\"make spell_fix\\\"]\\n        CodespellToml[\\\"uv run codespell --toml pyproject.toml\\\"]\\n    end\\n    \\n    MakeLint --> RuffCheck\\n    MakeLint --> RuffFormatDiff\\n    MakeLint --> RuffImports\\n    MakeLint --> MyPyLangGraph\\n    \\n    MakeFormat --> RuffFormat\\n    MakeFormat --> RuffImports\\n    \\n    MakeLintTests --> MyPyCacheTest\\n    MakeLintPackage --> MyPyCacheMain\\n    \\n    CodespellCheck --> CodespellToml\\n```\\n\\nThe code quality pipeline uses multiple Makefile targets with specific Ruff and MyPy configurations. Different lint targets handle package code vs test code with separate MyPy cache directories. The `PYTHON_FILES` variable allows targeted linting of changed files.\\n\\n**Sources:** [libs/langgraph/Makefile:115-136](), [libs/langgraph/Makefile:121-126](), [libs/langgraph/pyproject.toml:111-113]()\\n\\n### Tool Configuration in pyproject.toml\\n\\n```toml\\n[tool.ruff]\\nlint.select = [ \\\"E\\\", \\\"F\\\", \\\"I\\\", \\\"TID251\\\", \\\"UP\\\" ]\\nlint.ignore = [ \\\"E501\\\" ]\\nline-length = 88\\nindent-width = 4\\nextend-include = [\\\"*.ipynb\\\"]\\ntarget-version = \\\"py39\\\"\\n\\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\\n\\\"typing.TypedDict\\\".msg = \\\"Use typing_extensions.TypedDict instead.\\\"\\n\\n[tool.mypy]\\ndisallow_untyped_defs = \\\"True\\\"\\nexplicit_package_bases = \\\"True\\\"\\nwarn_unused_ignores = \\\"True\\\" \\nwarn_redundant_casts = \\\"True\\\"\\nallow_redefinition = \\\"True\\\"\\ndisable_error_code = \\\"typeddict-item, return-value, override, has-type\\\"\\n\\n[tool.codespell]\\nignore-words-list = \\\"infor,thead,stdio,nd,jupyter,lets,lite,uis,deque,langgraph,langchain,pydantic...\\\"\\n```\\n\\nRuff enforces specific rule sets including error codes (E), pyflakes (F), import sorting (I), tidyimports (TID251), and pyupgrade (UP) while ignoring line length (E501). The configuration includes Jupyter notebook support and bans `typing.TypedDict` in favor of `typing_extensions.TypedDict`. MyPy uses strict type checking with specific error codes disabled.\\n\\n**Sources:** [libs/langgraph/pyproject.toml:65-82](), [libs/langgraph/pyproject.toml:84-85](), [libs/langgraph/pyproject.toml:87-95](), [libs/langgraph/pyproject.toml:111-113]()\\n\\n### GitHub Actions Lint Integration\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"CI Lint Workflow\\\"\\n        LintJob[\\\"lint job\\\"]\\n        ChangedFiles[\\\"Ana06/get-changed-files@v2.3.0\\\"]\\n        UVSync[\\\"uv sync --frozen --group dev\\\"]\\n        MyPyCacheAction[\\\"actions/cache@v4 .mypy_cache\\\"]\\n    end\\n    \\n    subgraph \\\"Lint Execution\\\"\\n        MakeLintPackage[\\\"make lint_package\\\"]\\n        MakeLintTests[\\\"make lint_tests\\\"]\\n        RuffOutputFormat[\\\"RUFF_OUTPUT_FORMAT: github\\\"]\\n    end\\n    \\n    subgraph \\\"Cache Strategy\\\"\\n        MyPyLintCache[\\\"mypy-lint-${{ runner.os }}-${{ runner.arch }}-py${{ matrix.python-version }}\\\"]\\n        MyPyTestCache[\\\"mypy-test-${{ runner.os }}-${{ runner.arch }}-py${{ matrix.python-version }}\\\"]\\n        UVLockHash[\\\"${{ hashFiles(format('{0}/uv.lock', inputs.working-directory)) }}\\\"]\\n    end\\n    \\n    LintJob --> ChangedFiles\\n    ChangedFiles --> UVSync\\n    UVSync --> MyPyCacheAction\\n    MyPyCacheAction --> MakeLintPackage\\n    MakeLintPackage --> MakeLintTests\\n    \\n    MyPyCacheAction --> MyPyLintCache\\n    MyPyCacheAction --> MyPyTestCache\\n    MyPyLintCache --> UVLockHash\\n```\\n\\nThe GitHub Actions lint workflow uses change detection to skip unnecessary runs, caches MyPy results for performance, and sets `RUFF_OUTPUT_FORMAT: github` for inline annotations on pull requests.\\n\\n**Sources:** [.github/workflows/_lint.yml:14-16](), [.github/workflows/_lint.yml:53-87](), [.github/workflows/_lint.yml:79-87]()\\n\\n## Performance Testing and Benchmarking\\n\\nLangGraph includes performance testing infrastructure for regression detection and optimization validation.\\n\\n### Benchmark System with PyPerf and Py-Spy\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Makefile Benchmark Targets\\\"\\n        MakeBenchmark[\\\"make benchmark\\\"]\\n        MakeBenchmarkFast[\\\"make benchmark-fast\\\"]\\n        MakeProfile[\\\"make profile\\\"]\\n        OutputDir[\\\"mkdir -p out\\\"]\\n    end\\n    \\n    subgraph \\\"Benchmark Commands\\\"\\n        PyPerfRigorous[\\\"uv run python -m bench -o $(OUTPUT) --rigorous\\\"]\\n        PyPerfFast[\\\"uv run python -m bench -o $(OUTPUT) --fast\\\"]\\n        PySpyRecord[\\\"sudo uv run py-spy record -g -o out/profile.svg -- python $(GRAPH)\\\"]\\n    end\\n    \\n    subgraph \\\"Output Files\\\"\\n        BenchmarkJSON[\\\"out/benchmark.json\\\"]\\n        BenchmarkBaselineJSON[\\\"out/benchmark-baseline.json\\\"]\\n        ProfileSVG[\\\"out/profile.svg\\\"]\\n    end\\n    \\n    subgraph \\\"CI Integration\\\"\\n        BenchGitHubActions[\\\".github/workflows/bench.yml\\\"]\\n        BaselineGitHubActions[\\\".github/workflows/baseline.yml\\\"]\\n        PyPerfCompare[\\\"uv run pyperf compare_to out/main.json out/changes.json\\\"]\\n    end\\n    \\n    MakeBenchmark --> PyPerfRigorous\\n    MakeBenchmarkFast --> PyPerfFast\\n    MakeProfile --> PySpyRecord\\n    \\n    PyPerfRigorous --> BenchmarkJSON\\n    PyPerfFast --> BenchmarkJSON\\n    PySpyRecord --> ProfileSVG\\n    \\n    BenchGitHubActions --> PyPerfCompare\\n    BaselineGitHubActions --> BenchmarkBaselineJSON\\n```\\n\\nThe benchmark system uses `python -m bench` module with PyPerf for statistical benchmarking and Py-Spy for profiling. The `GRAPH` variable defaults to `bench/fanout_to_subgraph.py`, and outputs are stored in `out/` directory with specific filenames for CI integration.\\n\\n**Sources:** [libs/langgraph/Makefile:17-31](), [libs/langgraph/Makefile:27-31](), [.github/workflows/bench.yml:42-57](), [.github/workflows/baseline.yml:30-31]()\\n\\n### GitHub Actions Benchmark Workflow\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Baseline Creation Workflow\\\"\\n        BaselineYML[\\\".github/workflows/baseline.yml\\\"]\\n        BaselineTrigger[\\\"on: push branches: [main]\\\"]\\n        MakeBenchmarkBaseline[\\\"OUTPUT=out/benchmark-baseline.json make -s benchmark\\\"]\\n        ActionsCacheSave[\\\"actions/cache/save@v4\\\"]\\n    end\\n    \\n    subgraph \\\"PR Benchmark Workflow\\\"\\n        BenchYML[\\\".github/workflows/bench.yml\\\"]\\n        PRTrigger[\\\"on: pull_request paths: ['libs/**']\\\"]\\n        ActionsCacheRestore[\\\"actions/cache/restore@v4\\\"]\\n        MakeBenchmarkFast[\\\"make -s benchmark-fast\\\"]\\n        PyPerfCompareTable[\\\"uv run pyperf compare_to --table --group-by-speed\\\"]\\n    end\\n    \\n    subgraph \\\"Cache Strategy\\\"\\n        CacheKey[\\\"${{ runner.os }}-benchmark-baseline-${{ env.SHA }}\\\"]\\n        BaselineJSON[\\\"out/benchmark-baseline.json\\\"]\\n        MainJSON[\\\"out/main.json\\\"]\\n        ChangesJSON[\\\"out/changes.json\\\"]\\n    end\\n    \\n    BaselineYML --> MakeBenchmarkBaseline\\n    MakeBenchmarkBaseline --> ActionsCacheSave\\n    ActionsCacheSave --> CacheKey\\n    \\n    BenchYML --> ActionsCacheRestore\\n    ActionsCacheRestore --> MakeBenchmarkFast\\n    MakeBenchmarkFast --> PyPerfCompareTable\\n    \\n    BaselineJSON --> MainJSON\\n    ChangesJSON --> PyPerfCompareTable\\n```\\n\\nThe CI benchmark system uses two workflows: `baseline.yml` creates benchmark baselines on main branch pushes, while `bench.yml` compares PR changes against cached baselines. The comparison uses `pyperf compare_to` with table and speed grouping for GitHub Actions annotations.\\n\\n**Sources:** [.github/workflows/baseline.yml:30-37](), [.github/workflows/bench.yml:32-57](), [.github/workflows/bench.yml:49-57]()\\n\\n## Integration Testing\\n\\nLangGraph maintains comprehensive integration testing that validates end-to-end functionality across different deployment scenarios and backend configurations.\\n\\n### Docker-based Integration\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Docker Services\\\"\\n        PostgresCompose[\\\"tests/compose-postgres.yml\\\"]\\n        DevServer[\\\"langgraph dev server\\\"]\\n        TestDB[\\\"PostgreSQL container\\\"]\\n    end\\n    \\n    subgraph \\\"Integration Flow\\\"\\n        StartPostgres[\\\"make start-postgres\\\"]\\n        StartDevServer[\\\"make start-dev-server\\\"]\\n        RunTests[\\\"pytest integration\\\"]\\n        Cleanup[\\\"make stop-postgres\\\"]\\n    end\\n    \\n    subgraph \\\"Test Scenarios\\\"\\n        MultiBackend[\\\"Multi-backend tests\\\"]\\n        CLIIntegration[\\\"CLI integration\\\"]\\n        EndToEnd[\\\"End-to-end workflows\\\"]\\n    end\\n    \\n    PostgresCompose --> TestDB\\n    StartPostgres --> StartDevServer\\n    StartDevServer --> RunTests\\n    RunTests --> Cleanup\\n    \\n    TestDB --> MultiBackend\\n    DevServer --> CLIIntegration\\n    MultiBackend --> EndToEnd\\n```\\n\\nIntegration testing uses Docker Compose to orchestrate PostgreSQL and development server instances, enabling comprehensive testing of the full application stack.\\n\\n**Sources:** [libs/langgraph/Makefile:40-56](), [.github/workflows/_integration_test.yml:43-68]()\\n\\nThe integration test setup includes environment configuration, service orchestration, and cleanup procedures to ensure isolated and repeatable test runs.\\n\\n**Sources:** [libs/langgraph/Makefile:61-84]()\", \"# Page: Documentation System\\n\\n# Documentation System\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/_scripts/generate_api_reference_links.py](docs/_scripts/generate_api_reference_links.py)\\n- [docs/_scripts/notebook_hooks.py](docs/_scripts/notebook_hooks.py)\\n- [docs/docs/agents/overview.md](docs/docs/agents/overview.md)\\n- [docs/docs/concepts/server-mcp.md](docs/docs/concepts/server-mcp.md)\\n- [docs/docs/how-tos/memory/add-memory.md](docs/docs/how-tos/memory/add-memory.md)\\n- [docs/docs/index.md](docs/docs/index.md)\\n- [docs/docs/reference/mcp.md](docs/docs/reference/mcp.md)\\n- [docs/docs/reference/supervisor.md](docs/docs/reference/supervisor.md)\\n- [docs/docs/reference/swarm.md](docs/docs/reference/swarm.md)\\n- [docs/docs/troubleshooting/errors/INVALID_CHAT_HISTORY.md](docs/docs/troubleshooting/errors/INVALID_CHAT_HISTORY.md)\\n- [docs/docs/tutorials/workflows.md](docs/docs/tutorials/workflows.md)\\n- [docs/mkdocs.yml](docs/mkdocs.yml)\\n- [docs/tests/unit_tests/test_api_reference.py](docs/tests/unit_tests/test_api_reference.py)\\n- [docs/uv.lock](docs/uv.lock)\\n- [examples/tool-calling-errors.ipynb](examples/tool-calling-errors.ipynb)\\n- [examples/tool-calling.ipynb](examples/tool-calling.ipynb)\\n\\n</details>\\n\\n\\n\\n## Purpose and Scope\\n\\nThe Documentation System manages the build pipeline, content processing, and publishing workflow for LangGraph's comprehensive documentation site. This system converts multiple content formats (Markdown, Jupyter notebooks) into a unified static site with automated API reference generation, multi-language support, and advanced navigation features.\\n\\nFor information about the CLI tool used for local development and deployment, see [CLI Tool](#7.1). For details about the platform deployment options, see [LangGraph Platform](#7.2).\\n\\n## Architecture Overview\\n\\nThe documentation system is built on MkDocs with Material theme, enhanced by custom processing hooks and automated content generation. The system supports both Python and JavaScript ecosystems with conditional rendering and language-specific examples.\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Content Sources\\\"\\n        MD[\\\"Markdown Files\\\"]\\n        NB[\\\"Jupyter Notebooks\\\"]\\n        API[\\\"API Documentation\\\"]\\n        README[\\\"README.md\\\"]\\n    end\\n    \\n    subgraph \\\"Processing Pipeline\\\"\\n        HOOKS[\\\"notebook_hooks.py\\\"]\\n        CONVERT[\\\"convert_notebook\\\"]\\n        APIREF[\\\"generate_api_reference_links.py\\\"]\\n        AUTOLINKS[\\\"handle_auto_links\\\"]\\n    end\\n    \\n    subgraph \\\"MkDocs Core\\\"\\n        CONFIG[\\\"mkdocs.yml\\\"]\\n        MATERIAL[\\\"Material Theme\\\"]\\n        PLUGINS[\\\"Plugins\\\"]\\n        NAV[\\\"Navigation\\\"]\\n    end\\n    \\n    subgraph \\\"Output\\\"\\n        STATIC[\\\"Static Site\\\"]\\n        SEARCH[\\\"Search Index\\\"]\\n        REDIRECTS[\\\"HTML Redirects\\\"]\\n    end\\n    \\n    MD --> HOOKS\\n    NB --> CONVERT\\n    API --> APIREF\\n    README --> HOOKS\\n    \\n    HOOKS --> CONFIG\\n    CONVERT --> CONFIG\\n    APIREF --> CONFIG\\n    AUTOLINKS --> CONFIG\\n    \\n    CONFIG --> MATERIAL\\n    CONFIG --> PLUGINS\\n    CONFIG --> NAV\\n    \\n    MATERIAL --> STATIC\\n    PLUGINS --> SEARCH\\n    NAV --> STATIC\\n    HOOKS --> REDIRECTS\\n```\\n\\n**Sources:** [docs/mkdocs.yml:1-387](), [docs/_scripts/notebook_hooks.py:1-612]()\\n\\n## Build Pipeline Components\\n\\n### MkDocs Configuration\\n\\nThe core configuration defines the site structure, theme settings, and processing plugins:\\n\\n| Component | Purpose | Configuration |\\n|-----------|---------|---------------|\\n| Material Theme | Modern responsive design | Custom logo, color schemes, navigation features |\\n| Search Plugin | Full-text search | Custom separators, multi-language support |\\n| Exclude Search | Hide specific pages | Platform-specific content exclusion |\\n| MkDocstrings | API reference generation | Python object documentation with cross-references |\\n| Include Markdown | Content reuse | Shared snippets and templates |\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"mkdocs.yml Configuration\\\"\\n        THEME[\\\"theme: material\\\"]\\n        PLUGINS[\\\"plugins: search, mkdocstrings, tags\\\"]\\n        NAV[\\\"nav: structured hierarchy\\\"]\\n        HOOKS[\\\"hooks: notebook_hooks.py\\\"]\\n        EXTENSIONS[\\\"markdown_extensions: 15+ extensions\\\"]\\n    end\\n    \\n    subgraph \\\"Theme Features\\\"\\n        LOGO[\\\"Custom logos\\\"]\\n        PALETTE[\\\"Light/dark modes\\\"]\\n        FEATURES[\\\"Navigation, search, code\\\"]\\n    end\\n    \\n    THEME --> LOGO\\n    THEME --> PALETTE\\n    THEME --> FEATURES\\n    \\n    PLUGINS --> SEARCH_CONFIG[\\\"Custom search separators\\\"]\\n    PLUGINS --> API_CONFIG[\\\"Python API documentation\\\"]\\n    \\n    HOOKS --> NOTEBOOK_PROCESSING[\\\"Jupyter notebook conversion\\\"]\\n    HOOKS --> REDIRECT_GENERATION[\\\"HTML redirect pages\\\"]\\n```\\n\\n**Sources:** [docs/mkdocs.yml:6-51](), [docs/mkdocs.yml:52-186](), [docs/mkdocs.yml:311-360]()\\n\\n### Content Processing Hooks\\n\\nThe `notebook_hooks.py` module provides lifecycle event handlers for custom content processing:\\n\\n| Hook Function | Trigger | Purpose |\\n|---------------|---------|---------|\\n| `on_files` | File discovery | Convert `.ipynb` files to `NotebookFile` objects |\\n| `on_page_markdown` | Markdown processing | Apply transformations, inject API links |\\n| `on_post_page` | HTML generation | Inject GTM tracking, embed original markdown |\\n| `on_post_build` | Site completion | Generate redirect HTML pages |\\n\\n**Sources:** [docs/_scripts/notebook_hooks.py:217-232](), [docs/_scripts/notebook_hooks.py:427-441](), [docs/_scripts/notebook_hooks.py:545-557](), [docs/_scripts/notebook_hooks.py:561-612]()\\n\\n## Content Processing Pipeline\\n\\n### Notebook Conversion\\n\\nJupyter notebooks are converted to Markdown during the build process through the `convert_notebook` function:\\n\\n```mermaid\\ngraph TD\\n    NOTEBOOK[\\\".ipynb Files\\\"]\\n    CHECK[\\\"DISABLED env check\\\"]\\n    CONVERT[\\\"convert_notebook()\\\"]\\n    MARKDOWN[\\\"Processed Markdown\\\"]\\n    \\n    NOTEBOOK --> CHECK\\n    CHECK -->|\\\"Not disabled\\\"| CONVERT\\n    CHECK -->|\\\"DISABLE_NOTEBOOK_CONVERT=true\\\"| SKIP[\\\"Skip conversion\\\"]\\n    \\n    CONVERT --> MARKDOWN\\n    \\n    subgraph \\\"Notebook Processing\\\"\\n        CELLS[\\\"Extract cells\\\"]\\n        CODE[\\\"Process code blocks\\\"]\\n        OUTPUT[\\\"Handle outputs\\\"]\\n        METADATA[\\\"Parse metadata\\\"]\\n    end\\n    \\n    CONVERT --> CELLS\\n    CELLS --> CODE\\n    CODE --> OUTPUT\\n    OUTPUT --> METADATA\\n    METADATA --> MARKDOWN\\n```\\n\\n**Sources:** [docs/_scripts/notebook_hooks.py:25](), [docs/_scripts/notebook_hooks.py:394-396](), [docs/_scripts/notebook_convert.py:21]()\\n\\n### Multi-Language Support\\n\\nThe system supports conditional rendering for Python and JavaScript content using language-specific blocks:\\n\\n```mermaid\\ngraph LR\\n    CONTENT[\\\"Source Content\\\"]\\n    CONDITIONAL[\\\"_apply_conditional_rendering()\\\"]\\n    TARGET[\\\"TARGET_LANGUAGE env var\\\"]\\n    \\n    subgraph \\\"Language Blocks\\\"\\n        PYTHON_BLOCK[\\\":::python content :::\\\"]\\n        JS_BLOCK[\\\":::js content :::\\\"]\\n    end\\n    \\n    CONTENT --> CONDITIONAL\\n    TARGET --> CONDITIONAL\\n    \\n    CONDITIONAL --> PYTHON_BLOCK\\n    CONDITIONAL --> JS_BLOCK\\n    \\n    PYTHON_BLOCK -->|\\\"target=python\\\"| KEEP_PYTHON[\\\"Keep Python content\\\"]\\n    PYTHON_BLOCK -->|\\\"target=js\\\"| REMOVE_PYTHON[\\\"Remove Python content\\\"]\\n    \\n    JS_BLOCK -->|\\\"target=js\\\"| KEEP_JS[\\\"Keep JS content\\\"] \\n    JS_BLOCK -->|\\\"target=python\\\"| REMOVE_JS[\\\"Remove JS content\\\"]\\n```\\n\\n**Sources:** [docs/_scripts/notebook_hooks.py:261-287](), [docs/_scripts/notebook_hooks.py:398-401](), [docs/_scripts/notebook_hooks.py:413]()\\n\\n### Code Block Enhancement\\n\\nCode blocks receive automatic enhancements including syntax highlighting, line highlighting, and path attribution:\\n\\n| Enhancement | Function | Purpose |\\n|-------------|----------|---------|\\n| Highlight Comments | `_highlight_code_blocks()` | Convert `# highlight-next-line` to `hl_lines` attribute |\\n| Path Attribution | `_add_path_to_code_blocks()` | Add source file paths for executable code blocks |\\n| API References | `update_markdown_with_imports()` | Generate links to API documentation |\\n\\n**Sources:** [docs/_scripts/notebook_hooks.py:290-365](), [docs/_scripts/notebook_hooks.py:235-255](), [docs/_scripts/notebook_hooks.py:407-408]()\\n\\n## API Reference Generation\\n\\n### Import Detection and Link Generation\\n\\nThe `generate_api_reference_links.py` module automatically detects imports in code blocks and generates corresponding API documentation links:\\n\\n```mermaid\\ngraph TD\\n    CODE_BLOCK[\\\"Python Code Block\\\"]\\n    AST_PARSE[\\\"ast.parse()\\\"]\\n    WALK_NODES[\\\"ast.walk() ImportFrom nodes\\\"]\\n    \\n    subgraph \\\"Import Processing\\\"\\n        EXTRACT[\\\"Extract module and class names\\\"]\\n        ECOSYSTEM[\\\"Determine ecosystem (langchain/langgraph)\\\"]\\n        RESOLVE[\\\"_get_full_module_name()\\\"]\\n        LOOKUP[\\\"WELL_KNOWN_LANGGRAPH_OBJECTS lookup\\\"]\\n    end\\n    \\n    subgraph \\\"URL Generation\\\"\\n        LANGCHAIN_URL[\\\"_LANGCHAIN_API_REFERENCE + path\\\"]\\n        LANGGRAPH_URL[\\\"_LANGGRAPH_API_REFERENCE + namespace\\\"]\\n    end\\n    \\n    CODE_BLOCK --> AST_PARSE\\n    AST_PARSE --> WALK_NODES\\n    WALK_NODES --> EXTRACT\\n    \\n    EXTRACT --> ECOSYSTEM\\n    ECOSYSTEM --> RESOLVE\\n    RESOLVE --> LOOKUP\\n    \\n    LOOKUP -->|\\\"LangChain\\\"| LANGCHAIN_URL\\n    LOOKUP -->|\\\"LangGraph\\\"| LANGGRAPH_URL\\n    \\n    LANGCHAIN_URL --> API_LINK[\\\"Generated API Link\\\"]\\n    LANGGRAPH_URL --> API_LINK\\n```\\n\\n**Sources:** [docs/_scripts/generate_api_reference_links.py:121-159](), [docs/_scripts/generate_api_reference_links.py:93-111](), [docs/_scripts/generate_api_reference_links.py:18-84]()\\n\\n### Known Object Mapping\\n\\nLangGraph objects are mapped to their documentation locations through a predefined registry:\\n\\n| Module Pattern | Example Classes | Documentation Namespace |\\n|----------------|-----------------|-------------------------|\\n| `langgraph.prebuilt` | `create_react_agent`, `ToolNode` | `prebuilt` |\\n| `langgraph.graph` | `StateGraph`, `add_messages` | `graphs` |\\n| `langgraph.checkpoint.*` | `PostgresSaver`, `MemorySaver` | `checkpoints` |\\n| `langgraph.types` | `Send`, `Command`, `Interrupt` | `types` |\\n\\n**Sources:** [docs/_scripts/generate_api_reference_links.py:86-90](), [docs/_scripts/generate_api_reference_links.py:18-84]()\\n\\n## Redirect Management\\n\\n### Redirect Map System\\n\\nThe system maintains an extensive redirect map to handle content reorganization and URL changes:\\n\\n```mermaid\\ngraph TB\\n    REDIRECT_MAP[\\\"REDIRECT_MAP dictionary\\\"]\\n    \\n    subgraph \\\"Redirect Categories\\\"\\n        NOTEBOOK_REDIRECTS[\\\"Notebook consolidation\\\"]\\n        GRAPH_API_REDIRECTS[\\\"Graph API reorganization\\\"]\\n        MEMORY_REDIRECTS[\\\"Memory how-tos\\\"]\\n        CLOUD_REDIRECTS[\\\"Platform migration\\\"]\\n        EXTERNAL_REDIRECTS[\\\"Mintlify migration\\\"]\\n    end\\n    \\n    subgraph \\\"Redirect Processing\\\"\\n        POST_BUILD[\\\"on_post_build()\\\"]\\n        HTML_TEMPLATE[\\\"HTML_TEMPLATE\\\"]\\n        WRITE_HTML[\\\"_write_html()\\\"]\\n    end\\n    \\n    REDIRECT_MAP --> NOTEBOOK_REDIRECTS\\n    REDIRECT_MAP --> GRAPH_API_REDIRECTS  \\n    REDIRECT_MAP --> MEMORY_REDIRECTS\\n    REDIRECT_MAP --> CLOUD_REDIRECTS\\n    REDIRECT_MAP --> EXTERNAL_REDIRECTS\\n    \\n    POST_BUILD --> HTML_TEMPLATE\\n    HTML_TEMPLATE --> WRITE_HTML\\n    \\n    subgraph \\\"Generated Files\\\"\\n        INTERNAL_HTML[\\\"Internal redirect HTML\\\"]\\n        EXTERNAL_HTML[\\\"External redirect HTML\\\"]\\n    end\\n    \\n    WRITE_HTML --> INTERNAL_HTML\\n    WRITE_HTML --> EXTERNAL_HTML\\n```\\n\\n**Sources:** [docs/_scripts/notebook_hooks.py:28-209](), [docs/_scripts/notebook_hooks.py:446-461](), [docs/_scripts/notebook_hooks.py:464-478]()\\n\\n## Navigation and Site Structure\\n\\n### Hierarchical Navigation\\n\\nThe site navigation is structured hierarchically with clear sections for different user personas:\\n\\n| Section | Target Audience | Content Type |\\n|---------|-----------------|--------------|\\n| Get started | New users | Quickstarts, basic concepts |\\n| Guides | Developers | How-to guides, API documentation |\\n| Reference | Advanced users | Detailed API reference |\\n| Examples | All users | Template applications, tutorials |\\n\\n```mermaid\\ngraph TD\\n    NAV_ROOT[\\\"nav: root\\\"]\\n    \\n    subgraph \\\"Primary Sections\\\"\\n        GET_STARTED[\\\"Get started\\\"]\\n        GUIDES[\\\"Guides\\\"] \\n        REFERENCE[\\\"Reference\\\"]\\n        EXAMPLES[\\\"Examples\\\"]\\n        RESOURCES[\\\"Additional resources\\\"]\\n    end\\n    \\n    subgraph \\\"Get Started Content\\\"\\n        QUICKSTARTS[\\\"Quickstarts\\\"]\\n        CONCEPTS[\\\"General concepts\\\"]\\n    end\\n    \\n    subgraph \\\"Guides Content\\\"\\n        AGENT_DEV[\\\"Agent development\\\"]\\n        APIS[\\\"LangGraph APIs\\\"]\\n        CAPABILITIES[\\\"Core capabilities\\\"]\\n    end\\n    \\n    NAV_ROOT --> GET_STARTED\\n    NAV_ROOT --> GUIDES\\n    NAV_ROOT --> REFERENCE\\n    NAV_ROOT --> EXAMPLES\\n    NAV_ROOT --> RESOURCES\\n    \\n    GET_STARTED --> QUICKSTARTS\\n    GET_STARTED --> CONCEPTS\\n    \\n    GUIDES --> AGENT_DEV\\n    GUIDES --> APIS\\n    GUIDES --> CAPABILITIES\\n```\\n\\n**Sources:** [docs/mkdocs.yml:188-310]()\\n\\n### Search Configuration\\n\\nThe search system is optimized for technical documentation with custom separators and selective indexing:\\n\\n**Sources:** [docs/mkdocs.yml:53-54](), [docs/mkdocs.yml:55-149]()\\n\\n## Build Automation and Testing\\n\\n### Environment Configuration\\n\\nThe build system supports environment-based configuration:\\n\\n| Environment Variable | Purpose | Default |\\n|---------------------|---------|---------|\\n| `DISABLE_NOTEBOOK_CONVERT` | Skip notebook processing | `false` |\\n| `TARGET_LANGUAGE` | Language-specific rendering | `python` |\\n| `MD_OUTPUT_PATH` | Save processed markdown | `None` |\\n\\n**Sources:** [docs/_scripts/notebook_hooks.py:25](), [docs/_scripts/notebook_hooks.py:398-401](), [docs/_scripts/notebook_hooks.py:436-439]()\\n\\n### Testing Infrastructure\\n\\nThe documentation system includes unit tests for critical components:\\n\\n```mermaid\\ngraph LR\\n    TEST_FILES[\\\"test_api_reference.py\\\"]\\n    \\n    subgraph \\\"Test Categories\\\"\\n        IMPORT_TESTS[\\\"Import extraction tests\\\"]\\n        MARKDOWN_TESTS[\\\"Markdown processing tests\\\"]\\n        LINK_TESTS[\\\"API link generation tests\\\"]\\n    end\\n    \\n    subgraph \\\"Test Scenarios\\\"\\n        SINGLE_IMPORT[\\\"Single imports\\\"]\\n        MULTI_IMPORT[\\\"Multiple imports\\\"]\\n        ALIAS_IMPORT[\\\"Aliased imports\\\"]\\n        MULTILINE_IMPORT[\\\"Multiline imports\\\"]\\n    end\\n    \\n    TEST_FILES --> IMPORT_TESTS\\n    TEST_FILES --> MARKDOWN_TESTS\\n    TEST_FILES --> LINK_TESTS\\n    \\n    IMPORT_TESTS --> SINGLE_IMPORT\\n    IMPORT_TESTS --> MULTI_IMPORT\\n    IMPORT_TESTS --> ALIAS_IMPORT\\n    IMPORT_TESTS --> MULTILINE_IMPORT\\n```\\n\\n**Sources:** [docs/tests/unit_tests/test_api_reference.py:1-213]()\", \"# Page: CI/CD Pipeline\\n\\n# CI/CD Pipeline\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [.github/scripts/run_langgraph_cli_test.py](.github/scripts/run_langgraph_cli_test.py)\\n- [.github/workflows/_integration_test.yml](.github/workflows/_integration_test.yml)\\n- [.github/workflows/_lint.yml](.github/workflows/_lint.yml)\\n- [.github/workflows/_test.yml](.github/workflows/_test.yml)\\n- [.github/workflows/_test_langgraph.yml](.github/workflows/_test_langgraph.yml)\\n- [.github/workflows/_test_release.yml](.github/workflows/_test_release.yml)\\n- [.github/workflows/baseline.yml](.github/workflows/baseline.yml)\\n- [.github/workflows/bench.yml](.github/workflows/bench.yml)\\n- [.github/workflows/ci.yml](.github/workflows/ci.yml)\\n- [.github/workflows/codespell-ignore-words.txt](.github/workflows/codespell-ignore-words.txt)\\n- [.github/workflows/codespell.yml](.github/workflows/codespell.yml)\\n- [.github/workflows/deploy_docs.yml](.github/workflows/deploy_docs.yml)\\n- [.github/workflows/extract_ignored_words_list.py](.github/workflows/extract_ignored_words_list.py)\\n- [.github/workflows/link_check.yml](.github/workflows/link_check.yml)\\n- [.github/workflows/pr_lint.yml](.github/workflows/pr_lint.yml)\\n- [.github/workflows/release.yml](.github/workflows/release.yml)\\n- [.gitignore](.gitignore)\\n- [docs/.gitignore](docs/.gitignore)\\n- [docs/Makefile](docs/Makefile)\\n- [docs/_scripts/_patch.py](docs/_scripts/_patch.py)\\n- [docs/_scripts/prepare_notebooks_for_ci.py](docs/_scripts/prepare_notebooks_for_ci.py)\\n- [docs/docs/cloud/reference/api/openapi.json](docs/docs/cloud/reference/api/openapi.json)\\n- [docs/docs/examples/index.md](docs/docs/examples/index.md)\\n- [docs/overrides/main.html](docs/overrides/main.html)\\n- [docs/package.json](docs/package.json)\\n- [docs/pyproject.toml](docs/pyproject.toml)\\n- [libs/cli/Makefile](libs/cli/Makefile)\\n- [libs/cli/examples/.env.example](libs/cli/examples/.env.example)\\n- [libs/sdk-py/tests/test_select_fields_sync.py](libs/sdk-py/tests/test_select_fields_sync.py)\\n\\n</details>\\n\\n\\n\\nThis document covers LangGraph's continuous integration and continuous deployment pipeline implemented via GitHub Actions workflows. The pipeline handles automated testing, linting, building, releasing packages, and deploying documentation across the monorepo structure.\\n\\nFor information about the documentation build system specifically, see [Documentation System](#10.1). For testing strategies and frameworks, see [Testing Framework](#10.3).\\n\\n## Pipeline Overview\\n\\nLangGraph's CI/CD pipeline consists of multiple GitHub Actions workflows that handle different aspects of the development lifecycle. The pipeline is designed around a monorepo structure with multiple Python packages in the `libs/` directory.\\n\\n```mermaid\\ngraph TD\\n    PR[\\\"Pull Request\\\"] --> CI[\\\"ci.yml\\\"]\\n    Push[\\\"Push to main\\\"] --> CI\\n    Push --> Baseline[\\\"baseline.yml\\\"] \\n    Push --> DeployDocs[\\\"deploy_docs.yml\\\"]\\n    \\n    CI --> Changes[\\\"changes job\\\"]\\n    Changes --> Lint[\\\"_lint.yml\\\"]\\n    Changes --> Test[\\\"_test.yml\\\"] \\n    Changes --> TestLG[\\\"_test_langgraph.yml\\\"]\\n    Changes --> Integration[\\\"_integration_test.yml\\\"]\\n    Changes --> CheckSDK[\\\"check-sdk-methods\\\"]\\n    Changes --> CheckSchema[\\\"check-schema\\\"]\\n    \\n    Manual[\\\"Manual Trigger\\\"] --> Release[\\\"release.yml\\\"]\\n    Release --> TestRelease[\\\"_test_release.yml\\\"]\\n    Release --> Publish[\\\"PyPI Publish\\\"]\\n    \\n    Schedule[\\\"Daily Schedule\\\"] --> LinkCheck[\\\"link_check.yml\\\"]\\n    PR --> PRLint[\\\"pr_lint.yml\\\"]\\n    PR --> Bench[\\\"bench.yml\\\"]\\n    Push --> Codespell[\\\"codespell.yml\\\"]\\n```\\n\\nSources: [.github/workflows/ci.yml:1-179](), [.github/workflows/release.yml:1-328](), [.github/workflows/deploy_docs.yml:1-151]()\\n\\n## Main CI Workflow\\n\\nThe primary CI workflow in `ci.yml` orchestrates testing and quality checks across the monorepo. It uses a path-based change detection system to optimize build times by only running jobs for modified packages.\\n\\n### Change Detection and Matrix Strategy\\n\\n```mermaid\\ngraph LR\\n    Changes[\\\"changes job\\\"] --> Filter[\\\"dorny/paths-filter@v3\\\"]\\n    Filter --> PythonChanges[\\\"python: libs/langgraph/**<br/>libs/sdk-py/**<br/>libs/cli/**<br/>libs/checkpoint/**<br/>libs/checkpoint-sqlite/**<br/>libs/checkpoint-postgres/**<br/>libs/prebuilt/**\\\"]\\n    Filter --> DepsChanges[\\\"deps: **/pyproject.toml<br/>**/uv.lock\\\"]\\n    \\n    PythonChanges --> LintMatrix[\\\"lint matrix\\\"]\\n    PythonChanges --> TestMatrix[\\\"test matrix\\\"] \\n    DepsChanges --> LintMatrix\\n    DepsChanges --> TestMatrix\\n    \\n    LintMatrix --> LintJobs[\\\"libs/langgraph<br/>libs/sdk-py<br/>libs/cli<br/>libs/checkpoint<br/>libs/checkpoint-sqlite<br/>libs/checkpoint-postgres<br/>libs/prebuilt\\\"]\\n    TestMatrix --> TestJobs[\\\"libs/cli<br/>libs/checkpoint<br/>libs/checkpoint-sqlite<br/>libs/checkpoint-postgres<br/>libs/prebuilt<br/>libs/sdk-py\\\"]\\n```\\n\\nThe workflow uses `dorny/paths-filter@v3` to detect changes and conditionally run jobs only when relevant files are modified.\\n\\nSources: [.github/workflows/ci.yml:24-46](), [.github/workflows/ci.yml:47-87]()\\n\\n### Testing Infrastructure\\n\\nThe CI pipeline uses reusable workflows for testing with multi-version Python support:\\n\\n```mermaid\\ngraph TB\\n    TestWorkflow[\\\"_test.yml\\\"] --> PythonMatrix[\\\"Python Versions<br/>3.9, 3.10, 3.11, 3.12, 3.13\\\"]\\n    PythonMatrix --> SetupUV[\\\"astral-sh/setup-uv@v6\\\"]\\n    SetupUV --> Docker[\\\"Docker Hub Login<br/>DOCKERHUB_USERNAME<br/>DOCKERHUB_RO_TOKEN\\\"]\\n    Docker --> UVSync[\\\"uv sync --frozen --group dev\\\"]\\n    UVSync --> MakeTest[\\\"make test\\\"]\\n    MakeTest --> GitStatus[\\\"Git status check<br/>working tree clean\\\"]\\n    \\n    TestLangGraph[\\\"_test_langgraph.yml\\\"] --> LGMatrix[\\\"Python Versions<br/>3.9, 3.10, 3.11, 3.12, 3.13\\\"]\\n    LGMatrix --> LGTest[\\\"make test_parallel\\\"]\\n```\\n\\nThe `_test.yml` workflow is reusable across packages, while `_test_langgraph.yml` provides specialized testing for the core LangGraph package with parallel test execution.\\n\\nSources: [.github/workflows/_test.yml:1-64](), [.github/workflows/_test_langgraph.yml:1-59]()\\n\\n## Release Pipeline\\n\\nThe release workflow implements a secure, multi-stage release process with trusted publishing to PyPI.\\n\\n### Release Workflow Architecture\\n\\n```mermaid\\ngraph TD\\n    Manual[\\\"workflow_dispatch\\\"] --> BuildJob[\\\"build job\\\"]\\n    BuildJob --> UVBuild[\\\"uv build\\\"]\\n    UVBuild --> CheckVersion[\\\"check-version step\\\"]\\n    CheckVersion --> Artifacts[\\\"Upload dist artifacts\\\"]\\n    \\n    BuildJob --> ReleaseNotes[\\\"release-notes job\\\"]\\n    ReleaseNotes --> TagCheck[\\\"Check previous tags\\\"]\\n    TagCheck --> GenerateBody[\\\"Generate release body<br/>git log changes\\\"]\\n    \\n    Artifacts --> TestPyPI[\\\"test-pypi-publish job\\\"]\\n    TestPyPI --> TestRelease[\\\"_test_release.yml\\\"]\\n    TestRelease --> TestPyPIPublish[\\\"Publish to test.pypi.org\\\"]\\n    \\n    TestPyPI --> PreCheck[\\\"pre-release-checks job\\\"]\\n    PreCheck --> ImportTest[\\\"Import published package<br/>from test PyPI\\\"]\\n    ImportTest --> UnitTests[\\\"Run unit tests<br/>against published package\\\"]\\n    \\n    PreCheck --> Publish[\\\"publish job\\\"]\\n    Publish --> PyPIPublish[\\\"pypa/gh-action-pypi-publish\\\"]\\n    PyPIPublish --> MarkRelease[\\\"mark-release job\\\"]\\n    MarkRelease --> CreateTag[\\\"ncipollo/release-action\\\"]\\n```\\n\\nThe release process separates build and publish stages for security, following trusted publishing best practices.\\n\\nSources: [.github/workflows/release.yml:17-82](), [.github/workflows/release.yml:244-327]()\\n\\n### Version Detection and Tagging\\n\\nThe release workflow automatically detects package versions and generates appropriate Git tags:\\n\\n```mermaid\\ngraph LR\\n    CheckVersion[\\\"Check Version step\\\"] --> ExtractName[\\\"PKG_NAME from pyproject.toml\\\"]\\n    ExtractName --> ExtractVersion[\\\"VERSION from pyproject.toml<br/>or __init__.py\\\"]\\n    ExtractVersion --> ShortName[\\\"SHORT_PKG_NAME<br/>remove 'langgraph', '-'\\\"]\\n    ShortName --> TagGeneration[\\\"TAG generation<br/>VERSION or SHORT_PKG_NAME==VERSION\\\"]\\n    \\n    TagGeneration --> Examples[\\\"Examples:<br/>langgraph: '0.1.0'<br/>langgraph-cli: 'cli==0.1.0'<br/>langgraph-checkpoint: 'checkpoint==0.1.0'\\\"]\\n```\\n\\nSources: [.github/workflows/release.yml:59-82]()\\n\\n## Documentation Deployment\\n\\nThe documentation deployment workflow builds and deploys the MkDocs site to GitHub Pages.\\n\\n### Documentation Build Pipeline\\n\\n```mermaid\\ngraph TD\\n    Trigger[\\\"Push to main<br/>Pull Request<br/>workflow_dispatch\\\"] --> ChangedFiles[\\\"Get changed files<br/>Ana06/get-changed-files@v2.3.0\\\"]\\n    \\n    ChangedFiles --> SetupEnv[\\\"Setup Environment\\\"]\\n    SetupEnv --> UVPython[\\\"astral-sh/setup-uv@v6<br/>Python 3.12\\\"]\\n    UVPython --> YarnInstall[\\\"yarn install\\\"]\\n    YarnInstall --> UVSync[\\\"uv sync --all-groups\\\"]\\n    UVSync --> InsiderInstall[\\\"mkdocs-material-insiders<br/>if GITHUB_TOKEN available\\\"]\\n    \\n    InsiderInstall --> UnitTests[\\\"make tests\\\"]\\n    UnitTests --> LintDocs[\\\"make lint-docs\\\"]\\n    LintDocs --> LLMSText[\\\"make llms-text\\\"]\\n    LLMSText --> BuildSite[\\\"make build-docs<br/>DOWNLOAD_STATS=true on main\\\"]\\n    \\n    BuildSite --> LinkCheck[\\\"Check links in notebooks<br/>pytest --check-links\\\"]\\n    LinkCheck --> GitHubPages[\\\"GitHub Pages deployment<br/>actions/deploy-pages@v4\\\"]\\n```\\n\\nThe workflow includes comprehensive link checking for notebook files and conditional stats downloading on the main branch.\\n\\nSources: [.github/workflows/deploy_docs.yml:38-151](), [.github/workflows/deploy_docs.yml:87-136]()\\n\\n## Quality Assurance Workflows\\n\\n### Linting and Formatting\\n\\nThe linting workflow uses modern Python tooling with caching for performance:\\n\\n```mermaid\\ngraph LR\\n    LintWorkflow[\\\"_lint.yml\\\"] --> ChangedFiles[\\\"Get changed files<br/>filter by working-directory\\\"]\\n    ChangedFiles --> SetupPython[\\\"astral-sh/setup-uv@v6<br/>Python 3.12\\\"]\\n    SetupPython --> UVSync[\\\"uv sync --frozen --group dev\\\"]\\n    UVSync --> MypyCache[\\\"Restore .mypy_cache\\\"]\\n    MypyCache --> LintPackage[\\\"make lint_package<br/>or make lint\\\"]\\n    LintPackage --> TestDeps[\\\"uv sync --group dev\\\"]\\n    TestDeps --> MypyCacheTest[\\\"Restore .mypy_cache_test\\\"]\\n    MypyCacheTest --> LintTests[\\\"make lint_tests\\\"]\\n```\\n\\nThe workflow uses separate mypy caches for package and test code, with fallback commands for packages without specific lint targets.\\n\\nSources: [.github/workflows/_lint.yml:1-98]()\\n\\n### Spell Checking and Link Validation\\n\\n```mermaid\\ngraph TD\\n    Codespell[\\\"codespell.yml\\\"] --> ExtractWords[\\\"Extract ignore words<br/>from pyproject.toml\\\"]\\n    ExtractWords --> SpellCheck[\\\"codespell-project/actions-codespell\\\"]\\n    SpellCheck --> NotebookSpell[\\\"make codespell<br/>in docs/\\\"]\\n    NotebookSpell --> LibrarySpell[\\\"Codespell LangGraph Library<br/>libs/langgraph/langgraph/\\\"]\\n    \\n    LinkCheck[\\\"link_check.yml\\\"] --> MarkdownCheck[\\\"gaurav-nelson/github-action-markdown-link-check\\\"]\\n    MarkdownCheck --> ReadmeSync[\\\"Check README.md sync<br/>with libs/langgraph/README.md\\\"]\\n```\\n\\nSources: [.github/workflows/codespell.yml:1-49](), [.github/workflows/link_check.yml:1-50]()\\n\\n## Integration Testing\\n\\n### CLI Integration Tests\\n\\nThe CLI integration testing workflow validates the LangGraph CLI across multiple deployment scenarios:\\n\\n```mermaid\\ngraph TB\\n    IntegrationTest[\\\"_integration_test.yml\\\"] --> SetupCLI[\\\"pip install -e .<br/>CLI globally\\\"]\\n    SetupCLI --> ServiceA[\\\"Service A Test<br/>langgraph build -t langgraph-test-a<br/>timeout 60 run_langgraph_cli_test.py\\\"]\\n    SetupCLI --> ServiceB[\\\"Service B Test<br/>graphs/ directory<br/>langgraph-test-b\\\"]\\n    SetupCLI --> ServiceC[\\\"Service C Test<br/>graphs_reqs_a/ directory<br/>langgraph-test-c\\\"]\\n    SetupCLI --> ServiceD[\\\"Service D Test<br/>graphs_reqs_b/ directory<br/>langgraph-test-d\\\"]\\n    SetupCLI --> ServiceE[\\\"Service E Test<br/>JS service build<br/>langgraph-test-e\\\"]\\n    \\n    ServiceA --> CLITest[\\\"run_langgraph_cli_test.py<br/>Docker compose orchestration<br/>Health check /ok endpoint\\\"]\\n```\\n\\nEach test builds a Docker image and validates the complete deployment stack including API endpoints.\\n\\nSources: [.github/workflows/_integration_test.yml:1-90](), [.github/scripts/run_langgraph_cli_test.py:1-154]()\\n\\n### CLI Test Implementation\\n\\nThe `run_langgraph_cli_test.py` script provides comprehensive integration testing:\\n\\n```mermaid\\ngraph LR\\n    CLITest[\\\"run_langgraph_cli_test.py\\\"] --> ValidateConfig[\\\"langgraph_cli.config.validate_config_file\\\"]\\n    ValidateConfig --> PrepareArgs[\\\"prepare_args_and_stdin<br/>Docker compose arguments\\\"]\\n    PrepareArgs --> ComposeUp[\\\"docker compose up --wait\\\"]\\n    ComposeUp --> HealthCheck[\\\"HTTP GET /ok endpoint<br/>30 second timeout\\\"]\\n    HealthCheck --> ComposeDown[\\\"docker compose down -v<br/>cleanup\\\"]\\n    \\n    ComposeUp --> ErrorHandling[\\\"On failure:<br/>docker compose ps<br/>docker compose logs\\\"]\\n```\\n\\nSources: [.github/scripts/run_langgraph_cli_test.py:15-143]()\\n\\n## Performance and Benchmarking\\n\\n### Benchmark Workflows\\n\\n```mermaid\\ngraph TD\\n    Baseline[\\\"baseline.yml<br/>Push to main\\\"] --> BaselineBench[\\\"make benchmark<br/>out/benchmark-baseline.json\\\"]\\n    BaselineBench --> SaveBaseline[\\\"actions/cache/save<br/>benchmark-baseline key\\\"]\\n    \\n    PR[\\\"Pull Request\\\"] --> BenchWorkflow[\\\"bench.yml\\\"]\\n    BenchWorkflow --> RestoreBaseline[\\\"actions/cache/restore<br/>benchmark-baseline\\\"]\\n    BenchWorkflow --> RunBench[\\\"make benchmark-fast\\\"]\\n    RunBench --> Compare[\\\"pyperf compare_to<br/>main vs changes\\\"]\\n    Compare --> Annotation[\\\"GitHub annotations<br/>Benchmark results\\\"]\\n```\\n\\nThe benchmark system maintains baseline performance metrics and compares pull request changes against the main branch.\\n\\nSources: [.github/workflows/baseline.yml:1-38](), [.github/workflows/bench.yml:1-72]()\\n\\n## Schema Validation\\n\\nThe CI pipeline includes automated schema validation for the CLI configuration:\\n\\n```mermaid\\ngraph LR\\n    SchemaCheck[\\\"check-schema job\\\"] --> SetupEnv[\\\"astral-sh/setup-uv@v6<br/>Python 3.11\\\"]\\n    SetupEnv --> InstallCLI[\\\"cd libs/cli<br/>uv sync\\\"]\\n    InstallCLI --> CurrentSchema[\\\"cp schemas/schema.json<br/>schemas/schema.current.json\\\"]\\n    CurrentSchema --> GenerateSchema[\\\"uv run python generate_schema.py\\\"]\\n    GenerateSchema --> DiffCheck[\\\"diff schemas/schema.json<br/>schemas/schema.current.json\\\"]\\n    DiffCheck --> FailOnChange[\\\"Exit 1 if schema changed<br/>without regeneration\\\"]\\n```\\n\\nThis ensures that CLI schema changes are properly regenerated and committed.\\n\\nSources: [.github/workflows/ci.yml:111-146]()\", \"# Page: Testing Framework\\n\\n# Testing Framework\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [.github/workflows/codespell-ignore-words.txt](.github/workflows/codespell-ignore-words.txt)\\n- [.github/workflows/codespell.yml](.github/workflows/codespell.yml)\\n- [.github/workflows/deploy_docs.yml](.github/workflows/deploy_docs.yml)\\n- [.github/workflows/extract_ignored_words_list.py](.github/workflows/extract_ignored_words_list.py)\\n- [.github/workflows/pr_lint.yml](.github/workflows/pr_lint.yml)\\n- [.gitignore](.gitignore)\\n- [docs/.gitignore](docs/.gitignore)\\n- [docs/Makefile](docs/Makefile)\\n- [docs/_scripts/_patch.py](docs/_scripts/_patch.py)\\n- [docs/_scripts/prepare_notebooks_for_ci.py](docs/_scripts/prepare_notebooks_for_ci.py)\\n- [docs/docs/examples/index.md](docs/docs/examples/index.md)\\n- [docs/overrides/main.html](docs/overrides/main.html)\\n- [docs/package.json](docs/package.json)\\n- [docs/pyproject.toml](docs/pyproject.toml)\\n\\n</details>\\n\\n\\n\\nThe LangGraph testing framework provides comprehensive test infrastructure for validating documentation, notebooks, and build processes. This framework ensures documentation quality through automated testing of Jupyter notebooks, link validation, spell checking, and CI/CD integration for documentation deployment.\\n\\nFor information about the development workflow and code quality tools, see [CI/CD Pipeline](10.2). For documentation generation and API reference systems, see [Documentation System](10.1).\\n\\n## Documentation Testing Architecture\\n\\nThe testing framework is organized around multiple test suites that validate different aspects of the documentation system:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Documentation Tests\\\"\\n        UT[\\\"Unit Tests<br/>docs/tests/unit_tests/\\\"]\\n        NT[\\\"Notebook Tests<br/>VCR Cassette Testing\\\"]\\n        LT[\\\"Link Tests<br/>pytest-check-links\\\"]\\n        ST[\\\"Spell Tests<br/>codespell workflows\\\"]\\n    end\\n    \\n    subgraph \\\"Test Utilities\\\"\\n        VCR[\\\"VCR Cassettes<br/>docs/cassettes/\\\"]\\n        NBC[\\\"Notebook Preprocessing<br/>prepare_notebooks_for_ci.py\\\"]\\n        PATCH[\\\"urllib3 Patch<br/>_patch.py\\\"]\\n        IGNORE[\\\"Ignore Lists<br/>NOTEBOOKS_NO_EXECUTION\\\"]\\n    end\\n    \\n    subgraph \\\"CI/CD Integration\\\"\\n        DOCS[\\\"deploy_docs.yml<br/>Documentation Pipeline\\\"]\\n        SPELL[\\\"codespell.yml<br/>Spell Checking\\\"]\\n        LINT[\\\"pr_lint.yml<br/>PR Title Validation\\\"]\\n    end\\n    \\n    subgraph \\\"Build System\\\"\\n        MAKE[\\\"Makefile<br/>Test Commands\\\"]\\n        UV[\\\"uv Package Manager<br/>Dependency Resolution\\\"]\\n        PYTEST[\\\"pytest Framework<br/>Test Execution\\\"]\\n    end\\n    \\n    UT --> VCR\\n    NT --> NBC\\n    LT --> PATCH\\n    ST --> IGNORE\\n    \\n    DOCS --> MAKE\\n    SPELL --> UV\\n    LINT --> PYTEST\\n```\\n\\nSources: [docs/Makefile:33-35](), [docs/_scripts/prepare_notebooks_for_ci.py:1-264](), [.github/workflows/deploy_docs.yml:1-151](), [docs/_scripts/_patch.py:1-94]()\\n\\n## Documentation Test Organization\\n\\nThe test suite follows a hierarchical organization focused on documentation quality and notebook validation:\\n\\n### Core Test Categories\\n\\n| Test Category | Location | Purpose |\\n|---------------|----------|---------|\\n| Unit Tests | `docs/tests/unit_tests/` | Documentation script validation |\\n| Notebook Tests | `docs/docs/**/*.ipynb` | Jupyter notebook execution and API testing |\\n| Link Tests | `pytest-check-links` | Documentation link validation |\\n| Spell Tests | `codespell` workflows | Spelling and typo detection |\\n\\nThe test infrastructure is organized around documentation-specific components:\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Test Infrastructure\\\"\\n        NBC[prepare_notebooks_for_ci.py]\\n        VCR[VCR_Cassettes]\\n        PATCH[urllib3_patch.py]\\n        IGNORE[notebooks_no_execution.json]\\n    end\\n    \\n    subgraph \\\"Test Execution\\\"\\n        UNIT[\\\"make tests<br/>Unit Tests\\\"]\\n        LINT[\\\"make lint-docs<br/>Code Quality\\\"]\\n        LINKS[\\\"pytest --check-links<br/>Link Validation\\\"]\\n        SPELL[\\\"make codespell<br/>Spell Check\\\"]\\n    end\\n    \\n    subgraph \\\"CI Workflows\\\"\\n        DEPLOY[deploy_docs.yml]\\n        CODESPELL[codespell.yml]\\n        PRLINT[pr_lint.yml]\\n    end\\n    \\n    NBC --> VCR\\n    NBC --> PATCH\\n    NBC --> IGNORE\\n    \\n    UNIT --> DEPLOY\\n    LINT --> DEPLOY\\n    LINKS --> DEPLOY\\n    SPELL --> CODESPELL\\n```\\n\\nSources: [docs/_scripts/prepare_notebooks_for_ci.py:207-248](), [docs/Makefile:33-35](), [.github/workflows/deploy_docs.yml:65-67](), [.github/workflows/codespell.yml:17-49]()\\n\\n## CI/CD Pipeline Integration\\n\\nThe documentation testing framework integrates with GitHub Actions through specialized workflows that validate documentation quality and notebook execution:\\n\\n### Documentation Workflow Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Documentation Pipeline\\\"\\n        DEPLOY[deploy_docs.yml]\\n        CHANGES[\\\"get-changed-files<br/>Ana06/get-changed-files@v2.3.0\\\"]\\n        FILTER[\\\"docs/docs/** filter\\\"]\\n    end\\n    \\n    subgraph \\\"Test Execution Steps\\\"\\n        SETUP[\\\"setup-uv@v6<br/>Python 3.12\\\"]\\n        DEPS[\\\"uv sync --all-groups<br/>Install dependencies\\\"]\\n        UNIT[\\\"make tests<br/>Unit test execution\\\"]\\n        LINT[\\\"make lint-docs<br/>Code quality\\\"]\\n    end\\n    \\n    subgraph \\\"Notebook Processing\\\"\\n        PREP[prepare_notebooks_for_ci.py]\\n        VCR[\\\"VCR cassette injection\\\"]\\n        EXEC[\\\"Notebook execution\\\"]\\n        LINKS[\\\"pytest --check-links<br/>Link validation\\\"]\\n    end\\n    \\n    subgraph \\\"Quality Workflows\\\"\\n        SPELL[codespell.yml]\\n        EXTRACT[\\\"extract_ignored_words_list.py\\\"]\\n        PRLINT[pr_lint.yml]\\n        SEMANTIC[\\\"amannn/action-semantic-pull-request@v5\\\"]\\n    end\\n    \\n    DEPLOY --> CHANGES\\n    CHANGES --> FILTER\\n    \\n    SETUP --> DEPS\\n    DEPS --> UNIT\\n    UNIT --> LINT\\n    \\n    PREP --> VCR\\n    VCR --> EXEC\\n    EXEC --> LINKS\\n    \\n    SPELL --> EXTRACT\\n    PRLINT --> SEMANTIC\\n```\\n\\n### Documentation Test Matrix\\n\\nThe documentation CI pipeline uses targeted testing strategies:\\n\\n- **Unit Tests**: Run `make tests` for documentation script validation \\n- **Notebook Tests**: Execute notebooks with VCR cassettes for API call mocking\\n- **Link Validation**: Use `pytest-check-links` to validate documentation links\\n- **Spell Checking**: Run `codespell` across documentation and notebooks\\n- **Environment**: Python 3.12 with `uv` package manager for consistent dependencies\\n\\nSources: [.github/workflows/deploy_docs.yml:38-84](), [.github/workflows/codespell.yml:22-49](), [.github/workflows/pr_lint.yml:10-46]()\\n\\n## Notebook Testing Infrastructure\\n\\nThe framework provides specialized utilities for testing Jupyter notebooks and handling API calls in documentation:\\n\\n### VCR Testing System\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"VCR Components\\\"\\n        VCR[\\\"custom_vcr<br/>VCR instance\\\"]\\n        CASSETTES[\\\"docs/cassettes/<br/>*.msgpack.zlib\\\"]\\n        SERIALIZER[\\\"AdvancedCompressedSerializer<br/>msgpack + zlib compression\\\"]\\n    end\\n    \\n    subgraph \\\"Notebook Processing\\\"\\n        PREP[prepare_notebooks_for_ci.py]\\n        INJECT[\\\"VCR context injection\\\"]\\n        BLOCKLIST[\\\"BLOCKLIST_COMMANDS<br/>WebBaseLoader, draw_mermaid_png\\\"]\\n        NOEXEC[\\\"NOTEBOOKS_NO_EXECUTION<br/>Skip list\\\"]\\n    end\\n    \\n    subgraph \\\"Patch System\\\"\\n        PATCH[_patch.py]\\n        URLLIB3[\\\"urllib3 compatibility\\\"]\\n        LANGSMITH[\\\"LangSmith SDK support\\\"]\\n        ASYNCIO[\\\"nest_asyncio.apply()\\\"]\\n    end\\n    \\n    subgraph \\\"API Mocking\\\"\\n        HEADERS[\\\"filter_headers<br/>x-api-key, authorization\\\"]\\n        RECORD[\\\"record_mode: once<br/>Cassette recording\\\"]\\n        COMPRESS[\\\"compress_data()<br/>Base64 encoding\\\"]\\n    end\\n    \\n    VCR --> CASSETTES\\n    CASSETTES --> SERIALIZER\\n    \\n    PREP --> INJECT\\n    INJECT --> BLOCKLIST\\n    BLOCKLIST --> NOEXEC\\n    \\n    PATCH --> URLLIB3\\n    URLLIB3 --> LANGSMITH\\n    LANGSMITH --> ASYNCIO\\n    \\n    HEADERS --> RECORD\\n    RECORD --> COMPRESS\\n```\\n\\n### Key Testing Utilities\\n\\nThe documentation testing utilities provide notebook execution and API mocking:\\n\\n- **`custom_vcr`**: VCR instance with advanced compressed serialization for API call recording\\n- **`compress_data()`**: msgpack + zlib compression for efficient cassette storage\\n- **`has_blocklisted_command()`**: Skip cells with problematic commands like `WebBaseLoader`\\n- **`add_vcr_to_notebook()`**: Inject VCR context managers into notebook code cells\\n- **`patch_urllib3()`**: Fix urllib3 compatibility issues with VCR in concurrent workloads\\n- **`NOTEBOOKS_NO_EXECUTION`**: List of notebooks that should not be executed automatically\\n\\nSources: [docs/_scripts/prepare_notebooks_for_ci.py:98-191](), [docs/_scripts/prepare_notebooks_for_ci.py:26-56](), [docs/_scripts/_patch.py:53-94](), [docs/_scripts/prepare_notebooks_for_ci.py:165-185]()\\n\\n## Link Validation System\\n\\nThe documentation testing framework uses `pytest-check-links` to validate all external links in generated documentation:\\n\\n### Link Testing Architecture\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Link Validation\\\"\\n        PYTEST[\\\"pytest -v --check-links\\\"]\\n        IGNORE[\\\"check-links-ignore patterns\\\"]\\n        SCHEDULE[\\\"Scheduled vs PR builds\\\"]\\n        CHANGED[\\\"Changed notebook detection\\\"]\\n    end\\n    \\n    subgraph \\\"Ignore Patterns\\\"\\n        SMITH[\\\"https://(api|web|docs).smith.langchain.com/.*\\\"]\\n        ACADEMY[\\\"https://academy.langchain.com/.*\\\"]\\n        SOCIAL[\\\"https://x.com/.*, https://twitter.com/.*\\\"]\\n        GITHUB[\\\"https://github.com/.*\\\"]\\n        LOCAL[\\\"http://localhost:*, http://127.0.0.1:*\\\"]\\n    end\\n    \\n    subgraph \\\"Test Execution Modes\\\"\\n        ALL[\\\"Scheduled: All HTML files\\\"]\\n        INCREMENTAL[\\\"PR: Changed notebooks only\\\"]\\n        FILTER[\\\"git diff --name-only origin/main\\\"]\\n        CONVERT[\\\"sed -E 's|^docs/docs/|site/|; s/.ipynb$/\\\\/index.html/'\\\"]\\n    end\\n    \\n    subgraph \\\"File Discovery\\\"\\n        FIND[\\\"find site -name 'index.html'\\\"]\\n        EXCLUDE[\\\"grep -v 'storm/index.html'\\\"]\\n        VALIDATE[\\\"Link validation execution\\\"]\\n    end\\n    \\n    PYTEST --> IGNORE\\n    IGNORE --> SCHEDULE\\n    SCHEDULE --> CHANGED\\n    \\n    SMITH --> ACADEMY\\n    ACADEMY --> SOCIAL\\n    SOCIAL --> GITHUB\\n    GITHUB --> LOCAL\\n    \\n    ALL --> INCREMENTAL\\n    INCREMENTAL --> FILTER\\n    FILTER --> CONVERT\\n    \\n    FIND --> EXCLUDE\\n    EXCLUDE --> VALIDATE\\n```\\n\\n### Link Validation Strategy\\n\\nThe link validation system provides comprehensive coverage while optimizing execution time:\\n\\n1. **Scheduled Runs**: Validate all HTML files in the `site/` directory on schedule\\n2. **PR Validation**: Only validate HTML files corresponding to changed notebooks in pull requests\\n3. **Ignore Patterns**: Skip validation for known problematic domains and local development URLs\\n4. **Error Handling**: Exit code 5 (no tests collected) is treated as success for incremental validation\\n\\nThe system converts notebook paths to their corresponding HTML output paths using sed pattern replacement and validates links in the generated static site.\\n\\nSources: [.github/workflows/deploy_docs.yml:87-135](), [.github/workflows/deploy_docs.yml:94-109](), [.github/workflows/deploy_docs.yml:115-131]()\\n\\n## Documentation Test Execution\\n\\nThe framework employs different execution strategies for documentation testing based on content type and CI context:\\n\\n### Test Execution Strategies\\n\\n```mermaid\\ngraph LR\\n    subgraph \\\"Documentation Tests\\\"\\n        UNIT[\\\"make tests<br/>uv run pytest tests/unit_tests\\\"]\\n        LINT[\\\"make lint-docs<br/>ruff format --check docs\\\"]\\n        SPELL[\\\"make codespell<br/>codespell_notebooks.sh\\\"]\\n    end\\n    \\n    subgraph \\\"Notebook Processing\\\"\\n        PREP[\\\"prepare_notebooks_for_ci.py<br/>VCR injection\\\"]\\n        EXEC[\\\"Notebook execution<br/>with API mocking\\\"]\\n        SKIP[\\\"NOTEBOOKS_NO_EXECUTION<br/>Skip problematic notebooks\\\"]\\n    end\\n    \\n    subgraph \\\"Build Pipeline\\\"\\n        BUILD[\\\"make build-docs<br/>mkdocs build --strict\\\"]\\n        STATS[\\\"DOWNLOAD_STATS=true<br/>Main branch only\\\"]\\n        PREBUILT[\\\"build-prebuilt<br/>Third-party stats\\\"]\\n    end\\n    \\n    UNIT --> LINT\\n    LINT --> SPELL\\n    \\n    PREP --> EXEC\\n    EXEC --> SKIP\\n    \\n    BUILD --> STATS\\n    STATS --> PREBUILT\\n```\\n\\n### Test Isolation and Environment\\n\\nThe documentation testing framework ensures proper isolation and environment management:\\n\\n- **VCR Isolation**: Each notebook cell gets a unique cassette file to prevent API call conflicts\\n- **Environment Variables**: API keys are filtered from cassettes and fake values used in CI\\n- **Dependency Management**: `uv sync --all-groups` ensures consistent package versions\\n- **Notebook Preprocessing**: Problematic cells are identified and skipped automatically\\n- **Build Caching**: Docker layer caching and dependency caching optimize CI execution time\\n\\nSources: [docs/Makefile:33-35](), [docs/_scripts/prepare_notebooks_for_ci.py:134-139](), [.github/workflows/deploy_docs.yml:48-63](), [docs/_scripts/prepare_notebooks_for_ci.py:82-91]()\\n\\n## Cross-Library Test Coordination\\n\\nThe testing framework coordinates validation across the multi-package LangGraph ecosystem:\\n\\n### Library Test Matrix\\n\\n```mermaid\\ngraph TB\\n    subgraph \\\"Core Libraries\\\"\\n        LG[\\\"langgraph<br/>Core framework\\\"]\\n        CP[\\\"langgraph-checkpoint<br/>Base persistence\\\"]\\n        PB[\\\"langgraph-prebuilt<br/>High-level components\\\"]\\n    end\\n    \\n    subgraph \\\"Persistence Implementations\\\"\\n        CPS[\\\"langgraph-checkpoint-sqlite<br/>SQLite persistence\\\"]\\n        CPP[\\\"langgraph-checkpoint-postgres<br/>PostgreSQL persistence\\\"]\\n    end\\n    \\n    subgraph \\\"Client Libraries\\\" \\n        SDK[\\\"langgraph-sdk-py<br/>Python client\\\"]\\n        CLI[\\\"langgraph-cli<br/>Development tools\\\"]\\n    end\\n    \\n    subgraph \\\"Test Dependencies\\\"\\n        LG --> CP\\n        CP --> CPS\\n        CP --> CPP\\n        SDK --> LG\\n        CLI --> LG\\n        PB --> LG\\n    end\\n    \\n    subgraph \\\"Integration Testing\\\"\\n        IT[\\\"CLI Integration<br/>Docker container testing\\\"]\\n        ST[\\\"SDK Testing<br/>Remote graph validation\\\"]\\n        CT[\\\"Compatibility Testing<br/>Cross-version support\\\"]\\n    end\\n```\\n\\n### Test Coordination Strategies\\n\\n1. **Dependency Testing**: Libraries that depend on LangGraph core run tests with the latest core version\\n2. **Integration Validation**: CLI and SDK tests validate end-to-end functionality with containerized services\\n3. **Schema Compatibility**: Configuration schema changes are validated across CLI and platform integrations\\n4. **Version Matrix**: Tests run across Python 3.9-3.13 to ensure broad compatibility\\n\\nThe framework uses path-based change detection to optimize CI execution, only running tests for modified library components while maintaining cross-library compatibility validation.\\n\\nSources: [.github/workflows/ci.yml:24-86](), [.github/workflows/_integration_test.yml:10-75](), [.github/workflows/ci.yml:110-144]()\", \"# Page: Examples and Tutorials\\n\\n# Examples and Tutorials\\n\\n<details>\\n<summary>Relevant source files</summary>\\n\\nThe following files were used as context for generating this wiki page:\\n\\n- [docs/docs/cloud/how-tos/add-human-in-the-loop.md](docs/docs/cloud/how-tos/add-human-in-the-loop.md)\\n- [docs/docs/cloud/how-tos/configuration_cloud.md](docs/docs/cloud/how-tos/configuration_cloud.md)\\n- [docs/docs/concepts/assistants.md](docs/docs/concepts/assistants.md)\\n- [docs/docs/concepts/human_in_the_loop.md](docs/docs/concepts/human_in_the_loop.md)\\n- [docs/docs/concepts/low_level.md](docs/docs/concepts/low_level.md)\\n- [docs/docs/concepts/persistence.md](docs/docs/concepts/persistence.md)\\n- [docs/docs/concepts/time-travel.md](docs/docs/concepts/time-travel.md)\\n- [docs/docs/how-tos/assets/human_in_loop_parallel.png](docs/docs/how-tos/assets/human_in_loop_parallel.png)\\n- [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md](docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md)\\n- [docs/docs/how-tos/human_in_the_loop/time-travel.md](docs/docs/how-tos/human_in_the_loop/time-travel.md)\\n\\n</details>\\n\\n\\n\\nThis document provides comprehensive examples and tutorials for learning LangGraph through practical implementations. It covers everything from basic graph construction to advanced patterns like human-in-the-loop workflows and time travel debugging. Each example is designed to demonstrate core concepts while providing working code that can be adapted for your use cases.\\n\\nFor conceptual overviews of LangGraph components, see [Core Architecture](#2). For deployment-specific guides, see [Deployment and Platform](#7). For production persistence strategies, see [Persistence System](#5).\\n\\n## Getting Started with Basic Graphs\\n\\nThe foundation of LangGraph is the `StateGraph` class, which allows you to define nodes and edges that operate on a shared state schema. The most basic pattern involves creating a state schema, adding nodes, and connecting them with edges.\\n\\n### Basic State and Node Pattern\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Python Code Space\\\"\\n        StateDict[\\\"State: TypedDict\\\"]\\n        NodeFunc[\\\"def node_function(state: State)\\\"]\\n        StateGraph[\\\"StateGraph(State)\\\"]\\n        AddNode[\\\"builder.add_node('name', node_function)\\\"]\\n        AddEdge[\\\"builder.add_edge(START, 'name')\\\"]\\n        Compile[\\\"builder.compile()\\\"]\\n    end\\n    \\n    subgraph \\\"Execution Space\\\"\\n        Invoke[\\\"graph.invoke(input, config)\\\"]\\n        StatePassing[\\\"State flows through nodes\\\"]\\n        Output[\\\"Final state returned\\\"]\\n    end\\n    \\n    StateDict --> StateGraph\\n    NodeFunc --> AddNode\\n    StateGraph --> AddNode\\n    AddNode --> AddEdge\\n    AddEdge --> Compile\\n    Compile --> Invoke\\n    Invoke --> StatePassing\\n    StatePassing --> Output\\n```\\n\\nSources: [docs/docs/concepts/low_level.md:58-76](), [docs/docs/concepts/low_level.md:392-438]()\\n\\n### Simple Graph Example\\n\\nThe simplest graph consists of a single node that processes input and returns output. This pattern is demonstrated in the persistence examples where a basic state schema is defined using `TypedDict`, nodes are added using `builder.add_node()`, and execution flows from `START` to the node to `END`.\\n\\n```mermaid\\ngraph LR\\n    START --> \\\"node_a\\\"\\n    \\\"node_a\\\" --> \\\"node_b\\\" \\n    \\\"node_b\\\" --> END\\n    \\n    subgraph \\\"State Management\\\"\\n        State[\\\"State: {'foo': str, 'bar': list[str]}\\\"]\\n        Reducers[\\\"add operator for bar channel\\\"]\\n    end\\n```\\n\\nSources: [docs/docs/concepts/persistence.md:61-90]()\\n\\n## Core Patterns and State Management\\n\\n### State Reducers and Message Handling\\n\\nLangGraph provides powerful state management through reducers, which determine how state updates are applied. The most common pattern is using `add_messages` for conversation history and custom reducers for other data types.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"State Schema Patterns\\\"\\n        DefaultReducer[\\\"Default: Override\\\"]\\n        AddReducer[\\\"add: Append lists\\\"]\\n        AddMessages[\\\"add_messages: Smart message handling\\\"]\\n        CustomReducer[\\\"Custom: User-defined logic\\\"]\\n    end\\n    \\n    subgraph \\\"Code Implementation\\\"\\n        TypedDict[\\\"class State(TypedDict)\\\"]\\n        Annotated[\\\"Annotated[list[str], add]\\\"]\\n        MessagesState[\\\"MessagesState\\\"]\\n        CustomFunc[\\\"def custom_reducer(x, y)\\\"]\\n    end\\n    \\n    DefaultReducer --> TypedDict\\n    AddReducer --> Annotated\\n    AddMessages --> MessagesState\\n    CustomReducer --> CustomFunc\\n```\\n\\nSources: [docs/docs/concepts/low_level.md:210-288](), [docs/docs/concepts/low_level.md:289-390]()\\n\\n### Multiple Schema Pattern\\n\\nFor complex workflows, LangGraph supports multiple schemas including input/output schemas and private state channels. This allows nodes to communicate with different state subsets while maintaining a clean API boundary.\\n\\nSources: [docs/docs/concepts/low_level.md:82-208]()\\n\\n## Human-in-the-Loop Tutorials\\n\\n### Dynamic Interrupts with `interrupt()` Function\\n\\nThe `interrupt()` function provides the primary mechanism for pausing graph execution and collecting human input. This pattern is essential for approval workflows, content review, and interactive agents.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Interrupt Workflow\\\"\\n        NodeExec[\\\"Node starts execution\\\"]\\n        InterruptCall[\\\"interrupt(payload) called\\\"]\\n        GraphPause[\\\"Graph execution pauses\\\"]\\n        ReturnInterrupt[\\\"Returns __interrupt__ in result\\\"]\\n        HumanInput[\\\"Human provides input\\\"]\\n        ResumeCommand[\\\"Command(resume=value)\\\"]\\n        NodeComplete[\\\"Node completes with human input\\\"]\\n    end\\n    \\n    NodeExec --> InterruptCall\\n    InterruptCall --> GraphPause\\n    GraphPause --> ReturnInterrupt\\n    ReturnInterrupt --> HumanInput\\n    HumanInput --> ResumeCommand\\n    ResumeCommand --> NodeComplete\\n    \\n    subgraph \\\"Code Elements\\\"\\n        InterruptFunc[\\\"interrupt(data)\\\"]\\n        CommandClass[\\\"Command(resume=data)\\\"]\\n        CheckpointerReq[\\\"checkpointer required\\\"]\\n        ThreadId[\\\"thread_id in config\\\"]\\n    end\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:20-86](), [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:282-312]()\\n\\n### Common Human-in-the-Loop Patterns\\n\\nThe documentation identifies four primary patterns for human intervention:\\n\\n1. **Approve or Reject**: Use `interrupt()` before critical actions, return `Command(goto=node)` based on approval\\n2. **Edit Graph State**: Pause to review and modify state, return updated values\\n3. **Review Tool Calls**: Interrupt before tool execution, allow modification of tool calls\\n4. **Validate Human Input**: Pause to collect and validate user input before proceeding\\n\\nSources: [docs/docs/concepts/human_in_the_loop.md:39-46](), [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:394-471]()\\n\\n### Multiple Parallel Interrupts\\n\\nWhen nodes with interrupts run in parallel, LangGraph can handle multiple interrupts simultaneously and resume them all with a single command using interrupt ID mapping.\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/add-human-in-the-loop.md:313-393]()\\n\\n## Time Travel and Persistence Examples\\n\\n### Checkpoint-Based Time Travel\\n\\nTime travel functionality allows resuming execution from any previous checkpoint, either replaying the same state or modifying it to explore alternatives. This creates new forks in the execution history.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Time Travel Process\\\"\\n        RunGraph[\\\"graph.invoke(input, config)\\\"]\\n        GetHistory[\\\"graph.get_state_history(config)\\\"]\\n        SelectCheckpoint[\\\"Select checkpoint_id\\\"]\\n        UpdateState[\\\"graph.update_state() [optional]\\\"]\\n        ResumeExecution[\\\"graph.invoke(None, checkpoint_config)\\\"]\\n    end\\n    \\n    subgraph \\\"Checkpoint Structure\\\"\\n        StateSnapshot[\\\"StateSnapshot\\\"]\\n        ConfigField[\\\"config: {thread_id, checkpoint_id}\\\"]\\n        ValuesField[\\\"values: Current state\\\"]\\n        NextField[\\\"next: Nodes to execute\\\"]\\n        TasksField[\\\"tasks: PregelTask objects\\\"]\\n    end\\n    \\n    RunGraph --> GetHistory\\n    GetHistory --> SelectCheckpoint\\n    SelectCheckpoint --> UpdateState\\n    UpdateState --> ResumeExecution\\n    \\n    GetHistory --> StateSnapshot\\n    StateSnapshot --> ConfigField\\n    StateSnapshot --> ValuesField\\n    StateSnapshot --> NextField\\n    StateSnapshot --> TasksField\\n```\\n\\nSources: [docs/docs/how-tos/human_in_the_loop/time-travel.md:1-22](), [docs/docs/concepts/persistence.md:44-55](), [docs/docs/concepts/persistence.md:476-511]()\\n\\n### Memory Store Cross-Thread Persistence\\n\\nThe `Store` interface enables sharing information across different threads, essential for maintaining user context across conversations. The pattern involves namespacing by user ID and using semantic search for retrieval.\\n\\nSources: [docs/docs/concepts/persistence.md:637-921]()\\n\\n## Cloud Platform Tutorials\\n\\n### Assistant Management Workflow\\n\\nAssistants in LangGraph Platform allow managing configurations separately from graph logic, enabling multiple specialized versions of the same architecture.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Assistant Lifecycle\\\"\\n        GraphDeploy[\\\"Deploy graph to platform\\\"]\\n        DefaultAssistant[\\\"Default assistant created\\\"]\\n        CreateCustom[\\\"client.assistants.create()\\\"]\\n        UpdateAssistant[\\\"client.assistants.update()\\\"]\\n        SetVersion[\\\"client.assistants.set_latest()\\\"]\\n        UseAssistant[\\\"client.runs.stream(assistant_id)\\\"]\\n    end\\n    \\n    subgraph \\\"Configuration Elements\\\"\\n        ConfigSchema[\\\"Context schema in graph\\\"]\\n        AssistantConfig[\\\"Assistant-specific config\\\"]\\n        ModelProvider[\\\"model_name: 'openai'\\\"]\\n        SystemPrompt[\\\"system_prompt: 'You are...'\\\"]\\n        ThreadId[\\\"thread_id for conversation\\\"]\\n    end\\n    \\n    GraphDeploy --> DefaultAssistant\\n    DefaultAssistant --> CreateCustom\\n    CreateCustom --> UpdateAssistant\\n    UpdateAssistant --> SetVersion\\n    SetVersion --> UseAssistant\\n    \\n    ConfigSchema --> AssistantConfig\\n    AssistantConfig --> ModelProvider\\n    AssistantConfig --> SystemPrompt\\n    UseAssistant --> ThreadId\\n```\\n\\nSources: [docs/docs/cloud/how-tos/configuration_cloud.md:50-111](), [docs/docs/cloud/how-tos/configuration_cloud.md:122-213](), [docs/docs/concepts/assistants.md:1-35]()\\n\\n### Server API Human-in-the-Loop\\n\\nThe cloud platform provides server API endpoints for human-in-the-loop workflows, using `client.runs.wait()` with `Command` objects for resuming interrupted execution.\\n\\nSources: [docs/docs/cloud/how-tos/add-human-in-the-loop.md:1-486]()\\n\\n## Advanced Patterns\\n\\n### Control Flow with `Command` Objects\\n\\nThe `Command` primitive combines state updates with routing decisions in a single node return value, providing more sophisticated control flow than separate conditional edges.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\\"Command Pattern\\\"\\n        NodeLogic[\\\"Node processes state\\\"]\\n        CommandReturn[\\\"return Command(update=dict, goto=str)\\\"]\\n        StateUpdate[\\\"State channels updated\\\"]\\n        RouteToNode[\\\"Navigate to specified node\\\"]\\n    end\\n    \\n    subgraph \\\"Alternative Patterns\\\"\\n        ConditionalEdge[\\\"add_conditional_edges()\\\"]\\n        SeparateNodes[\\\"Separate routing logic\\\"]\\n        SendObjects[\\\"Send() for map-reduce\\\"]\\n    end\\n    \\n    NodeLogic --> CommandReturn\\n    CommandReturn --> StateUpdate\\n    CommandReturn --> RouteToNode\\n    \\n    ConditionalEdge --> SeparateNodes\\n    SeparateNodes --> SendObjects\\n```\\n\\nSources: [docs/docs/concepts/low_level.md:799-873]()\\n\\n### Send Pattern for Map-Reduce\\n\\nThe `Send` object enables dynamic fan-out patterns where the number of parallel executions is determined at runtime, commonly used for map-reduce workflows.\\n\\nSources: [docs/docs/concepts/low_level.md:768-798]()\\n\\n### Static vs Dynamic Interrupts\\n\\nLangGraph provides two interrupt mechanisms: static interrupts set at compile time (`interrupt_before`/`interrupt_after`) primarily for debugging, and dynamic interrupts using `interrupt()` function for production human-in-the-loop workflows.\\n\\nSources: [docs/docs/cloud/how-tos/add-human-in-the-loop.md:304-482]()\\n\\n## Integration Examples\\n\\n### LangChain Ecosystem Integration\\n\\nLangGraph integrates seamlessly with LangChain components, particularly for message handling, chat models, and tool calling. The `MessagesState` pattern provides built-in compatibility with LangChain message types.\\n\\nSources: [docs/docs/concepts/low_level.md:289-375]()\\n\\n### Checkpointer Backend Options\\n\\nLangGraph supports multiple persistence backends through the checkpointer interface: `InMemorySaver` for development, `SQLiteSaver` for local persistence, and `PostgresSaver` for production deployments.\\n\\nSources: [docs/docs/concepts/persistence.md:8-15]()\\n\\nThis comprehensive set of examples and tutorials provides the foundation for building sophisticated LangGraph applications across different deployment scenarios and use cases.\"]",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "read_wiki_contents",
      "id": "1867885d-281a-40c9-aa10-b554ae7af602",
      "tool_call_id": "call_d9dd5ec5a5e74e0da8ed28",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "",
      "additional_kwargs": {
        "tool_calls": [
          {
            "id": "call_34c79c2a06904965a78d5c",
            "function": {
              "arguments": "{\"query\": \"langgraph 中关于多agent的相关介绍\"}",
              "name": "KB_search"
            },
            "type": "function",
            "index": 0
          }
        ],
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 26,
          "prompt_tokens": 120514,
          "total_tokens": 120540,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-483bb7c1-4366-9600-aad4-9db18c8c352e",
        "service_tier": null,
        "finish_reason": "tool_calls",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--00997bb7-26cb-45d8-a9d1-d2fe0f42e80b-0",
      "example": false,
      "tool_calls": [
        {
          "name": "KB_search",
          "args": {
            "query": "langgraph 中关于多agent的相关介绍"
          },
          "id": "call_34c79c2a06904965a78d5c",
          "type": "tool_call"
        }
      ],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 120514,
        "output_tokens": 26,
        "total_tokens": 120540,
        "input_token_details": {},
        "output_token_details": {}
      }
    },
    {
      "content": "LangGraph 中的多代理架构（Multi-Agent Architecture）是一种基于图结构的智能体协作系统，其核心在于通过连接多个智能体（Agent）形成动态、可扩展的协作网络，以解决复杂任务。以下是典型应用案例与关键技术要点的总结：\n\n---\n\n### 一、典型应用案例\n\n1. **复杂问题求解与决策支持**\n   - **场景**：企业战略规划、医疗诊断辅助、金融风险评估。\n   - **实现方式**：多个专业智能体（如数据分析员、法规专家、财务分析师）协同工作，每个智能体负责特定子任务，通过图结构传递中间结果，最终生成综合决策建议。\n   - **优势**：避免单一模型的认知盲区，提升推理深度与可信度。\n\n2. **自动化工作流编排（Workflow Orchestration）**\n   - **场景**：客服系统、订单处理流程、内容审核。\n   - **实现方式**：将任务拆分为多个步骤（如用户意图识别 → 情绪分析 → 资料查询 → 回复生成），由不同智能体按顺序或条件触发执行，形成可回溯、可监控的工作流。\n   - **优势**：支持动态路由与异常处理，适应非线性流程。\n\n3. **科学研究与实验设计**\n   - **场景**：药物分子筛选、物理模拟优化。\n   - **实现方式**：设置“假设提出者”“模拟执行者”“结果验证者”等角色，智能体之间通过消息通信迭代改进实验方案。\n   - **优势**：支持探索式学习和反馈闭环，加速科研进程。\n\n4. **教育辅导系统**\n   - **场景**：个性化学习路径推荐、错题解析。\n   - **实现方式**：由“知识理解模块”“学习风格分析模块”“讲解生成模块”组成协作网络，根据学生行为动态调整教学策略。\n   - **优势**：实现高度个性化的互动式教学。\n\n---\n\n### 二、关键技术要点\n\n1. **图结构建模（Graph-Based Coordination）**\n   - 使用有向图表示智能体之间的依赖关系与控制流。\n   - 每个节点代表一个智能体，边表示消息传递或状态转移。\n   - 支持循环结构（如自洽验证）、分支判断（条件路由），增强灵活性。\n\n2. **智能体角色定义与职责分离**\n   - 明确划分智能体的功能角色（如协调者、执行者、检查者）。\n   - 通过提示工程（Prompt Engineering）为每个智能体定制角色设定（Role Prompt）。\n   - 支持动态角色切换（如从“提问者”变为“分析者”）。\n\n3. **状态管理与共享记忆**\n   - 所有智能体共享一个全局状态图（State Graph），包含任务上下文、中间结果、历史记录。\n   - 状态更新机制确保一致性，支持版本控制与回滚。\n   - 可集成外部存储（如数据库、向量库）作为长期记忆。\n\n4. **通信协议与消息机制**\n   - 定义标准消息格式（JSON Schema），包含 sender、receiver、content、action type 等字段。\n   - 支持同步/异步通信，以及广播、点对点等多种模式。\n   - 支持语义理解的消息过滤与路由。\n\n5. **容错与异常处理机制**\n   - 引入“监督智能体”或“仲裁者”角色，用于检测死锁、错误传播。\n   - 支持超时重试、降级执行、人工干预介入。\n   - 提供可视化调试工具追踪执行路径。\n\n6. **可扩展性与模块化设计**\n   - 智能体可独立开发、测试、部署，通过接口集成到图中。\n   - 支持插件式架构，便于接入外部工具（API、数据库、代码解释器等）。\n   - 可基于 LangGraph 的 `add_node`, `add_edge` 等 API 动态构建和修改图结构。\n\n---\n\n### 三、技术实现示例（简化伪代码）\n\n```python\nfrom langgraph.graph import StateGraph, END\n\n# 定义状态结构\nclass AgentState(TypedDict):\n    task: str\n    results: dict\n    current_agent: str\n\n# 创建图\nworkflow = StateGraph(AgentState)\n\n# 添加智能体节点\nworkflow.add_node(\"planner\", planner_agent)\nworkflow.add_node(\"researcher\", researcher_agent)\nworkflow.add_node(\"validator\", validator_agent)\n\n# 定义边：控制流\nworkflow.add_edge(\"planner\", \"researcher\")\nworkflow.add_conditional_edges(\n    \"researcher\",\n    lambda state: \"validate\" if state[\"results\"][\"confidence\"] > 0.8 else \"retry\",\n    {\"validate\": \"validator\", \"retry\": \"researcher\"}\n)\nworkflow.add_edge(\"validator\", END)\n\n# 编译图\napp = workflow.compile()\n```\n\n---\n\n### 总结\n\nLangGraph 的多代理架构通过**图结构**实现了智能体间的**动态协作、状态共享与流程控制**，适用于需要多角色协同、复杂决策与可追溯性的场景。其核心技术包括：**角色化智能体设计、状态驱动的图执行、灵活的消息通信、强大的容错机制**，是构建下一代自主智能系统的有力工具。",
      "additional_kwargs": {},
      "response_metadata": {},
      "type": "tool",
      "name": "KB_search",
      "id": "60900557-9ed1-414a-bf69-b44d1d0920e4",
      "tool_call_id": "call_34c79c2a06904965a78d5c",
      "artifact": null,
      "status": "success"
    },
    {
      "content": "好的，根据您提供的信息，我为您整理了关于 **LangGraph 中多 Agent（多智能体）系统**的详细介绍。\n\nLangGraph 并非直接提供一个名为“多 Agent”的独立组件，而是通过其核心的**图（Graph）架构**和**状态管理**能力，为构建复杂的多智能体（Multi-Agent）系统提供了强大的基础设施。您可以将每个智能体视为图中的一个节点（Node），并通过定义节点间的连接（Edge）和共享的状态（State）来实现智能体之间的协作、通信和任务编排。\n\n以下是基于 LangGraph 构建多智能体系统的关键介绍：\n\n### 一、 核心概念：以图构建多智能体系统\n\n在 LangGraph 中，多智能体系统的本质是一个**有向图**，其中：\n\n*   **节点 (Node)**：代表一个独立的智能体（Agent）。每个节点是一个可执行的函数（或 Runnable），它接收当前状态，进行处理（例如调用 LLM、执行工具、进行计算），然后返回对状态的更新。\n*   **边 (Edge)**：代表智能体之间的控制流和通信路径。边决定了一个智能体执行完毕后，下一步应该执行哪个（或哪些）智能体。\n*   **状态 (State)**：是整个系统共享的“记忆”或“上下文”。它是一个类似 `TypedDict` 的结构，可以包含所有智能体需要访问和更新的信息，例如对话历史、任务目标、中间结果、变量等。状态是实现智能体间协作的核心。\n\n### 二、 构建多智能体系统的关键技术\n\n1.  **状态驱动的协作 (State-Driven Collaboration)**\n    *   **共享状态模式**：所有智能体都读写同一个状态对象。例如，一个“研究者”智能体将搜索结果写入状态的 `research_results` 字段，随后的“撰写者”智能体可以读取该字段来生成报告。\n    *   **消息传递模式**：利用 `add_messages` 等内置的**状态更新器（State Updater）**。每个智能体向状态中的 `messages` 列表追加消息（如 AI 生成的文本、工具调用等），后续智能体可以基于完整的消息历史进行决策，实现了类似对话的协作。\n\n2.  **灵活的控制流 (Flexible Control Flow)**\n    *   **静态边 (Static Edges)**：使用 `add_edge` 定义固定的执行顺序，例如 `A -> B -> C`。\n    *   **条件边 (Conditional Edges)**：使用 `add_conditional_edges` 定义动态路由。一个智能体的输出可以决定下一步执行哪个智能体。这使得系统能根据任务进展、LLM 的判断或外部输入做出分支决策。\n    *   **`Send` 原语 (Send Primitive)**：这是实现**Map-Reduce**模式的关键。一个智能体可以返回一个 `Send('target_node', partial_state)` 对象，从而动态地向目标智能体发送特定的数据，实现一对多的并行处理。例如，一个“任务分发者”可以将一个大任务拆分成多个子任务，并通过多个 `Send` 对象分发给不同的“执行者”智能体。\n    *   **`Command` 原语 (Command Primitive)**：用于更复杂的控制，如在节点返回时直接更新状态 (`Command(update={...})`)、跳转到特定节点 (`Command(goto='node_name')`)，或用于恢复被中断的流程 (`Command(resume=...)`)。\n\n3.  **持久化与人类在环路 (Persistence & Human-in-the-Loop)**\n    *   **Checkpointing (检查点)**：通过 `checkpointer`，可以将图的执行状态（包括所有智能体的中间状态）持久化到数据库（如 SQLite, PostgreSQL）。这使得长时间运行的多智能体任务可以中断、恢复，甚至支持“时间旅行”调试。\n    *   **`interrupt()` 函数**：允许在任意智能体内部暂停执行，等待人类审核或输入。这对于需要人工监督、批准或干预的多智能体工作流至关重要。\n\n### 三、 典型的多智能体架构模式\n\n1.  **流水线 (Pipeline)**：\n    *   **描述**：最简单的模式，智能体按固定顺序执行，每个智能体的输出是下一个智能体的输入。\n    *   **适用场景**：数据处理流水线、内容生成（大纲 -> 草稿 -> 润色）。\n\n2.  **分治与聚合 (Map-Reduce)**：\n    *   **描述**：一个“分发”智能体将任务分解，通过 `Send` 原语分发给多个并行的“执行”智能体。然后，一个“聚合”智能体收集所有结果并进行汇总。\n    *   **适用场景**：并行搜索多个信息源、对多个文档进行分析总结。\n\n3.  **反思与验证 (Reflection & Validation)**：\n    *   **描述**：一个“执行”智能体完成任务后，将结果交给一个“批判者”或“验证者”智能体进行审查。如果验证失败，流程可以回到执行智能体进行修改。\n    *   **适用场景**：提高输出质量、事实核查、代码生成与测试。\n\n4.  **基于代理的建模 (Agent-Based Modeling)**：\n    *   **描述**：模拟一个由多个自主智能体组成的环境。每个智能体都有自己的目标和行为策略，它们通过共享状态（如一个虚拟世界的状态）进行交互。\n    *   **适用场景**：模拟社会动态、游戏 AI、经济模型。\n\n### 总结\n\nLangGraph 本身是一个**编排框架**，而非一个预设的多智能体产品。它通过 `StateGraph`、`Pregel` 运行时、状态管理、检查点和控制流原语（`Send`, `Command`, `interrupt`）等核心组件，为开发者提供了构建高度灵活、可扩展、可持久化和可调试的多智能体系统的强大工具集。开发者可以利用这些工具，根据具体需求设计出各种复杂的多智能体协作模式。",
      "additional_kwargs": {
        "refusal": null
      },
      "response_metadata": {
        "token_usage": {
          "completion_tokens": 1327,
          "prompt_tokens": 121693,
          "total_tokens": 123020,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        },
        "model_name": "qwen-plus-2025-07-14",
        "system_fingerprint": null,
        "id": "chatcmpl-c9e829c3-3b2a-9f32-8dd6-c53723e053b8",
        "service_tier": null,
        "finish_reason": "stop",
        "logprobs": null
      },
      "type": "ai",
      "name": null,
      "id": "run--a82da620-d11d-4e8c-bb1c-a5232e35f56a-0",
      "example": false,
      "tool_calls": [],
      "invalid_tool_calls": [],
      "usage_metadata": {
        "input_tokens": 121693,
        "output_tokens": 1327,
        "total_tokens": 123020,
        "input_token_details": {},
        "output_token_details": {}
      }
    }
  ],
  "created_at": "2025-09-04T09:17:40.301758",
  "updated_at": "2025-09-04T09:30:31.534124",
  "metadata": {}
}