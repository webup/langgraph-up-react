#!/usr/bin/env python3
"""
æµå¼è°ƒç”¨å®¢æˆ·ç«¯ç¤ºä¾‹
å±•ç¤ºå„ç§æµå¼å¤„ç†æ¨¡å¼
"""
import asyncio
import json
import os
from typing import AsyncGenerator, Dict, Any
from dotenv import load_dotenv
from common.context import Context
from react_agent import graph

# æ˜¾å¼åŠ è½½.envæ–‡ä»¶
load_dotenv()


async def basic_streaming():
    """åŸºç¡€æµå¼è°ƒç”¨"""
    print("=== åŸºç¡€æµå¼è°ƒç”¨ ===")
    
    question = "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    print(f"é—®é¢˜: {question}")
    print("å›ç­”: ", end="", flush=True)
    
    full_response = ""
    async for chunk in graph.astream(
        {"messages": [("user", question)]},
        context=Context(
            model="qwen:qwen-flash",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªAIä¸“å®¶ï¼Œè¯·è¯¦ç»†å›ç­”é—®é¢˜ã€‚"
        )
    ):
        # å¤„ç†æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º
        for node_name, node_output in chunk.items():
            if node_name == "call_model" and "messages" in node_output:
                message = node_output["messages"][-1]
                if hasattr(message, 'content') and message.content:
                    # å®æ—¶æ‰“å°å†…å®¹
                    print(message.content, end="", flush=True)
                    full_response = message.content
    
    print(f"\n\nå®Œæ•´å›ç­”: {full_response}\n")


async def streaming_with_tool_calls():
    """å¸¦å·¥å…·è°ƒç”¨çš„æµå¼å¤„ç†"""
    print("=== å¸¦å·¥å…·è°ƒç”¨çš„æµå¼å¤„ç† ===")
    
    question = "è¯·æœç´¢å¹¶å‘Šè¯‰æˆ‘æœ€æ–°çš„Pythonç‰ˆæœ¬ä¿¡æ¯"
    print(f"é—®é¢˜: {question}")
    print("å¤„ç†è¿‡ç¨‹:")
    
    step = 1
    async for chunk in graph.astream(
        {"messages": [("user", question)]},
        context=Context(
            model="qwen:qwen-flash",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶ï¼Œå¯ä»¥ä½¿ç”¨æœç´¢å·¥å…·è·å–æœ€æ–°ä¿¡æ¯ã€‚"
        )
    ):
        for node_name, node_output in chunk.items():
            print(f"\næ­¥éª¤ {step}: èŠ‚ç‚¹ '{node_name}'")
            
            if "messages" in node_output:
                for message in node_output["messages"]:
                    # å¤„ç†AIæ¶ˆæ¯
                    if hasattr(message, 'content') and message.content:
                        print(f"  ğŸ’­ æ€è€ƒ: {message.content[:100]}...")
                    
                    # å¤„ç†å·¥å…·è°ƒç”¨
                    if hasattr(message, 'tool_calls') and message.tool_calls:
                        for tool_call in message.tool_calls:
                            print(f"  ğŸ”§ è°ƒç”¨å·¥å…·: {tool_call.get('name', 'unknown')}")
                            print(f"     å‚æ•°: {tool_call.get('args', {})}")
                    
                    # å¤„ç†å·¥å…·ç»“æœ
                    if hasattr(message, 'name'):  # ToolMessage
                        print(f"  ğŸ“Š å·¥å…· '{message.name}' ç»“æœ: {str(message.content)[:100]}...")
            
            step += 1
    
    print()


async def streaming_with_interrupts():
    """å¸¦ä¸­æ–­çš„æµå¼å¤„ç†"""
    print("=== å¸¦ä¸­æ–­çš„æµå¼å¤„ç†ç¤ºä¾‹ ===")
    
    question = "è¯·åˆ†æ­¥éª¤è§£é‡Šå¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ"
    print(f"é—®é¢˜: {question}")
    print("å›ç­” (å¯ä¸­æ–­): ")
    
    count = 0
    async for chunk in graph.astream(
        {"messages": [("user", question)]},
        context=Context(
            model="qwen:qwen-flash",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ•™è‚²ä¸“å®¶ï¼Œè¯·åˆ†æ­¥éª¤è¯¦ç»†å›ç­”ã€‚"
        )
    ):
        count += 1
        
        for node_name, node_output in chunk.items():
            if node_name == "call_model" and "messages" in node_output:
                message = node_output["messages"][-1]
                if hasattr(message, 'content') and message.content:
                    print(f"[å— {count}] {message.content}")
        
        # æ¨¡æ‹Ÿç”¨æˆ·ä¸­æ–­ï¼ˆåœ¨ç¬¬3ä¸ªå—ååœæ­¢ï¼‰
        if count >= 3:
            print("\n[ç”¨æˆ·ä¸­æ–­] å·²è·å¾—è¶³å¤Ÿä¿¡æ¯ï¼Œåœæ­¢æ¥æ”¶...\n")
            break


async def streaming_json_mode():
    """JSONæ ¼å¼æµå¼è¾“å‡º"""
    print("=== JSONæ ¼å¼æµå¼è¾“å‡º ===")
    
    question = "è¯·ç”¨JSONæ ¼å¼åˆ—å‡ºPythonçš„5ä¸ªä¸»è¦ç‰¹ç‚¹"
    print(f"é—®é¢˜: {question}")
    print("JSONç»“æœ:")
    
    async for chunk in graph.astream(
        {"messages": [("user", question)]},
        context=Context(
            model="qwen:qwen-flash",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼å›ç­”ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"
        )
    ):
        for node_name, node_output in chunk.items():
            if node_name == "call_model" and "messages" in node_output:
                message = node_output["messages"][-1]
                if hasattr(message, 'content') and message.content:
                    try:
                        # å°è¯•è§£æJSON
                        content = message.content.strip()
                        if content.startswith('{') or content.startswith('['):
                            parsed = json.loads(content)
                            print(json.dumps(parsed, indent=2, ensure_ascii=False))
                        else:
                            print(f"éJSONå†…å®¹: {content}")
                    except json.JSONDecodeError:
                        print(f"JSONè§£æå¤±è´¥: {message.content}")
    
    print()


async def concurrent_streaming():
    """å¹¶å‘æµå¼å¤„ç†"""
    print("=== å¹¶å‘æµå¼å¤„ç† ===")
    
    questions = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ", 
        "ä»€ä¹ˆæ˜¯è®¡ç®—æœºè§†è§‰ï¼Ÿ"
    ]
    
    async def process_question(q: str, index: int):
        print(f"\n[çº¿ç¨‹ {index+1}] é—®é¢˜: {q}")
        print(f"[çº¿ç¨‹ {index+1}] å›ç­”: ", end="", flush=True)
        
        async for chunk in graph.astream(
            {"messages": [("user", q)]},
            context=Context(
                model="qwen:qwen-flash",
                system_prompt=f"ä½ æ˜¯AIä¸“å®¶#{index+1}ï¼Œè¯·ç®€æ´å›ç­”ã€‚"
            )
        ):
            for node_name, node_output in chunk.items():
                if node_name == "call_model" and "messages" in node_output:
                    message = node_output["messages"][-1]
                    if hasattr(message, 'content') and message.content:
                        print(f"[çº¿ç¨‹ {index+1}] {message.content}")
                        break
    
    # å¹¶å‘æ‰§è¡Œ
    tasks = [process_question(q, i) for i, q in enumerate(questions)]
    await asyncio.gather(*tasks, return_exceptions=True)
    
    print()


async def custom_stream_handler():
    """è‡ªå®šä¹‰æµå¤„ç†å™¨"""
    print("=== è‡ªå®šä¹‰æµå¤„ç†å™¨ ===")
    
    class CustomStreamHandler:
        def __init__(self):
            self.total_tokens = 0
            self.start_time = None
            self.responses = []
        
        async def handle_stream(self, question: str):
            import time
            self.start_time = time.time()
            
            print(f"ğŸ¤– å¼€å§‹å¤„ç†: {question}")
            
            async for chunk in graph.astream(
                {"messages": [("user", question)]},
                context=Context(
                    model="qwen:qwen-flash",
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªhelpful AIåŠ©æ‰‹ã€‚"
                )
            ):
                await self.process_chunk(chunk)
            
            self.print_summary()
        
        async def process_chunk(self, chunk):
            for node_name, node_output in chunk.items():
                if "messages" in node_output:
                    for message in node_output["messages"]:
                        if hasattr(message, 'content') and message.content:
                            self.responses.append(message.content)
                            # ä¼°ç®—tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•°/4ï¼‰
                            self.total_tokens += len(message.content) // 4
        
        def print_summary(self):
            import time
            duration = time.time() - self.start_time
            print(f"\nğŸ“Š å¤„ç†æ‘˜è¦:")
            print(f"   - å“åº”æ•°é‡: {len(self.responses)}")
            print(f"   - ä¼°ç®—Tokens: {self.total_tokens}")
            print(f"   - å¤„ç†æ—¶é—´: {duration:.2f}ç§’")
            if self.responses:
                print(f"   - æœ€ç»ˆå›ç­”: {self.responses[-1][:100]}...")
    
    handler = CustomStreamHandler()
    await handler.handle_stream("è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†")
    
    print()


async def main():
    """ä¸»å‡½æ•°"""
    print("LangGraph ReActæ™ºèƒ½ä½“æµå¼è°ƒç”¨ç¤ºä¾‹\n")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('DASHSCOPE_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° DASHSCOPE_API_KEY")
        print("è¯·ç¡®ä¿ .env æ–‡ä»¶å­˜åœ¨å¹¶åŒ…å«æ­£ç¡®çš„APIå¯†é’¥")
        return
    else:
        print(f"âœ… APIå¯†é’¥å·²é…ç½®: {api_key[:10]}...")
    
    try:
        await basic_streaming()
        
        # æœç´¢å·¥å…·ç¤ºä¾‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
        try:
            await streaming_with_tool_calls()
        except Exception as e:
            print(f"å·¥å…·è°ƒç”¨ç¤ºä¾‹è·³è¿‡: {e}\n")
        
        await streaming_with_interrupts()
        await streaming_json_mode()
        await concurrent_streaming()
        await custom_stream_handler()
        
    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")
        print("\nè§£å†³æ–¹æ¡ˆï¼š")
        print("1. é…ç½®ç¯å¢ƒ: cp .env.example .env")
        print("2. è®¾ç½®APIå¯†é’¥ï¼ˆè‡³å°‘éœ€è¦DASHSCOPE_API_KEYç”¨äºQwenæ¨¡å‹ï¼‰")
        print("3. å®‰è£…ä¾èµ–: uv sync --dev")


if __name__ == "__main__":
    asyncio.run(main())